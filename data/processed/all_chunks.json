[
  {
    "text": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance",
    "type": "semantic",
    "chunk_id": "wiki_0_chunk_0",
    "source_doc_id": "wiki_0",
    "source_title": "Machine learning",
    "source": "wikipedia",
    "chunk_index": 0,
    "word_count": 68,
    "char_count": 460,
    "metadata": {
      "original_length": 462,
      "cleaned_length": 462,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals",
    "type": "semantic",
    "chunk_id": "wiki_1_chunk_0",
    "source_doc_id": "wiki_1",
    "source_title": "Artificial intelligence",
    "source": "wikipedia",
    "chunk_index": 0,
    "word_count": 64,
    "char_count": 461,
    "metadata": {
      "original_length": 463,
      "cleaned_length": 463,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and training them to process data The adjective deep refers to the use of multiple layers in the network Methods used can be supervised, semi-supervised or unsupervised",
    "type": "semantic",
    "chunk_id": "wiki_2_chunk_0",
    "source_doc_id": "wiki_2",
    "source_title": "Deep learning",
    "source": "wikipedia",
    "chunk_index": 0,
    "word_count": 64,
    "char_count": 454,
    "metadata": {
      "original_length": 462,
      "cleaned_length": 458,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "Natural language processing (NLP) is the processing of natural language information by a computer The study of NLP, a subfield of computer science, is generally associated with artificial intelligence NLP is related to information retrieval, knowledge representation, computational linguistics, and more broadly with linguistics",
    "type": "semantic",
    "chunk_id": "wiki_3_chunk_0",
    "source_doc_id": "wiki_3",
    "source_title": "Natural language processing",
    "source": "wikipedia",
    "chunk_index": 0,
    "word_count": 44,
    "char_count": 328,
    "metadata": {
      "original_length": 331,
      "cleaned_length": 331,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e in the form of decisions Understanding in this context signifies the transformation of visual images into descriptions of the world that make sense to thought processes and can elicit appropriate action",
    "type": "semantic",
    "chunk_id": "wiki_4_chunk_0",
    "source_doc_id": "wiki_4",
    "source_title": "Computer vision",
    "source": "wikipedia",
    "chunk_index": 0,
    "word_count": 62,
    "char_count": 429,
    "metadata": {
      "original_length": 627,
      "cleaned_length": 625,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory",
    "type": "semantic",
    "chunk_id": "wiki_4_chunk_1",
    "source_doc_id": "wiki_4",
    "source_title": "Computer vision",
    "source": "wikipedia",
    "chunk_index": 1,
    "word_count": 28,
    "char_count": 189,
    "metadata": {
      "original_length": 627,
      "cleaned_length": 625,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark)",
    "type": "semantic",
    "chunk_id": "arxiv_0_chunk_0",
    "source_doc_id": "arxiv_0",
    "source_title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
    "source": "arxiv",
    "chunk_index": 0,
    "word_count": 47,
    "char_count": 377,
    "metadata": {
      "original_length": 1595,
      "cleaned_length": 1592,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps",
    "type": "semantic",
    "chunk_id": "arxiv_0_chunk_1",
    "source_doc_id": "arxiv_0",
    "source_title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
    "source": "arxiv",
    "chunk_index": 1,
    "word_count": 57,
    "char_count": 449,
    "metadata": {
      "original_length": 1595,
      "cleaned_length": 1592,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics",
    "type": "semantic",
    "chunk_id": "arxiv_0_chunk_2",
    "source_doc_id": "arxiv_0",
    "source_title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
    "source": "arxiv",
    "chunk_index": 2,
    "word_count": 59,
    "char_count": 441,
    "metadata": {
      "original_length": 1595,
      "cleaned_length": 1592,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation Project page: https:flux-reason-6m",
    "type": "semantic",
    "chunk_id": "arxiv_0_chunk_3",
    "source_doc_id": "arxiv_0",
    "source_title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
    "source": "arxiv",
    "chunk_index": 3,
    "word_count": 39,
    "char_count": 301,
    "metadata": {
      "original_length": 1595,
      "cleaned_length": 1592,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "Large language models require massive memory footprints, severely limiting deployment on consumer hardware Quantization reduces memory through lower numerical precision, but extreme 2-bit quantization suffers from catastrophic performance loss due to outliers in activations Rotation-based methods such as QuIP and QuaRot apply orthogonal transforms to eliminate outliers before quantization, using computational invariance: mathbfy mathbfWx (mathbfWQT)(mathbfQx) for orthogonal mathbfQ",
    "type": "semantic",
    "chunk_id": "arxiv_1_chunk_0",
    "source_doc_id": "arxiv_1",
    "source_title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms",
    "source": "arxiv",
    "chunk_index": 0,
    "word_count": 58,
    "char_count": 486,
    "metadata": {
      "original_length": 1790,
      "cleaned_length": 1740,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "However, these methods use fixed transforms--Hadamard matrices achieving optimal worst-case coherence mu 1sqrtn--that cannot adapt to specific weight distributions We identify that different transformer layers exhibit distinct outlier patterns, motivating layer-adaptive rotations rather than one-size-fits-all approaches We propose ButterflyQuant, which replaces Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens rotation angles",
    "type": "semantic",
    "chunk_id": "arxiv_1_chunk_1",
    "source_doc_id": "arxiv_1",
    "source_title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms",
    "source": "arxiv",
    "chunk_index": 1,
    "word_count": 53,
    "char_count": 469,
    "metadata": {
      "original_length": 1790,
      "cleaned_length": 1740,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "Unlike Hadamards discrete 1, -1 entries that are non-differentiable and prohibit gradient-based learning, butterfly transforms continuous parameterization enables smooth optimization while guaranteeing orthogonality by construction This orthogonal constraint ensures theoretical guarantees in outlier suppression while achieving O(n log n) computational complexity with only fracn log n2 learnable parameters",
    "type": "semantic",
    "chunk_id": "arxiv_1_chunk_2",
    "source_doc_id": "arxiv_1",
    "source_title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms",
    "source": "arxiv",
    "chunk_index": 2,
    "word_count": 48,
    "char_count": 408,
    "metadata": {
      "original_length": 1790,
      "cleaned_length": 1740,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "We further introduce a uniformity regularization on post-transformation activations to promote smoother distributions amenable to quantization Learning requires only 128 calibration samples and converges in minutes on a single GPU--a negligible one-time cost On LLaMA-2-7B with 2-bit quantization, ButterflyQuant achieves 15 4 perplexity versus 22 1 for QuaRot",
    "type": "semantic",
    "chunk_id": "arxiv_1_chunk_3",
    "source_doc_id": "arxiv_1",
    "source_title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms",
    "source": "arxiv",
    "chunk_index": 3,
    "word_count": 48,
    "char_count": 360,
    "metadata": {
      "original_length": 1790,
      "cleaned_length": 1740,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "Large language models require massive memory footprints, severely limiting deployment on consumer hardware Quantization reduces memory through lower numerical precision, but extreme 2-bit quantization suffers from catastrophic performance loss due to outliers in activations Rotation-based methods such as QuIP and QuaRot apply orthogonal transforms to eliminate outliers before quantization, using computational invariance: mathbfy mathbfWx (mathbfWQT)(mathbfQx) for orthogonal mathbfQ",
    "type": "semantic",
    "chunk_id": "arxiv_2_chunk_0",
    "source_doc_id": "arxiv_2",
    "source_title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms",
    "source": "arxiv",
    "chunk_index": 0,
    "word_count": 58,
    "char_count": 486,
    "metadata": {
      "original_length": 1790,
      "cleaned_length": 1740,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "However, these methods use fixed transforms--Hadamard matrices achieving optimal worst-case coherence mu 1sqrtn--that cannot adapt to specific weight distributions We identify that different transformer layers exhibit distinct outlier patterns, motivating layer-adaptive rotations rather than one-size-fits-all approaches We propose ButterflyQuant, which replaces Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens rotation angles",
    "type": "semantic",
    "chunk_id": "arxiv_2_chunk_1",
    "source_doc_id": "arxiv_2",
    "source_title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms",
    "source": "arxiv",
    "chunk_index": 1,
    "word_count": 53,
    "char_count": 469,
    "metadata": {
      "original_length": 1790,
      "cleaned_length": 1740,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "Unlike Hadamards discrete 1, -1 entries that are non-differentiable and prohibit gradient-based learning, butterfly transforms continuous parameterization enables smooth optimization while guaranteeing orthogonality by construction This orthogonal constraint ensures theoretical guarantees in outlier suppression while achieving O(n log n) computational complexity with only fracn log n2 learnable parameters",
    "type": "semantic",
    "chunk_id": "arxiv_2_chunk_2",
    "source_doc_id": "arxiv_2",
    "source_title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms",
    "source": "arxiv",
    "chunk_index": 2,
    "word_count": 48,
    "char_count": 408,
    "metadata": {
      "original_length": 1790,
      "cleaned_length": 1740,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "We further introduce a uniformity regularization on post-transformation activations to promote smoother distributions amenable to quantization Learning requires only 128 calibration samples and converges in minutes on a single GPU--a negligible one-time cost On LLaMA-2-7B with 2-bit quantization, ButterflyQuant achieves 15 4 perplexity versus 22 1 for QuaRot",
    "type": "semantic",
    "chunk_id": "arxiv_2_chunk_3",
    "source_doc_id": "arxiv_2",
    "source_title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms",
    "source": "arxiv",
    "chunk_index": 3,
    "word_count": 48,
    "char_count": 360,
    "metadata": {
      "original_length": 1790,
      "cleaned_length": 1740,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift",
    "type": "semantic",
    "chunk_id": "arxiv_3_chunk_0",
    "source_doc_id": "arxiv_3",
    "source_title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
    "source": "arxiv",
    "chunk_index": 0,
    "word_count": 54,
    "char_count": 427,
    "metadata": {
      "original_length": 1508,
      "cleaned_length": 1495,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation",
    "type": "semantic",
    "chunk_id": "arxiv_3_chunk_1",
    "source_doc_id": "arxiv_3",
    "source_title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
    "source": "arxiv",
    "chunk_index": 1,
    "word_count": 63,
    "char_count": 510,
    "metadata": {
      "original_length": 1508,
      "cleaned_length": 1495,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0 on RoboTwin 1 0 with the exploration-enhancing strategies we introduce SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks Moreover, we identify a novel phenomenon pushcut during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process",
    "type": "semantic",
    "chunk_id": "arxiv_3_chunk_2",
    "source_doc_id": "arxiv_3",
    "source_title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
    "source": "arxiv",
    "chunk_index": 2,
    "word_count": 69,
    "char_count": 500,
    "metadata": {
      "original_length": 1508,
      "cleaned_length": 1495,
      "chunking_strategy": "semantic"
    }
  },
  {
    "text": "Github: https:github comPRIME-RLSimpleVLA-RL",
    "type": "semantic",
    "chunk_id": "arxiv_3_chunk_3",
    "source_doc_id": "arxiv_3",
    "source_title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
    "source": "arxiv",
    "chunk_index": 3,
    "word_count": 3,
    "char_count": 44,
    "metadata": {
      "original_length": 1508,
      "cleaned_length": 1495,
      "chunking_strategy": "semantic"
    }
  }
]