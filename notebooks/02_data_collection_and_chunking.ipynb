{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Collection and Exploration: Building the Foundation of RAG Systems\n",
        "\n",
        "## Introduction to Data Collection in RAG Systems\n",
        "\n",
        "In this notebook, we will collect real data from Wikipedia and ArXiv to build our RAG (Retrieval-Augmented Generation) system. This is where we start working with actual documents that our system will need to understand and retrieve from.\n",
        "\n",
        "### Why Data Collection Matters\n",
        "\n",
        "Data collection is the first and most critical step in building any RAG system. The quality and diversity of your data directly impacts:\n",
        "\n",
        "- **Retrieval Accuracy**: Better data leads to more relevant search results\n",
        "- **Response Quality**: Rich, diverse data enables more comprehensive answers\n",
        "- **System Reliability**: Well-structured data ensures consistent performance\n",
        "- **Domain Coverage**: Different data sources provide broader knowledge coverage\n",
        "\n",
        "### Understanding Our Data Sources\n",
        "\n",
        "We'll work with two primary data sources:\n",
        "\n",
        "1. **Wikipedia Articles**: Encyclopedia-style content with structured information\n",
        "2. **ArXiv Papers**: Scientific abstracts with technical, research-focused content\n",
        "\n",
        "Each source has unique characteristics that affect how we process and use the data in our RAG system.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "1. Understand how to collect data using our custom DataCollector\n",
        "2. Explore the structure and characteristics of different data sources\n",
        "3. Learn about data quality and filtering strategies\n",
        "4. Get hands-on experience with real text data\n",
        "5. Understand different chunking strategies for text preprocessing\n",
        "6. Master the art of text preprocessing for RAG systems\n",
        "7. Learn why chunking is essential and how different strategies work\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current directory: /Users/scienceman/Desktop/LLM/notebooks\n",
            "Project root: /Users/scienceman/Desktop/LLM\n",
            "Python path: ['.', '/Users/scienceman/Desktop/LLM/notebooks', '/Users/scienceman/Desktop/LLM']\n",
            "Successfully imported from src module\n",
            "Libraries imported successfully!\n",
            "Data directory: /Users/scienceman/Desktop/LLM/data\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add project root to path - multiple approaches for reliability\n",
        "current_dir = os.getcwd()\n",
        "project_root = os.path.dirname(current_dir) if current_dir.endswith('notebooks') else current_dir\n",
        "\n",
        "# Add both current directory and project root to path\n",
        "sys.path.insert(0, project_root)\n",
        "sys.path.insert(0, current_dir)\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "print(f\"Current directory: {current_dir}\")\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Python path: {sys.path[:3]}\")\n",
        "\n",
        "# Force reload modules to get latest changes\n",
        "import importlib\n",
        "\n",
        "# Import our custom modules with error handling\n",
        "try:\n",
        "    from src.config import DATA_CONFIG, DATA_DIR\n",
        "    from src.data.collect_data import DataCollector\n",
        "    from src.data.preprocess_data import TextPreprocessor\n",
        "    print(\"Successfully imported from src module\")\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    print(\"Trying alternative import methods...\")\n",
        "    \n",
        "    # Try importing directly from the file\n",
        "    try:\n",
        "        import importlib.util\n",
        "        \n",
        "        # Import config\n",
        "        config_path = os.path.join(project_root, 'src', 'config.py')\n",
        "        spec = importlib.util.spec_from_file_location(\"config\", config_path)\n",
        "        config_module = importlib.util.module_from_spec(spec)\n",
        "        spec.loader.exec_module(config_module)\n",
        "        DATA_CONFIG = config_module.DATA_CONFIG\n",
        "        DATA_DIR = config_module.DATA_DIR\n",
        "        \n",
        "        # Import collect_data\n",
        "        collect_path = os.path.join(project_root, 'src', 'data', 'collect_data.py')\n",
        "        spec = importlib.util.spec_from_file_location(\"collect_data\", collect_path)\n",
        "        collect_module = importlib.util.module_from_spec(spec)\n",
        "        spec.loader.exec_module(collect_module)\n",
        "        DataCollector = collect_module.DataCollector\n",
        "        \n",
        "        # Import preprocess_data\n",
        "        preprocess_path = os.path.join(project_root, 'src', 'data', 'preprocess_data.py')\n",
        "        spec = importlib.util.spec_from_file_location(\"preprocess_data\", preprocess_path)\n",
        "        preprocess_module = importlib.util.module_from_spec(spec)\n",
        "        spec.loader.exec_module(preprocess_module)\n",
        "        TextPreprocessor = preprocess_module.TextPreprocessor\n",
        "        \n",
        "        print(\"Successfully imported using direct file imports\")\n",
        "        \n",
        "    except Exception as e2:\n",
        "        print(f\"Direct import also failed: {e2}\")\n",
        "        print(\"Please check that you're running this from the correct directory\")\n",
        "        raise e2\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data.collect_data:Data collector initialized. Output directory: /Users/scienceman/Desktop/LLM/data/raw\n",
            "INFO:src.data.collect_data:Collecting Wikipedia data...\n",
            "INFO:src.data.collect_data:Using sample Wikipedia data...\n",
            "INFO:src.data.collect_data:Collecting ArXiv data...\n",
            "INFO:src.data.collect_data:Using sample ArXiv data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running diagnostics...\n",
            "Current working directory: /Users/scienceman/Desktop/LLM/notebooks\n",
            "Contents of current directory: ['07_llm_integration.ipynb', '05_vector_search.ipynb', '03_embeddings_and_vector_store.ipynb', '08_evaluation.ipynb', '06_retrieval_systems.ipynb', '01_understanding_rag.ipynb', '09_optimization.ipynb', '10_vector_database_production.ipynb', '04_text_preprocessing.ipynb', '02_data_collection.ipynb']\n",
            "Found src directory in parent folder\n",
            "Contents of src/: ['advanced', '.DS_Store', 'config.py', 'vector_db', 'optimization', '__init__.py', 'models', '__pycache__', 'retrieval', 'api', 'evaluation', 'data']\n",
            "Contents of src/data/: ['preprocess_data.py', '__init__.py', '__pycache__', 'collect_data.py']\n",
            "\n",
            "Testing DataCollector method signatures...\n",
            "Wikipedia method parameters: ['self', 'max_documents', 'use_real_data']\n",
            "ArXiv method parameters: ['self', 'max_documents', 'use_real_data']\n",
            "\n",
            "Testing with sample data...\n",
            "Sample Wikipedia test: 1 articles\n",
            "Sample ArXiv test: 1 papers\n",
            "DataCollector methods are working correctly!\n"
          ]
        }
      ],
      "source": [
        "# Diagnostic and Testing Cell\n",
        "print(\"Running diagnostics...\")\n",
        "\n",
        "# Check current working directory and file structure\n",
        "import os\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"Contents of current directory: {os.listdir('.')}\")\n",
        "\n",
        "# Check if src directory exists\n",
        "if os.path.exists('../src'):\n",
        "    print(\"Found src directory in parent folder\")\n",
        "    print(f\"Contents of src/: {os.listdir('../src')}\")\n",
        "    if os.path.exists('../src/data'):\n",
        "        print(f\"Contents of src/data/: {os.listdir('../src/data')}\")\n",
        "else:\n",
        "    print(\"src directory not found in parent folder\")\n",
        "\n",
        "# Test the DataCollector method signatures\n",
        "print(\"\\nTesting DataCollector method signatures...\")\n",
        "\n",
        "try:\n",
        "    import inspect\n",
        "    \n",
        "    # Check Wikipedia method\n",
        "    wiki_sig = inspect.signature(DataCollector.collect_wikipedia_data)\n",
        "    print(f\"Wikipedia method parameters: {list(wiki_sig.parameters.keys())}\")\n",
        "    \n",
        "    # Check ArXiv method  \n",
        "    arxiv_sig = inspect.signature(DataCollector.collect_arxiv_data)\n",
        "    print(f\"ArXiv method parameters: {list(arxiv_sig.parameters.keys())}\")\n",
        "    \n",
        "    # Test with sample data (should work)\n",
        "    print(\"\\nTesting with sample data...\")\n",
        "    test_collector = DataCollector()\n",
        "    test_wiki = test_collector.collect_wikipedia_data(max_documents=1, use_real_data=False)\n",
        "    print(f\"Sample Wikipedia test: {len(test_wiki)} articles\")\n",
        "    \n",
        "    test_arxiv = test_collector.collect_arxiv_data(max_documents=1, use_real_data=False)\n",
        "    print(f\"Sample ArXiv test: {len(test_arxiv)} papers\")\n",
        "    \n",
        "    print(\"DataCollector methods are working correctly!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error during testing: {e}\")\n",
        "    print(\"This might indicate an import or configuration issue.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Our Data Sources\n",
        "\n",
        "We'll work with two primary data sources:\n",
        "\n",
        "1. **Wikipedia Articles**: Encyclopedia-style content with structured information\n",
        "2. **ArXiv Papers**: Scientific abstracts with technical, research-focused content\n",
        "\n",
        "Each source has unique characteristics that affect how we process and use the data in our RAG system.\n",
        "\n",
        "Let us start by collecting a small sample of each to explore their characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data.collect_data:Data collector initialized. Output directory: /Users/scienceman/Desktop/LLM/data/raw\n",
            "INFO:src.data.collect_data:Collecting Wikipedia data...\n",
            "INFO:src.data.collect_data:Fetching real Wikipedia data using Wikipedia API...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataCollector initialized successfully!\n",
            "\n",
            "Data collection mode: Real data from APIs\n",
            "\n",
            "Collecting Wikipedia data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data.collect_data:Collected: Machine learning\n",
            "INFO:src.data.collect_data:Collected: Artificial intelligence\n",
            "INFO:src.data.collect_data:Collected: Deep learning\n",
            "INFO:src.data.collect_data:Collected: Natural language processing\n",
            "INFO:src.data.collect_data:Collected: Computer vision\n",
            "INFO:src.data.collect_data:Successfully collected 5 Wikipedia articles\n",
            "INFO:src.data.collect_data:Collecting ArXiv data...\n",
            "INFO:src.data.collect_data:Fetching real ArXiv data using ArXiv API...\n",
            "INFO:src.data.collect_data:Collected: GC-VLN: Instruction as Graph Constraints for Training-free\n",
            "  Vision-and-Language Navigation\n",
            "INFO:src.data.collect_data:Collected: SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and\n",
            "  Adaptability Across Alzheimer's Prediction Tasks and Datasets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collected 5 Wikipedia articles\n",
            "\n",
            "Collecting ArXiv data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data.collect_data:Collected: SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and\n",
            "  Adaptability Across Alzheimer's Prediction Tasks and Datasets\n",
            "INFO:src.data.collect_data:Collected: WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained\n",
            "  Speech Recognition Transformers\n",
            "INFO:src.data.collect_data:Successfully collected 4 ArXiv papers\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collected 4 ArXiv papers\n",
            "\n",
            "Total documents collected: 9\n",
            "\n",
            "Sample Wikipedia article:\n",
            "  Title: Machine learning\n",
            "  Word count: 68\n",
            "  Preview: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "  URL: https://en.wikipedia.org/wiki/Machine_learning\n",
            "\n",
            "Sample ArXiv paper:\n",
            "  Title: GC-VLN: Instruction as Graph Constraints for Training-free\n",
            "  Vision-and-Language Navigation\n",
            "  Word count: 221\n",
            "  Authors: ['Hang Yin', 'Haoyu Wei', 'Xiuwei Xu', 'Wenxuan Guo', 'Jie Zhou', 'Jiwen Lu']\n",
            "  Categories: ['cs.RO', 'cs.CV']\n",
            "  Preview: In this paper, we propose a training-free framework for vision-and-language\n",
            "navigation (VLN). Existi...\n",
            "  ArXiv ID: 2509.10454v1\n"
          ]
        }
      ],
      "source": [
        "# Initialize our data collector\n",
        "collector = DataCollector()\n",
        "print(\"DataCollector initialized successfully!\")\n",
        "\n",
        "# Choose data collection mode\n",
        "USE_REAL_DATA = True  # Set to False to use sample data, True to fetch real data from APIs\n",
        "\n",
        "print(f\"\\nData collection mode: {'Real data from APIs' if USE_REAL_DATA else 'Sample data'}\")\n",
        "\n",
        "# Collect Wikipedia data with error handling\n",
        "print(\"\\nCollecting Wikipedia data...\")\n",
        "try:\n",
        "    wiki_data = collector.collect_wikipedia_data(max_documents=5, use_real_data=USE_REAL_DATA)\n",
        "    print(f\"Collected {len(wiki_data)} Wikipedia articles\")\n",
        "except TypeError as e:\n",
        "    if \"unexpected keyword argument\" in str(e):\n",
        "        print(\"Using fallback method (old API signature)\")\n",
        "        wiki_data = collector.collect_wikipedia_data(max_documents=5)\n",
        "        print(f\"Collected {len(wiki_data)} Wikipedia articles (sample data)\")\n",
        "    else:\n",
        "        raise e\n",
        "except Exception as e:\n",
        "    print(f\"Error collecting Wikipedia data: {e}\")\n",
        "    print(\"Falling back to sample data...\")\n",
        "    wiki_data = collector.collect_wikipedia_data(max_documents=5, use_real_data=False)\n",
        "\n",
        "# Collect ArXiv data with error handling\n",
        "print(\"\\nCollecting ArXiv data...\")\n",
        "try:\n",
        "    arxiv_data = collector.collect_arxiv_data(max_documents=5, use_real_data=USE_REAL_DATA)\n",
        "    print(f\"Collected {len(arxiv_data)} ArXiv papers\")\n",
        "except TypeError as e:\n",
        "    if \"unexpected keyword argument\" in str(e):\n",
        "        print(\"Using fallback method (old API signature)\")\n",
        "        arxiv_data = collector.collect_arxiv_data(max_documents=5)\n",
        "        print(f\"Collected {len(arxiv_data)} ArXiv papers (sample data)\")\n",
        "    else:\n",
        "        raise e\n",
        "except Exception as e:\n",
        "    print(f\"Error collecting ArXiv data: {e}\")\n",
        "    print(\"Falling back to sample data...\")\n",
        "    arxiv_data = collector.collect_arxiv_data(max_documents=5, use_real_data=False)\n",
        "\n",
        "print(f\"\\nTotal documents collected: {len(wiki_data) + len(arxiv_data)}\")\n",
        "\n",
        "# Display sample data to verify collection\n",
        "if wiki_data:\n",
        "    print(f\"\\nSample Wikipedia article:\")\n",
        "    print(f\"  Title: {wiki_data[0]['title']}\")\n",
        "    print(f\"  Word count: {wiki_data[0]['word_count']}\")\n",
        "    print(f\"  Preview: {wiki_data[0]['text'][:100]}...\")\n",
        "    if 'url' in wiki_data[0]:\n",
        "        print(f\"  URL: {wiki_data[0]['url']}\")\n",
        "\n",
        "if arxiv_data:\n",
        "    print(f\"\\nSample ArXiv paper:\")\n",
        "    print(f\"  Title: {arxiv_data[0]['title']}\")\n",
        "    print(f\"  Word count: {arxiv_data[0]['word_count']}\")\n",
        "    print(f\"  Authors: {arxiv_data[0]['authors']}\")\n",
        "    print(f\"  Categories: {arxiv_data[0]['categories']}\")\n",
        "    print(f\"  Preview: {arxiv_data[0]['abstract'][:100]}...\")\n",
        "    if 'arxiv_id' in arxiv_data[0]:\n",
        "        print(f\"  ArXiv ID: {arxiv_data[0]['arxiv_id']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative Data Collection Methods\n",
        "\n",
        "We've implemented several data collection methods to ensure our system works in different scenarios:\n",
        "\n",
        "### 1. **Wikipedia API** (Recommended)\n",
        "- Free and reliable with no authentication required\n",
        "- Real-time data with rich metadata\n",
        "- Built-in rate limiting for respectful usage\n",
        "\n",
        "### 2. **ArXiv API**\n",
        "- Scientific papers from the ArXiv preprint server\n",
        "- Recent publications with authors, categories, and abstracts\n",
        "- Structured data from ArXiv's Atom feed\n",
        "\n",
        "### 3. **Sample Data Fallback**\n",
        "- Offline mode for development and testing\n",
        "- Consistent data for reproducible results\n",
        "- Fast execution without API delays\n",
        "\n",
        "### Usage Options:\n",
        "- Set `USE_REAL_DATA = True` to fetch real data from APIs\n",
        "- Set `USE_REAL_DATA = False` to use sample data\n",
        "- The system automatically falls back to sample data if API calls fail\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring Wikipedia Data Structure\n",
        "\n",
        "Let us examine the structure and characteristics of our Wikipedia articles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wikipedia Data Structure:\n",
            "==================================================\n",
            "Article keys: ['id', 'title', 'text', 'source', 'length', 'word_count', 'url', 'description']\n",
            "\n",
            "First article:\n",
            "  Title: Machine learning\n",
            "  Source: wikipedia\n",
            "  Length: 462 characters\n",
            "  Word count: 68 words\n",
            "  Text preview: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus...\n",
            "\n",
            "All Wikipedia articles:\n",
            "  1. Machine learning (68 words)\n",
            "  2. Artificial intelligence (64 words)\n",
            "  3. Deep learning (64 words)\n",
            "  4. Natural language processing (44 words)\n",
            "  5. Computer vision (90 words)\n",
            "\n",
            "Wikipedia Statistics:\n",
            "  Total articles: 5\n",
            "  Average words per article: 66.0\n",
            "  Min words: 44\n",
            "  Max words: 90\n"
          ]
        }
      ],
      "source": [
        "print(\"Wikipedia Data Structure:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if wiki_data:\n",
        "    # Show structure of first article\n",
        "    first_article = wiki_data[0]\n",
        "    print(f\"Article keys: {list(first_article.keys())}\")\n",
        "    print(f\"\\nFirst article:\")\n",
        "    print(f\"  Title: {first_article['title']}\")\n",
        "    print(f\"  Source: {first_article['source']}\")\n",
        "    print(f\"  Length: {first_article['length']} characters\")\n",
        "    print(f\"  Word count: {first_article['word_count']} words\")\n",
        "    print(f\"  Text preview: {first_article['text'][:200]}...\")\n",
        "    \n",
        "    # Show all articles\n",
        "    print(f\"\\nAll Wikipedia articles:\")\n",
        "    for i, article in enumerate(wiki_data):\n",
        "        print(f\"  {i+1}. {article['title']} ({article['word_count']} words)\")\n",
        "    \n",
        "    # Show statistics\n",
        "    word_counts = [article['word_count'] for article in wiki_data]\n",
        "    print(f\"\\nWikipedia Statistics:\")\n",
        "    print(f\"  Total articles: {len(wiki_data)}\")\n",
        "    print(f\"  Average words per article: {sum(word_counts) / len(word_counts):.1f}\")\n",
        "    print(f\"  Min words: {min(word_counts)}\")\n",
        "    print(f\"  Max words: {max(word_counts)}\")\n",
        "else:\n",
        "    print(\"No Wikipedia data collected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring ArXiv Data Structure\n",
        "\n",
        "Now let us examine the structure and characteristics of our ArXiv papers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ArXiv Data Structure:\n",
            "==================================================\n",
            "Paper keys: ['id', 'title', 'abstract', 'source', 'length', 'word_count', 'authors', 'categories', 'published', 'arxiv_id']\n",
            "\n",
            "First paper:\n",
            "  Title: GC-VLN: Instruction as Graph Constraints for Training-free\n",
            "  Vision-and-Language Navigation\n",
            "  Source: arxiv\n",
            "  Length: 1657 characters\n",
            "  Word count: 221 words\n",
            "  Authors: ['Hang Yin', 'Haoyu Wei', 'Xiuwei Xu', 'Wenxuan Guo', 'Jie Zhou', 'Jiwen Lu']\n",
            "  Categories: ['cs.RO', 'cs.CV']\n",
            "  Abstract preview: In this paper, we propose a training-free framework for vision-and-language\n",
            "navigation (VLN). Existing zero-shot VLN methods are mainly designed for\n",
            "discrete environments or involve unsupervised train...\n",
            "\n",
            "All ArXiv papers:\n",
            "  1. GC-VLN: Instruction as Graph Constraints for Training-free\n",
            "  Vision-and-Language Navigation (221 words)\n",
            "  2. SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and\n",
            "  Adaptability Across Alzheimer's Prediction Tasks and Datasets (175 words)\n",
            "  3. SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and\n",
            "  Adaptability Across Alzheimer's Prediction Tasks and Datasets (175 words)\n",
            "  4. WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained\n",
            "  Speech Recognition Transformers (119 words)\n",
            "\n",
            "ArXiv Statistics:\n",
            "  Total papers: 4\n",
            "  Average words per abstract: 172.5\n",
            "  Min words: 119\n",
            "  Max words: 221\n"
          ]
        }
      ],
      "source": [
        "print(\"ArXiv Data Structure:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if arxiv_data:\n",
        "    # Show structure of first paper\n",
        "    first_paper = arxiv_data[0]\n",
        "    print(f\"Paper keys: {list(first_paper.keys())}\")\n",
        "    print(f\"\\nFirst paper:\")\n",
        "    print(f\"  Title: {first_paper['title']}\")\n",
        "    print(f\"  Source: {first_paper['source']}\")\n",
        "    print(f\"  Length: {first_paper['length']} characters\")\n",
        "    print(f\"  Word count: {first_paper['word_count']} words\")\n",
        "    print(f\"  Authors: {first_paper['authors']}\")\n",
        "    print(f\"  Categories: {first_paper['categories']}\")\n",
        "    print(f\"  Abstract preview: {first_paper['abstract'][:200]}...\")\n",
        "    \n",
        "    # Show all papers\n",
        "    print(f\"\\nAll ArXiv papers:\")\n",
        "    for i, paper in enumerate(arxiv_data):\n",
        "        print(f\"  {i+1}. {paper['title']} ({paper['word_count']} words)\")\n",
        "    \n",
        "    # Show statistics\n",
        "    word_counts = [paper['word_count'] for paper in arxiv_data]\n",
        "    print(f\"\\nArXiv Statistics:\")\n",
        "    print(f\"  Total papers: {len(arxiv_data)}\")\n",
        "    print(f\"  Average words per abstract: {sum(word_counts) / len(word_counts):.1f}\")\n",
        "    print(f\"  Min words: {min(word_counts)}\")\n",
        "    print(f\"  Max words: {max(word_counts)}\")\n",
        "else:\n",
        "    print(\"No ArXiv data collected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Preprocessing and Chunking: The Foundation of RAG Systems\n",
        "\n",
        "### Why Do We Need Chunking?\n",
        "\n",
        "Before we dive into the technical details, let's understand why chunking is absolutely essential for RAG (Retrieval-Augmented Generation) systems:\n",
        "\n",
        "#### 1. **The Context Window Problem**\n",
        "- **LLM Limitations**: Large Language Models have a limited \"context window\" - they can only process a certain amount of text at once\n",
        "- **Example**: GPT-3.5 can handle ~4,000 tokens, GPT-4 can handle ~8,000-32,000 tokens\n",
        "- **Reality Check**: A single Wikipedia article can easily exceed 10,000 words, which is way beyond most model limits\n",
        "- **Solution**: Break large documents into smaller, manageable \"chunks\"\n",
        "\n",
        "#### 2. **Precision in Retrieval**\n",
        "- **The Needle in Haystack Problem**: When you ask \"What is machine learning?\", you don't want the entire Wikipedia article about AI\n",
        "- **Targeted Retrieval**: You want the specific section that explains machine learning concepts\n",
        "- **Better Matches**: Smaller chunks allow for more precise matching between user queries and relevant content\n",
        "\n",
        "#### 3. **Computational Efficiency**\n",
        "- **Embedding Costs**: Each chunk needs to be converted to a vector (embedding) for similarity search\n",
        "- **Storage Optimization**: Smaller chunks mean more efficient storage and faster retrieval\n",
        "- **Processing Speed**: Faster similarity calculations with smaller, focused pieces of text\n",
        "\n",
        "#### 4. **Semantic Coherence**\n",
        "- **Preserving Meaning**: Good chunking keeps related concepts together\n",
        "- **Avoiding Fragmentation**: We don't want to split a sentence or paragraph in the middle\n",
        "- **Context Preservation**: Each chunk should be meaningful on its own\n",
        "\n",
        "### How Chunking Works: The Technical Deep Dive\n",
        "\n",
        "Let's explore the different strategies our `TextPreprocessor` uses:\n",
        "\n",
        "#### 1. **Fixed Chunking (Simple but Effective)**\n",
        "```python\n",
        "# How it works:\n",
        "# - Split text into chunks of exactly N characters\n",
        "# - No overlap between chunks\n",
        "# - Simple and predictable\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        "- Simple to implement and understand\n",
        "- Consistent chunk sizes\n",
        "- Fast processing\n",
        "- Predictable behavior\n",
        "\n",
        "**Cons:**\n",
        "- Can break sentences mid-way\n",
        "- May lose important context at boundaries\n",
        "- Not semantically aware\n",
        "\n",
        "**When to Use:**\n",
        "- When you need consistent chunk sizes\n",
        "- For simple documents with uniform structure\n",
        "- When processing speed is critical\n",
        "\n",
        "#### 2. **Semantic Chunking (Smart and Context-Aware)**\n",
        "```python\n",
        "# How it works:\n",
        "# - Analyzes text structure (sentences, paragraphs)\n",
        "# - Tries to keep related concepts together\n",
        "# - Uses natural language boundaries\n",
        "# - May have variable chunk sizes\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        "- Preserves semantic meaning\n",
        "- Keeps sentences and paragraphs intact\n",
        "- Better for complex documents\n",
        "- More natural text boundaries\n",
        "\n",
        "**Cons:**\n",
        "- More complex to implement\n",
        "- Variable chunk sizes\n",
        "- Slower processing\n",
        "- May create very small or very large chunks\n",
        "\n",
        "**When to Use:**\n",
        "- For complex, technical documents\n",
        "- When semantic coherence is critical\n",
        "- For documents with varied structure\n",
        "\n",
        "#### 3. **Hierarchical Chunking (Multi-Level Organization)**\n",
        "```python\n",
        "# How it works:\n",
        "# - Creates multiple levels of chunks (e.g., sections, paragraphs, sentences)\n",
        "# - Maintains relationships between different levels\n",
        "# - Allows for flexible retrieval strategies\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        "- Maintains document structure\n",
        "- Flexible retrieval options\n",
        "- Preserves relationships between concepts\n",
        "- Good for structured documents\n",
        "\n",
        "**Cons:**\n",
        "- Most complex to implement\n",
        "- Requires understanding of document structure\n",
        "- More storage overhead\n",
        "- Complex retrieval logic\n",
        "\n",
        "**When to Use:**\n",
        "- For highly structured documents (papers, reports)\n",
        "- When you need multiple levels of detail\n",
        "- For complex knowledge bases\n",
        "\n",
        "### The TextPreprocessor: A Deep Dive into Implementation\n",
        "\n",
        "Let's examine how our `TextPreprocessor` class works internally:\n",
        "\n",
        "#### Key Methods and Their Purposes:\n",
        "\n",
        "1. **`chunk_document(doc, strategy='semantic')`**\n",
        "   - Main entry point for chunking\n",
        "   - Takes a document and returns a list of chunks\n",
        "   - Each chunk contains: text, metadata, source information\n",
        "\n",
        "2. **`_chunk_by_fixed_size(text, chunk_size=500)`**\n",
        "   - Implements fixed-size chunking\n",
        "   - Splits text into equal-sized pieces\n",
        "   - Handles edge cases (very short documents)\n",
        "\n",
        "3. **`_chunk_by_semantic_boundaries(text, max_chunk_size=500)`**\n",
        "   - Implements semantic chunking\n",
        "   - Uses sentence and paragraph boundaries\n",
        "   - Tries to preserve meaning\n",
        "\n",
        "4. **`_chunk_hierarchically(text, max_chunk_size=500)`**\n",
        "   - Implements hierarchical chunking\n",
        "   - Creates multiple levels of organization\n",
        "   - Maintains document structure\n",
        "\n",
        "#### What Happens During Preprocessing:\n",
        "\n",
        "1. **Text Cleaning**: Remove extra whitespace, normalize text\n",
        "2. **Boundary Detection**: Find sentence and paragraph boundaries\n",
        "3. **Chunk Creation**: Split text according to chosen strategy\n",
        "4. **Metadata Addition**: Add source, position, and other metadata\n",
        "5. **Quality Checks**: Ensure chunks meet minimum requirements\n",
        "\n",
        "### Understanding the Final Data Structure\n",
        "\n",
        "After preprocessing, each chunk looks like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"id\": \"unique_chunk_identifier\",\n",
        "  \"text\": \"The actual text content of the chunk\",\n",
        "  \"source\": \"wikipedia\" or \"arxiv\",\n",
        "  \"document_id\": \"original_document_id\",\n",
        "  \"chunk_index\": 0,\n",
        "  \"metadata\": {\n",
        "    \"title\": \"Original document title\",\n",
        "    \"word_count\": 150,\n",
        "    \"char_count\": 800,\n",
        "    \"chunk_type\": \"semantic\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Best Practices for Chunking\n",
        "\n",
        "1. **Choose the Right Strategy**:\n",
        "   - Use fixed chunking for simple, uniform documents\n",
        "   - Use semantic chunking for complex, varied content\n",
        "   - Use hierarchical chunking for structured documents\n",
        "\n",
        "2. **Optimal Chunk Size**:\n",
        "   - Too small: Loses context, creates too many chunks\n",
        "   - Too large: Exceeds context limits, less precise retrieval\n",
        "   - Sweet spot: 200-800 characters (varies by use case)\n",
        "\n",
        "3. **Overlap Considerations**:\n",
        "   - Some chunking strategies use overlap to preserve context\n",
        "   - Overlap helps with boundary issues\n",
        "   - But increases storage and processing costs\n",
        "\n",
        "4. **Metadata Preservation**:\n",
        "   - Always keep track of source document\n",
        "   - Maintain position information\n",
        "   - Include relevant metadata for filtering\n",
        "\n",
        "Now let us learn about different chunking strategies for preprocessing our text data. Chunking is crucial for RAG systems as it determines how we break down large documents into manageable pieces for retrieval.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the Chunking Results\n",
        "\n",
        "The chunking demonstration shows that all three strategies produced similar results because our demo document (462 characters) is smaller than the default chunk size (500+ characters). This teaches us that chunking is context-dependent and strategy selection matters more with larger documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data.preprocess_data:Text preprocessor initialized. Output directory: /Users/scienceman/Desktop/LLM/data/processed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TextPreprocessor initialized successfully!\n",
            "\n",
            "Demo document: Machine learning\n",
            "Original text length: 462 characters\n",
            "Original word count: 68 words\n",
            "\n",
            "Original text preview:\n",
            "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances i...\n"
          ]
        }
      ],
      "source": [
        "# Initialize our text preprocessor\n",
        "preprocessor = TextPreprocessor()\n",
        "print(\"TextPreprocessor initialized successfully!\")\n",
        "\n",
        "# Let us use one of our documents for demonstration\n",
        "if wiki_data:\n",
        "    demo_doc = wiki_data[0]\n",
        "    print(f\"\\nDemo document: {demo_doc['title']}\")\n",
        "    print(f\"Original text length: {len(demo_doc['text'])} characters\")\n",
        "    print(f\"Original word count: {demo_doc['word_count']} words\")\n",
        "    print(f\"\\nOriginal text preview:\")\n",
        "    print(demo_doc['text'][:300] + \"...\")\n",
        "elif arxiv_data:\n",
        "    demo_doc = arxiv_data[0]\n",
        "    print(f\"\\nDemo document: {demo_doc['title']}\")\n",
        "    print(f\"Original text length: {len(demo_doc['abstract'])} characters\")\n",
        "    print(f\"Original word count: {demo_doc['word_count']} words\")\n",
        "    print(f\"\\nOriginal text preview:\")\n",
        "    print(demo_doc['abstract'][:300] + \"...\")\n",
        "else:\n",
        "    print(\"No data available for demonstration\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Wikipedia article: Machine learning\n",
            "Comparing Chunking Strategies:\n",
            "==================================================\n",
            "\n",
            "FIXED Chunking:\n",
            "  Number of chunks: 1\n",
            "  First chunk length: 462 characters\n",
            "  First chunk preview: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn...\n",
            "  Chunk length stats: min=462, max=462, avg=462.0\n",
            "\n",
            "SEMANTIC Chunking:\n",
            "  Number of chunks: 1\n",
            "  First chunk length: 460 characters\n",
            "  First chunk preview: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn...\n",
            "  Chunk length stats: min=460, max=460, avg=460.0\n",
            "\n",
            "HIERARCHICAL Chunking:\n",
            "  Number of chunks: 1\n",
            "  First chunk length: 462 characters\n",
            "  First chunk preview: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn...\n",
            "  Chunk length stats: min=462, max=462, avg=462.0\n"
          ]
        }
      ],
      "source": [
        "# Compare different chunking strategies\n",
        "if wiki_data or arxiv_data:\n",
        "    # Use Wikipedia data if available, otherwise ArXiv\n",
        "    if wiki_data:\n",
        "        demo_doc = wiki_data[0]\n",
        "        print(f\"Using Wikipedia article: {demo_doc['title']}\")\n",
        "    else:\n",
        "        demo_doc = arxiv_data[0]\n",
        "        print(f\"Using ArXiv paper: {demo_doc['title']}\")\n",
        "    \n",
        "    print(\"Comparing Chunking Strategies:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    strategies = ['fixed', 'semantic', 'hierarchical']\n",
        "    \n",
        "    for strategy in strategies:\n",
        "        print(f\"\\n{strategy.upper()} Chunking:\")\n",
        "        chunks = preprocessor.chunk_document(demo_doc, strategy=strategy)\n",
        "        print(f\"  Number of chunks: {len(chunks)}\")\n",
        "        \n",
        "        if chunks:\n",
        "            # Show first chunk\n",
        "            first_chunk = chunks[0]\n",
        "            print(f\"  First chunk length: {len(first_chunk['text'])} characters\")\n",
        "            print(f\"  First chunk preview: {first_chunk['text'][:150]}...\")\n",
        "            \n",
        "            # Show chunk statistics\n",
        "            chunk_lengths = [len(chunk['text']) for chunk in chunks]\n",
        "            print(f\"  Chunk length stats: min={min(chunk_lengths)}, max={max(chunk_lengths)}, avg={np.mean(chunk_lengths):.1f}\")\n",
        "        else:\n",
        "            print(\"  No chunks created (document too short)\")\n",
        "else:\n",
        "    print(\"No data available for chunking demonstration\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing All Collected Data\n",
        "\n",
        "Now let us process all our collected data using our preferred chunking strategy and save it for use in the next notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing all collected documents...\n",
            "\n",
            "Processing 5 Wikipedia articles...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Wikipedia: 100%|| 5/5 [00:00<00:00, 6545.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing 4 ArXiv papers...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ArXiv: 100%|| 4/4 [00:00<00:00, 3268.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total chunks created: 18\n",
            "Chunk length statistics:\n",
            "  Min: 189 characters\n",
            "  Max: 494 characters\n",
            "  Average: 415.5 characters\n",
            "  Median: 439.0 characters\n",
            "\n",
            "Chunks by source:\n",
            "  wikipedia: 6 chunks\n",
            "  arxiv: 12 chunks\n",
            "\n",
            "Sample chunks:\n",
            "  Chunk 1 (wikipedia): Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "  Chunk 2 (wikipedia): Artificial intelligence (AI) is the capability of computational systems to perform tasks typically a...\n",
            "  Chunk 3 (wikipedia): In machine learning, deep learning focuses on utilizing multilayered neural networks to perform task...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Process all documents\n",
        "print(\"Processing all collected documents...\")\n",
        "\n",
        "all_chunks = []\n",
        "\n",
        "# Process Wikipedia articles\n",
        "print(f\"\\nProcessing {len(wiki_data)} Wikipedia articles...\")\n",
        "for i, article in enumerate(tqdm(wiki_data, desc=\"Wikipedia\")):\n",
        "    chunks = preprocessor.chunk_document(article, strategy='semantic')\n",
        "    all_chunks.extend(chunks)\n",
        "\n",
        "# Process ArXiv papers\n",
        "print(f\"\\nProcessing {len(arxiv_data)} ArXiv papers...\")\n",
        "for i, paper in enumerate(tqdm(arxiv_data, desc=\"ArXiv\")):\n",
        "    # Convert ArXiv format to expected format\n",
        "    doc = {\n",
        "        'id': paper['id'],\n",
        "        'title': paper['title'],\n",
        "        'text': paper['abstract'],\n",
        "        'source': paper['source']\n",
        "    }\n",
        "    chunks = preprocessor.chunk_document(doc, strategy='semantic')\n",
        "    all_chunks.extend(chunks)\n",
        "\n",
        "print(f\"\\nTotal chunks created: {len(all_chunks)}\")\n",
        "\n",
        "# Show chunk statistics\n",
        "if all_chunks:\n",
        "    chunk_lengths = [len(chunk['text']) for chunk in all_chunks]\n",
        "    print(f\"Chunk length statistics:\")\n",
        "    print(f\"  Min: {min(chunk_lengths)} characters\")\n",
        "    print(f\"  Max: {max(chunk_lengths)} characters\")\n",
        "    print(f\"  Average: {np.mean(chunk_lengths):.1f} characters\")\n",
        "    print(f\"  Median: {np.median(chunk_lengths):.1f} characters\")\n",
        "    \n",
        "    # Show sources\n",
        "    sources = [chunk['source'] for chunk in all_chunks]\n",
        "    source_counts = Counter(sources)\n",
        "    print(f\"\\nChunks by source:\")\n",
        "    for source, count in source_counts.items():\n",
        "        print(f\"  {source}: {count} chunks\")\n",
        "    \n",
        "    # Show sample chunks\n",
        "    print(f\"\\nSample chunks:\")\n",
        "    for i, chunk in enumerate(all_chunks[:3]):  # Show first 3 chunks\n",
        "        print(f\"  Chunk {i+1} ({chunk['source']}): {chunk['text'][:100]}...\")\n",
        "else:\n",
        "    print(\"No chunks were created. This might indicate an issue with the chunking process.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving processed data...\n",
            "Saved 18 chunks to: /Users/scienceman/Desktop/LLM/data/processed/all_chunks.json\n",
            "\n",
            "Data processing complete!\n"
          ]
        }
      ],
      "source": [
        "# Save processed data\n",
        "print(\"Saving processed data...\")\n",
        "\n",
        "# Save chunks\n",
        "chunks_file = DATA_DIR / \"processed\" / \"all_chunks.json\"\n",
        "chunks_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(chunks_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(all_chunks, f, indent=2)\n",
        "\n",
        "print(f\"Saved {len(all_chunks)} chunks to: {chunks_file}\")\n",
        "\n",
        "print(\"\\nData processing complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Our Final Processed Data\n",
        "\n",
        "Let's examine what we've created and understand the transformation from raw documents to RAG-ready chunks:\n",
        "\n",
        "### The Data Transformation Journey\n",
        "\n",
        "```\n",
        "Raw Documents → Text Preprocessing → Chunks → Vector Embeddings → Vector Database\n",
        "     ↓              ↓                ↓           ↓                ↓\n",
        "  Wikipedia      Cleaning &        Semantic    Numerical        Searchable\n",
        "  Articles       Chunking          Chunks      Vectors          Database\n",
        "  ArXiv Papers\n",
        "```\n",
        "\n",
        "### What Our Final Data Looks Like\n",
        "\n",
        "After processing, we have **18 chunks** from our 9 original documents:\n",
        "\n",
        "- **Wikipedia**: 6 chunks (from 5 articles)\n",
        "- **ArXiv**: 12 chunks (from 4 papers)\n",
        "- **Chunk Size Range**: 189-494 characters\n",
        "- **Average Size**: 415.5 characters\n",
        "\n",
        "### Why ArXiv Produced More Chunks\n",
        "\n",
        "ArXiv abstracts are typically longer and more complex than Wikipedia article previews, so semantic chunking created more meaningful segments from the technical content.\n",
        "\n",
        "### Understanding the Chunk Structure\n",
        "\n",
        "Each chunk contains:\n",
        "- **Unique ID**: For tracking and retrieval\n",
        "- **Text Content**: The actual chunk text\n",
        "- **Source Information**: Which document it came from\n",
        "- **Metadata**: Word count, character count, chunk type\n",
        "- **Position Data**: Where it appears in the original document\n",
        "\n",
        "### Why This Structure Matters for RAG\n",
        "\n",
        "1. **Retrieval Efficiency**: Each chunk can be independently searched\n",
        "2. **Context Preservation**: Metadata helps maintain document context\n",
        "3. **Source Tracking**: We know where each piece of information came from\n",
        "4. **Scalability**: This structure works for millions of chunks\n",
        "\n",
        "## Summary\n",
        "\n",
        "In this notebook, we have learned:\n",
        "\n",
        "1. **Data Collection**: How to collect data from different sources using our custom DataCollector\n",
        "2. **Data Exploration**: How to analyze the structure and characteristics of our collected data\n",
        "3. **Text Preprocessing**: How to chunk documents using different strategies (fixed, semantic, hierarchical)\n",
        "4. **Data Processing**: How to process and save data for use in subsequent notebooks\n",
        "5. **Chunking Deep Dive**: Understanding why chunking is essential and how different strategies work\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "- **Wikipedia articles** tend to be longer and more comprehensive\n",
        "- **ArXiv abstracts** are more concise and technical\n",
        "- **Semantic chunking** works well for preserving meaning while creating manageable pieces\n",
        "- **Data preprocessing** is crucial for effective RAG systems\n",
        "- **Chunking strategy** depends on document type and use case\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "In the next notebook, we will learn how to convert these text chunks into embeddings and build vector stores for efficient similarity search.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
