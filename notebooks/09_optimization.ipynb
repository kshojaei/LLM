{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Performance Optimization and Deployment\n",
        "\n",
        "In this notebook, we'll learn how to optimize RAG systems for production deployment and analyze performance trade-offs.\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this notebook, you will:\n",
        "1. Optimize RAG systems for speed and memory usage\n",
        "2. Analyze latency and cost trade-offs\n",
        "3. Implement caching and batch processing\n",
        "4. Learn about deployment strategies and scaling\n",
        "5. Monitor and debug production RAG systems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Let's import the libraries we need for performance optimization and analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import psutil\n",
        "import threading\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# Add project root to path\n",
        "import sys\n",
        "sys.path.append(str(Path.cwd().parent))\n",
        "\n",
        "# Import our modules\n",
        "from src.optimization.performance_analysis import PerformanceProfiler, CostAnalyzer, LatencyAnalyzer, PerformanceOptimizer\n",
        "from src.retrieval.retrieval_system import RetrievalSystem, RetrievalConfig\n",
        "from src.models.llm_models import RAGGenerator, PromptTemplate\n",
        "from src.config import DATA_DIR\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "\n",
        "# Create sample data for performance testing\n",
        "print(\"Creating sample data for performance testing...\")\n",
        "\n",
        "sample_chunks = [\n",
        "    {\n",
        "        'id': f'chunk_{i}',\n",
        "        'text': f'This is sample document {i} with some content about machine learning and artificial intelligence.',\n",
        "        'title': f'Document {i}',\n",
        "        'source': 'test',\n",
        "        'chunk_id': f'chunk_{i}'\n",
        "    }\n",
        "    for i in range(100)  # Create 100 sample chunks\n",
        "]\n",
        "\n",
        "print(f\"Created {len(sample_chunks)} sample chunks for performance testing\")\n",
        "\n",
        "# Initialize performance monitoring\n",
        "profiler = PerformanceProfiler()\n",
        "cost_analyzer = CostAnalyzer()\n",
        "latency_analyzer = LatencyAnalyzer()\n",
        "\n",
        "print(\"Performance monitoring tools initialized!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Profiling\n",
        "\n",
        "Let's profile our RAG system to identify bottlenecks and optimization opportunities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a retrieval system for performance testing\n",
        "print(\"Setting up retrieval system for performance testing...\")\n",
        "retrieval_config = RetrievalConfig(top_k=5, use_reranking=False)\n",
        "retrieval_system = RetrievalSystem(retrieval_config)\n",
        "retrieval_system.add_documents(sample_chunks)\n",
        "\n",
        "# Test queries for performance evaluation\n",
        "test_queries = [\n",
        "    \"What is machine learning?\",\n",
        "    \"How does artificial intelligence work?\",\n",
        "    \"Explain deep learning\",\n",
        "    \"What are neural networks?\",\n",
        "    \"Tell me about computer vision\"\n",
        "]\n",
        "\n",
        "print(f\"Created retrieval system with {len(sample_chunks)} documents\")\n",
        "print(f\"Prepared {len(test_queries)} test queries\")\n",
        "\n",
        "# Performance testing function\n",
        "def performance_test():\n",
        "    \"\"\"Run performance tests on the retrieval system.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for i, query in enumerate(test_queries):\n",
        "        print(f\"Testing query {i+1}/{len(test_queries)}: {query}\")\n",
        "        \n",
        "        # Profile the retrieval\n",
        "        with profiler.profile_request(f\"query_{i}\", \"retrieval\"):\n",
        "            start_time = time.time()\n",
        "            retrieved_docs = retrieval_system.retrieve(query, method=\"dense\", top_k=5)\n",
        "            end_time = time.time()\n",
        "            \n",
        "            # Record latency\n",
        "            latency_analyzer.record_latency(\"retrieval\", end_time - start_time, {\n",
        "                'query': query,\n",
        "                'num_results': len(retrieved_docs)\n",
        "            })\n",
        "            \n",
        "            results.append({\n",
        "                'query': query,\n",
        "                'duration': end_time - start_time,\n",
        "                'num_results': len(retrieved_docs),\n",
        "                'results': retrieved_docs\n",
        "            })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run performance tests\n",
        "print(\"\\nRunning performance tests...\")\n",
        "performance_results = performance_test()\n",
        "\n",
        "# Analyze results\n",
        "print(\"\\nPerformance Analysis:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "durations = [result['duration'] for result in performance_results]\n",
        "print(f\"Average retrieval time: {np.mean(durations):.3f} seconds\")\n",
        "print(f\"Min retrieval time: {np.min(durations):.3f} seconds\")\n",
        "print(f\"Max retrieval time: {np.max(durations):.3f} seconds\")\n",
        "print(f\"Standard deviation: {np.std(durations):.3f} seconds\")\n",
        "\n",
        "# Show detailed results\n",
        "print(f\"\\nDetailed Results:\")\n",
        "for i, result in enumerate(performance_results):\n",
        "    print(f\"{i+1}. Query: {result['query'][:50]}...\")\n",
        "    print(f\"   Duration: {result['duration']:.3f}s, Results: {result['num_results']}\")\n",
        "\n",
        "# Get performance summary\n",
        "summary = profiler.get_performance_summary()\n",
        "print(f\"\\nPerformance Summary:\")\n",
        "for key, value in summary.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
