{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embeddings and Vector Store\n",
        "\n",
        "In this notebook, we'll learn how to convert our text chunks into embeddings (numerical vectors) and build a vector store for efficient similarity search. This is the core of how RAG systems find relevant information.\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this notebook, you will:\n",
        "1. Understand different embedding models and their trade-offs\n",
        "2. Generate embeddings for your text chunks\n",
        "3. Build and query a vector database\n",
        "4. Compare different embedding approaches\n",
        "5. Learn about vector store optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Let's import the libraries we need and load our processed data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Embedding and vector store imports\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import faiss\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Add project root to path\n",
        "import sys\n",
        "sys.path.append(str(Path.cwd().parent))\n",
        "\n",
        "# Import our configuration\n",
        "from src.config import DATA_CONFIG, DATA_DIR\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "\n",
        "# Check if we have processed data\n",
        "processed_dir = DATA_DIR / \"processed\"\n",
        "chunks_file = processed_dir / \"all_chunks.json\"\n",
        "\n",
        "if chunks_file.exists():\n",
        "    print(f\"Found processed chunks: {chunks_file}\")\n",
        "    with open(chunks_file, 'r', encoding='utf-8') as f:\n",
        "        all_chunks = json.load(f)\n",
        "    print(f\"Loaded {len(all_chunks)} chunks\")\n",
        "else:\n",
        "    print(\"No processed chunks found. Please run the data collection notebook first.\")\n",
        "    all_chunks = []\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
