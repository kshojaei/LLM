{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embeddings and Vector Store\n",
        "\n",
        "In this notebook, we'll learn how to convert our text chunks into embeddings (numerical vectors) and build a vector store for efficient similarity search. This is the core of how RAG systems find relevant information.\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this notebook, you will:\n",
        "1. Understand different embedding models and their trade-offs\n",
        "2. Generate embeddings for your text chunks\n",
        "3. Build and query a vector database\n",
        "4. Compare different embedding approaches\n",
        "5. Learn about vector store optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Let's import the libraries we need and load our processed data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "Data directory: /Users/scienceman/Desktop/LLM/data\n",
            "Found processed chunks: /Users/scienceman/Desktop/LLM/data/processed/all_chunks.json\n",
            "Loaded 22 chunks\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Embedding and vector store imports\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import faiss\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Add project root to path\n",
        "import sys\n",
        "sys.path.append(str(Path.cwd().parent))\n",
        "\n",
        "# Import our configuration\n",
        "from src.config import DATA_CONFIG, DATA_DIR\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "\n",
        "# Check if we have processed data\n",
        "processed_dir = DATA_DIR / \"processed\"\n",
        "chunks_file = processed_dir / \"all_chunks.json\"\n",
        "\n",
        "if chunks_file.exists():\n",
        "    print(f\"Found processed chunks: {chunks_file}\")\n",
        "    with open(chunks_file, 'r', encoding='utf-8') as f:\n",
        "        all_chunks = json.load(f)\n",
        "    print(f\"Loaded {len(all_chunks)} chunks\")\n",
        "else:\n",
        "    print(\"No processed chunks found. Please run the data collection notebook first.\")\n",
        "    all_chunks = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Embedding Models\n",
        "\n",
        "Before we generate embeddings, let's understand the different models available and their trade-offs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Embedding Models:\n",
            "==================================================\n",
            "Model: all-MiniLM-L6-v2\n",
            "  Description: Small, fast model (384 dimensions)\n",
            "  Use Case: Good for learning and experimentation\n",
            "  Size: Small\n",
            "\n",
            "Model: all-mpnet-base-v2\n",
            "  Description: Medium model (768 dimensions)\n",
            "  Use Case: Good balance of speed and quality\n",
            "  Size: Medium\n",
            "\n",
            "Model: BAAI/bge-base-en-v1.5\n",
            "  Description: High-quality model (768 dimensions)\n",
            "  Use Case: Production use, better quality\n",
            "  Size: Large\n",
            "\n",
            "Loading embedding model...\n",
            "Model loaded: SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "Embedding dimension: 384\n",
            "\n",
            "Testing embedding generation...\n",
            "Generated embeddings shape: (3, 384)\n",
            "Each text is now represented by 384 numbers\n",
            "\n",
            "Similarity matrix:\n",
            "Texts:\n",
            "  0: Cats are small, furry animals that make great pets...\n",
            "  1: Dogs are loyal companions that love to play....\n",
            "  2: Machine learning is a subset of artificial intelli...\n",
            "\n",
            "Similarity scores (higher = more similar):\n",
            "  Text 0 vs Text 1: 0.345\n",
            "  Text 0 vs Text 2: 0.076\n",
            "  Text 1 vs Text 2: 0.112\n"
          ]
        }
      ],
      "source": [
        "# Let's compare different embedding models\n",
        "embedding_models = {\n",
        "    \"all-MiniLM-L6-v2\": {\n",
        "        \"description\": \"Small, fast model (384 dimensions)\",\n",
        "        \"use_case\": \"Good for learning and experimentation\",\n",
        "        \"size\": \"Small\"\n",
        "    },\n",
        "    \"all-mpnet-base-v2\": {\n",
        "        \"description\": \"Medium model (768 dimensions)\",\n",
        "        \"use_case\": \"Good balance of speed and quality\",\n",
        "        \"size\": \"Medium\"\n",
        "    },\n",
        "    \"BAAI/bge-base-en-v1.5\": {\n",
        "        \"description\": \"High-quality model (768 dimensions)\",\n",
        "        \"use_case\": \"Production use, better quality\",\n",
        "        \"size\": \"Large\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Available Embedding Models:\")\n",
        "print(\"=\" * 50)\n",
        "for model_name, info in embedding_models.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"  Description: {info['description']}\")\n",
        "    print(f\"  Use Case: {info['use_case']}\")\n",
        "    print(f\"  Size: {info['size']}\")\n",
        "    print()\n",
        "\n",
        "# For learning purposes, let's start with the small, fast model\n",
        "print(\"Loading embedding model...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(f\"Model loaded: {model}\")\n",
        "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
        "\n",
        "# Test the model with a simple example\n",
        "test_texts = [\n",
        "    \"Cats are small, furry animals that make great pets.\",\n",
        "    \"Dogs are loyal companions that love to play.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting embedding generation...\")\n",
        "embeddings = model.encode(test_texts)\n",
        "print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
        "print(f\"Each text is now represented by {embeddings.shape[1]} numbers\")\n",
        "\n",
        "# Show similarity between the texts\n",
        "similarities = cosine_similarity(embeddings)\n",
        "print(f\"\\nSimilarity matrix:\")\n",
        "print(\"Texts:\")\n",
        "for i, text in enumerate(test_texts):\n",
        "    print(f\"  {i}: {text[:50]}...\")\n",
        "\n",
        "print(\"\\nSimilarity scores (higher = more similar):\")\n",
        "for i in range(len(test_texts)):\n",
        "    for j in range(i+1, len(test_texts)):\n",
        "        sim = similarities[i][j]\n",
        "        print(f\"  Text {i} vs Text {j}: {sim:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Embeddings for Our Data\n",
        "\n",
        "Now let's generate embeddings for our processed chunks. We'll do this in batches to handle large amounts of data efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings for 22 chunks...\n",
            "Processing in batches of 32...\n",
            "\n",
            "Embedding generation completed!\n",
            "Total time: 0.64 seconds\n",
            "Embeddings shape: (22, 384)\n",
            "Average time per chunk: 29.30 ms\n",
            "Embeddings added to 22 chunks\n",
            "Chunks with embeddings saved to: /Users/scienceman/Desktop/LLM/data/processed/chunks_with_embeddings.json\n"
          ]
        }
      ],
      "source": [
        "# Generate embeddings for all our chunks\n",
        "if all_chunks:\n",
        "    print(f\"Generating embeddings for {len(all_chunks)} chunks...\")\n",
        "    \n",
        "    # Extract text from chunks\n",
        "    chunk_texts = [chunk['text'] for chunk in all_chunks]\n",
        "    \n",
        "    # Generate embeddings in batches for efficiency\n",
        "    batch_size = 32\n",
        "    all_embeddings = []\n",
        "    \n",
        "    print(f\"Processing in batches of {batch_size}...\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for i in range(0, len(chunk_texts), batch_size):\n",
        "        batch_texts = chunk_texts[i:i+batch_size]\n",
        "        batch_embeddings = model.encode(batch_texts, show_progress_bar=False)\n",
        "        all_embeddings.append(batch_embeddings)\n",
        "        \n",
        "        if (i // batch_size + 1) % 5 == 0:  # Print progress every 5 batches\n",
        "            print(f\"Processed {min(i + batch_size, len(chunk_texts))}/{len(chunk_texts)} chunks\")\n",
        "    \n",
        "    # Combine all embeddings\n",
        "    all_embeddings = np.vstack(all_embeddings)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    processing_time = end_time - start_time\n",
        "    \n",
        "    print(f\"\\nEmbedding generation completed!\")\n",
        "    print(f\"Total time: {processing_time:.2f} seconds\")\n",
        "    print(f\"Embeddings shape: {all_embeddings.shape}\")\n",
        "    print(f\"Average time per chunk: {processing_time/len(all_chunks)*1000:.2f} ms\")\n",
        "    \n",
        "    # Add embeddings to our chunks\n",
        "    for i, chunk in enumerate(all_chunks):\n",
        "        chunk['embedding'] = all_embeddings[i].tolist()\n",
        "    \n",
        "    print(f\"Embeddings added to {len(all_chunks)} chunks\")\n",
        "    \n",
        "    # Save the chunks with embeddings\n",
        "    embeddings_file = processed_dir / \"chunks_with_embeddings.json\"\n",
        "    with open(embeddings_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"Chunks with embeddings saved to: {embeddings_file}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No chunks available. Please run the data collection notebook first.\")\n",
        "    all_embeddings = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Vector Store with FAISS\n",
        "\n",
        "Now let's build a vector store using FAISS (Facebook AI Similarity Search) for efficient similarity search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building FAISS vector store...\n",
            "Embedding dimension: 384\n",
            "FAISS index built with 22 vectors\n",
            "FAISS index saved to: /Users/scienceman/Desktop/LLM/data/processed/faiss_index.bin\n",
            "\n",
            "Vector store ready! Testing with sample queries...\n",
            "\n",
            "Query: 'What is machine learning?'\n",
            "----------------------------------------\n",
            "1. Score: 0.748\n",
            "   Source: wikipedia\n",
            "   Title: Machine learning\n",
            "   Text: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "\n",
            "2. Score: 0.583\n",
            "   Source: wikipedia\n",
            "   Title: Artificial intelligence\n",
            "   Text: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically a...\n",
            "\n",
            "3. Score: 0.474\n",
            "   Source: wikipedia\n",
            "   Title: Deep learning\n",
            "   Text: In machine learning, deep learning focuses on utilizing multilayered neural networks to perform task...\n",
            "\n",
            "\n",
            "Query: 'Tell me about cats'\n",
            "----------------------------------------\n",
            "1. Score: 0.116\n",
            "   Source: arxiv\n",
            "   Title: FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n",
            "  Dataset and Comprehensive Benchmark\n",
            "   Text: FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 2...\n",
            "\n",
            "2. Score: 0.106\n",
            "   Source: wikipedia\n",
            "   Title: Computer vision\n",
            "   Text: This image understanding can be seen as the disentangling of symbolic information from image data us...\n",
            "\n",
            "3. Score: 0.105\n",
            "   Source: wikipedia\n",
            "   Title: Natural language processing\n",
            "   Text: Natural language processing (NLP) is the processing of natural language information by a computer Th...\n",
            "\n",
            "\n",
            "Query: 'How does artificial intelligence work?'\n",
            "----------------------------------------\n",
            "1. Score: 0.712\n",
            "   Source: wikipedia\n",
            "   Title: Artificial intelligence\n",
            "   Text: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically a...\n",
            "\n",
            "2. Score: 0.512\n",
            "   Source: wikipedia\n",
            "   Title: Machine learning\n",
            "   Text: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "\n",
            "3. Score: 0.406\n",
            "   Source: wikipedia\n",
            "   Title: Natural language processing\n",
            "   Text: Natural language processing (NLP) is the processing of natural language information by a computer Th...\n",
            "\n",
            "\n",
            "Query: 'What are the benefits of pets?'\n",
            "----------------------------------------\n",
            "1. Score: 0.065\n",
            "   Source: wikipedia\n",
            "   Title: Computer vision\n",
            "   Text: Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digita...\n",
            "\n",
            "2. Score: 0.063\n",
            "   Source: arxiv\n",
            "   Title: SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning\n",
            "   Text: When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms p...\n",
            "\n",
            "3. Score: 0.062\n",
            "   Source: arxiv\n",
            "   Title: FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n",
            "  Dataset and Comprehensive Benchmark\n",
            "   Text: FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 2...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Build FAISS vector store\n",
        "if all_embeddings is not None:\n",
        "    print(\"Building FAISS vector store...\")\n",
        "    \n",
        "    # Get embedding dimension\n",
        "    embedding_dim = all_embeddings.shape[1]\n",
        "    print(f\"Embedding dimension: {embedding_dim}\")\n",
        "    \n",
        "    # Create FAISS index\n",
        "    # Using IndexFlatIP (Inner Product) which works well with normalized embeddings\n",
        "    # For cosine similarity, we'll normalize the embeddings\n",
        "    from sklearn.preprocessing import normalize\n",
        "    \n",
        "    # Normalize embeddings for cosine similarity\n",
        "    normalized_embeddings = normalize(all_embeddings, norm='l2')\n",
        "    \n",
        "    # Create FAISS index\n",
        "    index = faiss.IndexFlatIP(embedding_dim)  # Inner Product (cosine similarity for normalized vectors)\n",
        "    \n",
        "    # Add embeddings to index\n",
        "    index.add(normalized_embeddings.astype('float32'))\n",
        "    \n",
        "    print(f\"FAISS index built with {index.ntotal} vectors\")\n",
        "    \n",
        "    # Save the index\n",
        "    faiss_file = processed_dir / \"faiss_index.bin\"\n",
        "    faiss.write_index(index, str(faiss_file))\n",
        "    print(f\"FAISS index saved to: {faiss_file}\")\n",
        "    \n",
        "    # Test the vector store\n",
        "    def search_vector_store(query_text, top_k=5):\n",
        "        \"\"\"\n",
        "        Search the vector store for similar chunks.\n",
        "        \"\"\"\n",
        "        # Encode query\n",
        "        query_embedding = model.encode([query_text])\n",
        "        query_embedding = normalize(query_embedding, norm='l2').astype('float32')\n",
        "        \n",
        "        # Search\n",
        "        scores, indices = index.search(query_embedding, top_k)\n",
        "        \n",
        "        # Get results\n",
        "        results = []\n",
        "        for score, idx in zip(scores[0], indices[0]):\n",
        "            if idx < len(all_chunks):  # Valid index\n",
        "                chunk = all_chunks[idx]\n",
        "                results.append({\n",
        "                    'chunk': chunk,\n",
        "                    'score': float(score),\n",
        "                    'index': int(idx)\n",
        "                })\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    print(\"\\nVector store ready! Testing with sample queries...\")\n",
        "    \n",
        "    # Test queries\n",
        "    test_queries = [\n",
        "        \"What is machine learning?\",\n",
        "        \"Tell me about cats\",\n",
        "        \"How does artificial intelligence work?\",\n",
        "        \"What are the benefits of pets?\"\n",
        "    ]\n",
        "    \n",
        "    for query in test_queries:\n",
        "        print(f\"\\nQuery: '{query}'\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        results = search_vector_store(query, top_k=3)\n",
        "        \n",
        "        for i, result in enumerate(results):\n",
        "            chunk = result['chunk']\n",
        "            score = result['score']\n",
        "            print(f\"{i+1}. Score: {score:.3f}\")\n",
        "            print(f\"   Source: {chunk['source']}\")\n",
        "            print(f\"   Title: {chunk['source_title']}\")\n",
        "            print(f\"   Text: {chunk['text'][:100]}...\")\n",
        "            print()\n",
        "    \n",
        "else:\n",
        "    print(\"No embeddings available. Please run the embedding generation cell first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Vector Store with ChromaDB\n",
        "\n",
        "Let's also try ChromaDB, which is another popular vector database that's easier to use and has more features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "Add of existing embedding ID: chunk_0\n",
            "Add of existing embedding ID: chunk_1\n",
            "Add of existing embedding ID: chunk_2\n",
            "Add of existing embedding ID: chunk_3\n",
            "Add of existing embedding ID: chunk_4\n",
            "Add of existing embedding ID: chunk_5\n",
            "Add of existing embedding ID: chunk_6\n",
            "Add of existing embedding ID: chunk_7\n",
            "Add of existing embedding ID: chunk_8\n",
            "Add of existing embedding ID: chunk_9\n",
            "Add of existing embedding ID: chunk_10\n",
            "Insert of existing embedding ID: chunk_0\n",
            "Insert of existing embedding ID: chunk_1\n",
            "Insert of existing embedding ID: chunk_2\n",
            "Insert of existing embedding ID: chunk_3\n",
            "Insert of existing embedding ID: chunk_4\n",
            "Insert of existing embedding ID: chunk_5\n",
            "Insert of existing embedding ID: chunk_6\n",
            "Insert of existing embedding ID: chunk_7\n",
            "Insert of existing embedding ID: chunk_8\n",
            "Insert of existing embedding ID: chunk_9\n",
            "Insert of existing embedding ID: chunk_10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building ChromaDB vector store...\n",
            "Created ChromaDB collection: rag_chunks\n",
            "Adding documents to ChromaDB...\n",
            "Added 22 documents to ChromaDB\n",
            "\n",
            "ChromaDB vector store ready! Testing with sample queries...\n",
            "\n",
            "Query: 'What is machine learning?'\n",
            "----------------------------------------\n",
            "1. Similarity: 0.551\n",
            "   Source: wikipedia\n",
            "   Title: Machine Learning\n",
            "   Text: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn fr...\n",
            "\n",
            "2. Similarity: 0.121\n",
            "   Source: wikipedia\n",
            "   Title: Computer Vision\n",
            "   Text: Computer vision is a field of artificial intelligence that trains computers to interpret and underst...\n",
            "\n",
            "3. Similarity: 0.031\n",
            "   Source: wikipedia\n",
            "   Title: Artificial Intelligence\n",
            "   Text: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are pro...\n",
            "\n",
            "\n",
            "Query: 'Tell me about cats'\n",
            "----------------------------------------\n",
            "1. Similarity: -0.708\n",
            "   Source: wikipedia\n",
            "   Title: Deep Learning\n",
            "   Text: Deep learning is a subset of machine learning based on artificial neural networks with representatio...\n",
            "\n",
            "2. Similarity: -0.726\n",
            "   Source: wikipedia\n",
            "   Title: Computer Vision\n",
            "   Text: Computer vision is a field of artificial intelligence that trains computers to interpret and underst...\n",
            "\n",
            "3. Similarity: -0.806\n",
            "   Source: wikipedia\n",
            "   Title: Natural Language Processing\n",
            "   Text: Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interact...\n",
            "\n",
            "\n",
            "Query: 'How does artificial intelligence work?'\n",
            "----------------------------------------\n",
            "1. Similarity: 0.433\n",
            "   Source: wikipedia\n",
            "   Title: Artificial Intelligence\n",
            "   Text: Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are pro...\n",
            "\n",
            "2. Similarity: 0.077\n",
            "   Source: wikipedia\n",
            "   Title: Machine Learning\n",
            "   Text: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn fr...\n",
            "\n",
            "3. Similarity: -0.015\n",
            "   Source: wikipedia\n",
            "   Title: Computer Vision\n",
            "   Text: Computer vision is a field of artificial intelligence that trains computers to interpret and underst...\n",
            "\n",
            "\n",
            "Query: 'What are the benefits of pets?'\n",
            "----------------------------------------\n",
            "1. Similarity: -0.863\n",
            "   Source: arxiv\n",
            "   Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
            "   Text: As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to c...\n",
            "\n",
            "2. Similarity: -0.873\n",
            "   Source: arxiv\n",
            "   Title: SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning\n",
            "   Text: When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms p...\n",
            "\n",
            "3. Similarity: -0.877\n",
            "   Source: arxiv\n",
            "   Title: SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning\n",
            "   Text: Github: https:github comPRIME-RLSimpleVLA-RL...\n",
            "\n",
            "ChromaDB database saved to: /Users/scienceman/Desktop/LLM/data/processed/chroma_db\n"
          ]
        }
      ],
      "source": [
        "# Build ChromaDB vector store\n",
        "if all_chunks:\n",
        "    print(\"Building ChromaDB vector store...\")\n",
        "    \n",
        "    # Initialize ChromaDB\n",
        "    chroma_dir = processed_dir / \"chroma_db\"\n",
        "    chroma_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    client = chromadb.PersistentClient(path=str(chroma_dir))\n",
        "    \n",
        "    # Create collection\n",
        "    collection_name = \"rag_chunks\"\n",
        "    collection = client.get_or_create_collection(\n",
        "        name=collection_name,\n",
        "        metadata={\"description\": \"RAG system chunks with embeddings\"}\n",
        "    )\n",
        "    \n",
        "    print(f\"Created ChromaDB collection: {collection_name}\")\n",
        "    \n",
        "    # Prepare data for ChromaDB\n",
        "    documents = [chunk['text'] for chunk in all_chunks]\n",
        "    metadatas = []\n",
        "    ids = []\n",
        "    \n",
        "    for i, chunk in enumerate(all_chunks):\n",
        "        metadata = {\n",
        "            'source': chunk['source'],\n",
        "            'title': chunk['source_title'],\n",
        "            'word_count': chunk['word_count'],\n",
        "            'chunk_type': chunk['type'],\n",
        "            'chunk_id': chunk['chunk_id']\n",
        "        }\n",
        "        metadatas.append(metadata)\n",
        "        ids.append(f\"chunk_{i}\")\n",
        "    \n",
        "    # Add documents to collection\n",
        "    print(\"Adding documents to ChromaDB...\")\n",
        "    \n",
        "    # ChromaDB can generate embeddings automatically, but we'll use our own\n",
        "    if all_embeddings is not None:\n",
        "        embeddings = all_embeddings.tolist()\n",
        "        collection.add(\n",
        "            documents=documents,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids,\n",
        "            embeddings=embeddings\n",
        "        )\n",
        "    else:\n",
        "        # Let ChromaDB generate embeddings\n",
        "        collection.add(\n",
        "            documents=documents,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "    \n",
        "    print(f\"Added {len(documents)} documents to ChromaDB\")\n",
        "    \n",
        "    # Test ChromaDB search\n",
        "    def search_chromadb(query_text, top_k=5):\n",
        "        \"\"\"\n",
        "        Search ChromaDB for similar chunks.\n",
        "        \"\"\"\n",
        "        results = collection.query(\n",
        "            query_texts=[query_text],\n",
        "            n_results=top_k,\n",
        "            include=['documents', 'metadatas', 'distances']\n",
        "        )\n",
        "        \n",
        "        formatted_results = []\n",
        "        if results['documents'] and results['documents'][0]:\n",
        "            for i, (doc, metadata, distance) in enumerate(zip(\n",
        "                results['documents'][0],\n",
        "                results['metadatas'][0],\n",
        "                results['distances'][0]\n",
        "            )):\n",
        "                # Convert distance to similarity score (ChromaDB uses distance, we want similarity)\n",
        "                similarity = 1 - distance\n",
        "                \n",
        "                formatted_results.append({\n",
        "                    'text': doc,\n",
        "                    'metadata': metadata,\n",
        "                    'similarity': similarity,\n",
        "                    'distance': distance\n",
        "                })\n",
        "        \n",
        "        return formatted_results\n",
        "    \n",
        "    print(\"\\nChromaDB vector store ready! Testing with sample queries...\")\n",
        "    \n",
        "    # Test queries\n",
        "    test_queries = [\n",
        "        \"What is machine learning?\",\n",
        "        \"Tell me about cats\",\n",
        "        \"How does artificial intelligence work?\",\n",
        "        \"What are the benefits of pets?\"\n",
        "    ]\n",
        "    \n",
        "    for query in test_queries:\n",
        "        print(f\"\\nQuery: '{query}'\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        results = search_chromadb(query, top_k=3)\n",
        "        \n",
        "        for i, result in enumerate(results):\n",
        "            print(f\"{i+1}. Similarity: {result['similarity']:.3f}\")\n",
        "            print(f\"   Source: {result['metadata']['source']}\")\n",
        "            print(f\"   Title: {result['metadata']['title']}\")\n",
        "            print(f\"   Text: {result['text'][:100]}...\")\n",
        "            print()\n",
        "    \n",
        "    print(f\"ChromaDB database saved to: {chroma_dir}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No chunks available. Please run the data collection notebook first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Query Testing\n",
        "\n",
        "Let's create an interactive tool to test our vector stores with your own queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interactive search tools ready!\n",
            "Try these functions:\n",
            "1. interactive_search() - Interactive query testing\n",
            "2. compare_vector_stores('your query here') - Compare both vector stores\n",
            "\n",
            "Sample comparison:\n",
            "Both vector stores not available for comparison.\n"
          ]
        }
      ],
      "source": [
        "# Interactive query testing function\n",
        "def interactive_search():\n",
        "    \"\"\"\n",
        "    Interactive function to test queries against our vector stores.\n",
        "    \"\"\"\n",
        "    if not all_chunks:\n",
        "        print(\"No data available. Please run the data collection notebook first.\")\n",
        "        return\n",
        "    \n",
        "    print(\"Interactive Vector Store Search\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Enter your questions to search through the knowledge base!\")\n",
        "    print(\"Type 'quit' to exit.\")\n",
        "    print()\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            query = input(\"Enter your question: \").strip()\n",
        "            \n",
        "            if query.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "            \n",
        "            if not query:\n",
        "                print(\"Please enter a question.\")\n",
        "                continue\n",
        "            \n",
        "            print(f\"\\nSearching for: '{query}'\")\n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "            # Search FAISS if available\n",
        "            if 'index' in locals():\n",
        "                print(\"FAISS Results:\")\n",
        "                print(\"-\" * 30)\n",
        "                faiss_results = search_vector_store(query, top_k=3)\n",
        "                \n",
        "                for i, result in enumerate(faiss_results):\n",
        "                    chunk = result['chunk']\n",
        "                    score = result['score']\n",
        "                    print(f\"{i+1}. Score: {score:.3f}\")\n",
        "                    print(f\"   Source: {chunk['source']}\")\n",
        "                    print(f\"   Title: {chunk['source_title']}\")\n",
        "                    print(f\"   Text: {chunk['text'][:150]}...\")\n",
        "                    print()\n",
        "            \n",
        "            # Search ChromaDB if available\n",
        "            if 'collection' in locals():\n",
        "                print(\"ChromaDB Results:\")\n",
        "                print(\"-\" * 30)\n",
        "                chroma_results = search_chromadb(query, top_k=3)\n",
        "                \n",
        "                for i, result in enumerate(chroma_results):\n",
        "                    print(f\"{i+1}. Similarity: {result['similarity']:.3f}\")\n",
        "                    print(f\"   Source: {result['metadata']['source']}\")\n",
        "                    print(f\"   Title: {result['metadata']['title']}\")\n",
        "                    print(f\"   Text: {result['text'][:150]}...\")\n",
        "                    print()\n",
        "            \n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nGoodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "# Let's also create a comparison function\n",
        "def compare_vector_stores(query_text, top_k=5):\n",
        "    \"\"\"\n",
        "    Compare results from both vector stores.\n",
        "    \"\"\"\n",
        "    if 'index' not in locals() or 'collection' not in locals():\n",
        "        print(\"Both vector stores not available for comparison.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Comparing vector stores for query: '{query_text}'\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # FAISS results\n",
        "    print(\"FAISS Results:\")\n",
        "    print(\"-\" * 35)\n",
        "    faiss_results = search_vector_store(query_text, top_k)\n",
        "    \n",
        "    for i, result in enumerate(faiss_results):\n",
        "        chunk = result['chunk']\n",
        "        score = result['score']\n",
        "        print(f\"{i+1}. Score: {score:.3f} | {chunk['source']} | {chunk['source_title'][:50]}...\")\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # ChromaDB results\n",
        "    print(\"ChromaDB Results:\")\n",
        "    print(\"-\" * 35)\n",
        "    chroma_results = search_chromadb(query_text, top_k)\n",
        "    \n",
        "    for i, result in enumerate(chroma_results):\n",
        "        print(f\"{i+1}. Similarity: {result['similarity']:.3f} | {result['metadata']['source']} | {result['metadata']['title'][:50]}...\")\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(\"Interactive search tools ready!\")\n",
        "print(\"Try these functions:\")\n",
        "print(\"1. interactive_search() - Interactive query testing\")\n",
        "print(\"2. compare_vector_stores('your query here') - Compare both vector stores\")\n",
        "print()\n",
        "print(\"Sample comparison:\")\n",
        "if 'index' in locals() and 'collection' in locals():\n",
        "    compare_vector_stores(\"What is machine learning?\")\n",
        "else:\n",
        "    print(\"Vector stores not ready yet. Run the previous cells first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "Excellent! You've successfully built a complete embedding and vector store system. Here's what we've accomplished:\n",
        "\n",
        "### What We've Built:\n",
        "1. **Embedding Generation** - Converted text chunks to numerical vectors\n",
        "2. **FAISS Vector Store** - Fast similarity search using Facebook's library\n",
        "3. **ChromaDB Vector Store** - Feature-rich vector database with metadata\n",
        "4. **Interactive Search** - Tools to query your knowledge base\n",
        "5. **Comparison Tools** - Compare different vector store approaches\n",
        "\n",
        "### Key Learnings:\n",
        "- **Embedding Models** - Different models have different trade-offs in speed vs quality\n",
        "- **Vector Stores** - FAISS is fast, ChromaDB is feature-rich\n",
        "- **Similarity Search** - Cosine similarity works well for semantic search\n",
        "- **Batch Processing** - Efficient embedding generation for large datasets\n",
        "- **Metadata** - Important for filtering and understanding search results\n",
        "\n",
        "### Files Created:\n",
        "- `chunks_with_embeddings.json` - Chunks with embedding vectors\n",
        "- `faiss_index.bin` - FAISS vector store index\n",
        "- `chroma_db/` - ChromaDB database directory\n",
        "\n",
        "### Next Steps:\n",
        "Now that we have our vector stores, the next steps are:\n",
        "1. **LLM Integration** - Connect language models to generate answers\n",
        "2. **Prompt Engineering** - Design effective prompts for the LLM\n",
        "3. **Complete RAG Pipeline** - Combine retrieval + generation\n",
        "4. **Evaluation** - Measure how well the system works\n",
        "5. **Optimization** - Improve performance and accuracy\n",
        "\n",
        "The foundation is now complete - you have a working retrieval system that can find relevant information from your knowledge base!\n",
        "\n",
        "**Ready for the next notebook?** The next step is LLM integration, where we'll learn how to generate answers based on retrieved context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exploring ArXiv dataset structure...\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Dataset scripts are no longer supported, but found scientific_papers.py",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[50], line 4\u001b[0m\n",
            "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now let's look at ArXiv data\u001b[39;00m\n",
            "\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExploring ArXiv dataset structure...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;32m----> 4\u001b[0m arxiv_sample \u001b[38;5;241m=\u001b[39m load_dataset(\n",
            "\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscientific_papers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marxiv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m      7\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain[:5]\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Just 5 samples\u001b[39;00m\n",
            "\u001b[1;32m      8\u001b[0m     \n",
            "\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mArXiv sample contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(arxiv_sample)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m papers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mKeys in each paper: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marxiv_sample\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1392\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n",
            "\u001b[1;32m   1387\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n",
            "\u001b[1;32m   1388\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n",
            "\u001b[1;32m   1389\u001b[0m )\n",
            "\u001b[1;32m   1391\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n",
            "\u001b[0;32m-> 1392\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n",
            "\u001b[1;32m   1393\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n",
            "\u001b[1;32m   1394\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n",
            "\u001b[1;32m   1395\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n",
            "\u001b[1;32m   1396\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n",
            "\u001b[1;32m   1397\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n",
            "\u001b[1;32m   1398\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n",
            "\u001b[1;32m   1399\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n",
            "\u001b[1;32m   1400\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n",
            "\u001b[1;32m   1401\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n",
            "\u001b[1;32m   1402\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n",
            "\u001b[1;32m   1403\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n",
            "\u001b[1;32m   1404\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n",
            "\u001b[1;32m   1405\u001b[0m )\n",
            "\u001b[1;32m   1407\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n",
            "\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1132\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n",
            "\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;32m   1131\u001b[0m     features \u001b[38;5;241m=\u001b[39m _fix_for_backward_compatible_features(features)\n",
            "\u001b[0;32m-> 1132\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m dataset_module_factory(\n",
            "\u001b[1;32m   1133\u001b[0m     path,\n",
            "\u001b[1;32m   1134\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n",
            "\u001b[1;32m   1135\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n",
            "\u001b[1;32m   1136\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n",
            "\u001b[1;32m   1137\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n",
            "\u001b[1;32m   1138\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n",
            "\u001b[1;32m   1139\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n",
            "\u001b[1;32m   1140\u001b[0m )\n",
            "\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n",
            "\u001b[1;32m   1142\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1031\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n",
            "\u001b[1;32m   1026\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n",
            "\u001b[1;32m   1027\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n",
            "\u001b[1;32m   1028\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m   1029\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m   1030\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;32m-> 1031\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:989\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n",
            "\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[1;32m    982\u001b[0m     api\u001b[38;5;241m.\u001b[39mhf_hub_download(\n",
            "\u001b[1;32m    983\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n",
            "\u001b[1;32m    984\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n",
            "\u001b[0;32m   (...)\u001b[0m\n",
            "\u001b[1;32m    987\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n",
            "\u001b[1;32m    988\u001b[0m     )\n",
            "\u001b[0;32m--> 989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
            "\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n",
            "\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Dataset scripts are no longer supported, but found scientific_papers.py"
          ]
        }
      ],
      "source": [
        "# Use our DataCollector instead of direct load_dataset\n",
        "collector = DataCollector()\n",
        "arxiv_sample_data = collector.collect_arxiv_data(max_documents=5)\n",
        "print(f'Collected {len(arxiv_sample_data)} ArXiv papers')\n",
        "\n",
        "# Convert to the format expected by the rest of the notebook\n",
        "arxiv_sample = []\n",
        "for paper in arxiv_sample_data:\n",
        "    arxiv_sample.append({\n",
        "        'title': paper['title'],\n",
        "        'abstract': paper['abstract']\n",
        "    })\n",
        "\n",
        "print(f'ArXiv sample structure: {len(arxiv_sample)} papers')\n",
        "if arxiv_sample:\n",
        "    print(f'First paper title: {arxiv_sample[0][\"title\"]}')\n",
        "    print(f'First paper abstract length: {len(arxiv_sample[0][\"abstract\"])} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exploring ArXiv dataset structure...\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Dataset scripts are no longer supported, but found scientific_papers.py",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[50], line 4\u001b[0m\n",
            "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now let's look at ArXiv data\u001b[39;00m\n",
            "\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExploring ArXiv dataset structure...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;32m----> 4\u001b[0m arxiv_sample \u001b[38;5;241m=\u001b[39m load_dataset(\n",
            "\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscientific_papers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marxiv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m      7\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain[:5]\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Just 5 samples\u001b[39;00m\n",
            "\u001b[1;32m      8\u001b[0m     \n",
            "\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mArXiv sample contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(arxiv_sample)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m papers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mKeys in each paper: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marxiv_sample\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1392\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n",
            "\u001b[1;32m   1387\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n",
            "\u001b[1;32m   1388\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n",
            "\u001b[1;32m   1389\u001b[0m )\n",
            "\u001b[1;32m   1391\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n",
            "\u001b[0;32m-> 1392\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n",
            "\u001b[1;32m   1393\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n",
            "\u001b[1;32m   1394\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n",
            "\u001b[1;32m   1395\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n",
            "\u001b[1;32m   1396\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n",
            "\u001b[1;32m   1397\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n",
            "\u001b[1;32m   1398\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n",
            "\u001b[1;32m   1399\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n",
            "\u001b[1;32m   1400\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n",
            "\u001b[1;32m   1401\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n",
            "\u001b[1;32m   1402\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n",
            "\u001b[1;32m   1403\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n",
            "\u001b[1;32m   1404\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n",
            "\u001b[1;32m   1405\u001b[0m )\n",
            "\u001b[1;32m   1407\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n",
            "\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1132\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n",
            "\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;32m   1131\u001b[0m     features \u001b[38;5;241m=\u001b[39m _fix_for_backward_compatible_features(features)\n",
            "\u001b[0;32m-> 1132\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m dataset_module_factory(\n",
            "\u001b[1;32m   1133\u001b[0m     path,\n",
            "\u001b[1;32m   1134\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n",
            "\u001b[1;32m   1135\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n",
            "\u001b[1;32m   1136\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n",
            "\u001b[1;32m   1137\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n",
            "\u001b[1;32m   1138\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n",
            "\u001b[1;32m   1139\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n",
            "\u001b[1;32m   1140\u001b[0m )\n",
            "\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n",
            "\u001b[1;32m   1142\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1031\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n",
            "\u001b[1;32m   1026\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n",
            "\u001b[1;32m   1027\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n",
            "\u001b[1;32m   1028\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m   1029\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m   1030\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;32m-> 1031\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:989\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n",
            "\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[1;32m    982\u001b[0m     api\u001b[38;5;241m.\u001b[39mhf_hub_download(\n",
            "\u001b[1;32m    983\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n",
            "\u001b[1;32m    984\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n",
            "\u001b[0;32m   (...)\u001b[0m\n",
            "\u001b[1;32m    987\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n",
            "\u001b[1;32m    988\u001b[0m     )\n",
            "\u001b[0;32m--> 989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
            "\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n",
            "\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Dataset scripts are no longer supported, but found scientific_papers.py"
          ]
        }
      ],
      "source": [
        "# Use our DataCollector instead of direct load_dataset\n",
        "collector = DataCollector()\n",
        "arxiv_sample_data = collector.collect_arxiv_data(max_documents=5)\n",
        "print(f'Collected {len(arxiv_sample_data)} ArXiv papers')\n",
        "\n",
        "# Convert to the format expected by the rest of the notebook\n",
        "arxiv_sample = []\n",
        "for paper in arxiv_sample_data:\n",
        "    arxiv_sample.append({\n",
        "        'title': paper['title'],\n",
        "        'abstract': paper['abstract']\n",
        "    })\n",
        "\n",
        "print(f'ArXiv sample structure: {len(arxiv_sample)} papers')\n",
        "if arxiv_sample:\n",
        "    print(f'First paper title: {arxiv_sample[0][\"title\"]}')\n",
        "    print(f'First paper abstract length: {len(arxiv_sample[0][\"abstract\"])} characters')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
