{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embeddings and Vector Store: The Heart of RAG Systems\n",
        "\n",
        "## Introduction to Embeddings and Vector Stores\n",
        "\n",
        "In this notebook, we'll learn how to convert our text chunks into embeddings (numerical vectors) and build a vector store for efficient similarity search. This is the core of how RAG systems find relevant information.\n",
        "\n",
        "### Why Embeddings Matter\n",
        "\n",
        "Embeddings are the bridge between human language and machine understanding. They transform text into numerical vectors that capture semantic meaning, allowing computers to:\n",
        "\n",
        "- **Understand Similarity**: Find documents that are conceptually related\n",
        "- **Enable Search**: Perform fast similarity searches across large datasets\n",
        "- **Preserve Context**: Maintain semantic relationships between concepts\n",
        "- **Scale Efficiently**: Handle millions of documents with fast retrieval\n",
        "\n",
        "### The Vector Store Advantage\n",
        "\n",
        "Vector stores are specialized databases designed for high-dimensional vector operations. They provide:\n",
        "\n",
        "- **Fast Similarity Search**: Find relevant documents in milliseconds\n",
        "- **Scalability**: Handle millions of vectors efficiently\n",
        "- **Flexibility**: Support various similarity metrics and search strategies\n",
        "- **Integration**: Easy integration with RAG systems\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "1. Understand different embedding models and their trade-offs\n",
        "2. Generate embeddings for your text chunks using state-of-the-art models\n",
        "3. Build and query vector databases using FAISS and ChromaDB\n",
        "4. Compare different embedding approaches and their performance\n",
        "5. Learn about vector store optimization and best practices\n",
        "6. Master the art of semantic search for RAG systems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current directory: /Users/scienceman/Desktop/LLM/notebooks\n",
            "Project root: /Users/scienceman/Desktop/LLM\n",
            "Python path: ['.', '/Users/scienceman/Desktop/LLM/notebooks', '/Users/scienceman/Desktop/LLM']\n",
            "Successfully imported from src module\n",
            "Libraries imported successfully!\n",
            "Data directory: /Users/scienceman/Desktop/LLM/data\n",
            "Found processed chunks: /Users/scienceman/Desktop/LLM/data/processed/all_chunks.json\n",
            "Loaded 18 chunks\n",
            "\n",
            "Sample chunk structure:\n",
            "  Keys: ['text', 'type', 'chunk_id', 'source_doc_id', 'source_title', 'source', 'chunk_index', 'word_count', 'char_count', 'metadata']\n",
            "  Text preview: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "  Source: wikipedia\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Embedding and vector store imports\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import faiss\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Add project root to path - multiple approaches for reliability\n",
        "current_dir = os.getcwd()\n",
        "project_root = os.path.dirname(current_dir) if current_dir.endswith('notebooks') else current_dir\n",
        "\n",
        "# Add both current directory and project root to path\n",
        "sys.path.insert(0, project_root)\n",
        "sys.path.insert(0, current_dir)\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "print(f\"Current directory: {current_dir}\")\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Python path: {sys.path[:3]}\")\n",
        "\n",
        "# Import our configuration with error handling\n",
        "try:\n",
        "    from src.config import DATA_CONFIG, DATA_DIR\n",
        "    print(\"Successfully imported from src module\")\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    print(\"Trying alternative import methods...\")\n",
        "    \n",
        "    # Try importing directly from the file\n",
        "    try:\n",
        "        import importlib.util\n",
        "        \n",
        "        # Import config\n",
        "        config_path = os.path.join(project_root, 'src', 'config.py')\n",
        "        spec = importlib.util.spec_from_file_location(\"config\", config_path)\n",
        "        config_module = importlib.util.module_from_spec(spec)\n",
        "        spec.loader.exec_module(config_module)\n",
        "        DATA_CONFIG = config_module.DATA_CONFIG\n",
        "        DATA_DIR = config_module.DATA_DIR\n",
        "        \n",
        "        print(\"Successfully imported using direct file imports\")\n",
        "        \n",
        "    except Exception as e2:\n",
        "        print(f\"Direct import also failed: {e2}\")\n",
        "        print(\"Please check that you're running this from the correct directory\")\n",
        "        raise e2\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "\n",
        "# Check if we have processed data\n",
        "processed_dir = DATA_DIR / \"processed\"\n",
        "chunks_file = processed_dir / \"all_chunks.json\"\n",
        "\n",
        "if chunks_file.exists():\n",
        "    print(f\"Found processed chunks: {chunks_file}\")\n",
        "    with open(chunks_file, 'r', encoding='utf-8') as f:\n",
        "        all_chunks = json.load(f)\n",
        "    print(f\"Loaded {len(all_chunks)} chunks\")\n",
        "    \n",
        "    # Display sample chunk structure\n",
        "    if all_chunks:\n",
        "        print(f\"\\nSample chunk structure:\")\n",
        "        sample_chunk = all_chunks[0]\n",
        "        print(f\"  Keys: {list(sample_chunk.keys())}\")\n",
        "        print(f\"  Text preview: {sample_chunk.get('text', '')[:100]}...\")\n",
        "        print(f\"  Source: {sample_chunk.get('source', 'unknown')}\")\n",
        "else:\n",
        "    print(\"No processed chunks found. Please run the data collection notebook first.\")\n",
        "    all_chunks = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Embedding Models: The Foundation of Semantic Search\n",
        "\n",
        "### What Are Embedding Models?\n",
        "\n",
        "Embedding models are neural networks that convert text into dense numerical vectors. These vectors capture the semantic meaning of text, allowing us to:\n",
        "\n",
        "- **Measure Similarity**: Calculate how similar two pieces of text are\n",
        "- **Enable Search**: Find relevant documents based on meaning, not just keywords\n",
        "- **Preserve Relationships**: Maintain conceptual relationships between different texts\n",
        "- **Enable Machine Understanding**: Allow computers to work with human language\n",
        "\n",
        "### How Embedding Models Work\n",
        "\n",
        "1. **Text Preprocessing**: Convert text into tokens (words, subwords, or characters)\n",
        "2. **Neural Processing**: Pass tokens through a neural network (usually a transformer)\n",
        "3. **Vector Generation**: Extract dense vectors that represent the text's meaning\n",
        "4. **Normalization**: Often normalize vectors for better similarity calculations\n",
        "\n",
        "### Key Characteristics of Good Embedding Models\n",
        "\n",
        "- **Semantic Understanding**: Captures meaning, not just word overlap\n",
        "- **Context Awareness**: Understands how words change meaning in different contexts\n",
        "- **Multilingual Support**: Works across different languages\n",
        "- **Efficiency**: Fast enough for real-time applications\n",
        "- **Quality**: Produces meaningful similarity scores\n",
        "\n",
        "### Model Selection Criteria\n",
        "\n",
        "When choosing an embedding model, consider:\n",
        "\n",
        "- **Task Type**: General purpose vs. domain-specific\n",
        "- **Performance**: Speed vs. quality trade-offs\n",
        "- **Size**: Model size vs. computational requirements\n",
        "- **Language**: English-only vs. multilingual\n",
        "- **Domain**: General knowledge vs. specialized fields\n",
        "\n",
        "Before we generate embeddings, let's understand the different models available and their trade-offs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Embedding Models:\n",
            "============================================================\n",
            "Model: all-MiniLM-L6-v2\n",
            "  Description: Small, fast model (384 dimensions)\n",
            "  Use Case: Good for learning and experimentation\n",
            "  Size: Small\n",
            "  Pros: Fast inference, Low memory usage, Good for prototyping\n",
            "  Cons: Lower quality than larger models, Limited context understanding\n",
            "\n",
            "Model: all-mpnet-base-v2\n",
            "  Description: Medium model (768 dimensions)\n",
            "  Use Case: Good balance of speed and quality\n",
            "  Size: Medium\n",
            "  Pros: Better quality than MiniLM, Reasonable speed, Good general performance\n",
            "  Cons: Slower than MiniLM, Higher memory usage\n",
            "\n",
            "Model: BAAI/bge-base-en-v1.5\n",
            "  Description: High-quality model (768 dimensions)\n",
            "  Use Case: Production use, better quality\n",
            "  Size: Large\n",
            "  Pros: Excellent quality, State-of-the-art performance, Good for production\n",
            "  Cons: Slower inference, Higher memory requirements, Larger download size\n",
            "\n",
            "Loading embedding model...\n",
            "We'll use all-MiniLM-L6-v2 for this tutorial - it's fast and perfect for learning!\n",
            "\n",
            "Model loaded successfully!\n",
            "Model name: SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "Embedding dimension: 384\n",
            "Model max sequence length: 256\n",
            "\n",
            "Testing embedding generation with 3 sample texts...\n",
            "Sample texts:\n",
            "  1. Cats are small, furry animals that make great pets.\n",
            "  2. Dogs are loyal companions that love to play.\n",
            "  3. Machine learning is a subset of artificial intelligence.\n",
            "\n",
            "Generating embeddings...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8adbb2455194350a957c1ed0bf773f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated embeddings shape: (3, 384)\n",
            "Each text is now represented by 384 numbers\n",
            "Data type: float32\n",
            "\n",
            "Calculating similarity matrix...\n",
            "Similarity matrix shape: (3, 3)\n",
            "\n",
            "Similarity scores (higher = more similar, range: 0-1):\n",
            "Texts:\n",
            "  1: Cats are small, furry animals that make great pets...\n",
            "  2: Dogs are loyal companions that love to play....\n",
            "  3: Machine learning is a subset of artificial intelli...\n",
            "\n",
            "Pairwise similarities:\n",
            "  Text 1 vs Text 2: 0.345\n",
            "  Text 1 vs Text 3: 0.076\n",
            "  Text 2 vs Text 3: 0.112\n",
            "\n",
            "Interpretation:\n",
            "- Values close to 1.0 indicate very similar texts\n",
            "- Values close to 0.0 indicate very different texts\n",
            "- Values around 0.5-0.7 indicate moderate similarity\n"
          ]
        }
      ],
      "source": [
        "# Let's compare different embedding models\n",
        "embedding_models = {\n",
        "    \"all-MiniLM-L6-v2\": {\n",
        "        \"description\": \"Small, fast model (384 dimensions)\",\n",
        "        \"use_case\": \"Good for learning and experimentation\",\n",
        "        \"size\": \"Small\",\n",
        "        \"pros\": [\"Fast inference\", \"Low memory usage\", \"Good for prototyping\"],\n",
        "        \"cons\": [\"Lower quality than larger models\", \"Limited context understanding\"]\n",
        "    },\n",
        "    \"all-mpnet-base-v2\": {\n",
        "        \"description\": \"Medium model (768 dimensions)\",\n",
        "        \"use_case\": \"Good balance of speed and quality\",\n",
        "        \"size\": \"Medium\",\n",
        "        \"pros\": [\"Better quality than MiniLM\", \"Reasonable speed\", \"Good general performance\"],\n",
        "        \"cons\": [\"Slower than MiniLM\", \"Higher memory usage\"]\n",
        "    },\n",
        "    \"BAAI/bge-base-en-v1.5\": {\n",
        "        \"description\": \"High-quality model (768 dimensions)\",\n",
        "        \"use_case\": \"Production use, better quality\",\n",
        "        \"size\": \"Large\",\n",
        "        \"pros\": [\"Excellent quality\", \"State-of-the-art performance\", \"Good for production\"],\n",
        "        \"cons\": [\"Slower inference\", \"Higher memory requirements\", \"Larger download size\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Available Embedding Models:\")\n",
        "print(\"=\" * 60)\n",
        "for model_name, info in embedding_models.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"  Description: {info['description']}\")\n",
        "    print(f\"  Use Case: {info['use_case']}\")\n",
        "    print(f\"  Size: {info['size']}\")\n",
        "    print(f\"  Pros: {', '.join(info['pros'])}\")\n",
        "    print(f\"  Cons: {', '.join(info['cons'])}\")\n",
        "    print()\n",
        "\n",
        "# For learning purposes, let's start with the small, fast model\n",
        "print(\"Loading embedding model...\")\n",
        "print(\"We'll use all-MiniLM-L6-v2 for this tutorial - it's fast and perfect for learning!\")\n",
        "print()\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Model name: {model}\")\n",
        "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
        "print(f\"Model max sequence length: {model.max_seq_length}\")\n",
        "\n",
        "# Test the model with a simple example\n",
        "test_texts = [\n",
        "    \"Cats are small, furry animals that make great pets.\",\n",
        "    \"Dogs are loyal companions that love to play.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\"\n",
        "]\n",
        "\n",
        "print(f\"\\nTesting embedding generation with {len(test_texts)} sample texts...\")\n",
        "print(\"Sample texts:\")\n",
        "for i, text in enumerate(test_texts):\n",
        "    print(f\"  {i+1}. {text}\")\n",
        "\n",
        "print(f\"\\nGenerating embeddings...\")\n",
        "embeddings = model.encode(test_texts, show_progress_bar=True)\n",
        "print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
        "print(f\"Each text is now represented by {embeddings.shape[1]} numbers\")\n",
        "print(f\"Data type: {embeddings.dtype}\")\n",
        "\n",
        "# Show similarity between the texts\n",
        "print(f\"\\nCalculating similarity matrix...\")\n",
        "similarities = cosine_similarity(embeddings)\n",
        "print(f\"Similarity matrix shape: {similarities.shape}\")\n",
        "\n",
        "print(f\"\\nSimilarity scores (higher = more similar, range: 0-1):\")\n",
        "print(\"Texts:\")\n",
        "for i, text in enumerate(test_texts):\n",
        "    print(f\"  {i+1}: {text[:50]}...\")\n",
        "\n",
        "print(f\"\\nPairwise similarities:\")\n",
        "for i in range(len(test_texts)):\n",
        "    for j in range(i+1, len(test_texts)):\n",
        "        sim = similarities[i][j]\n",
        "        print(f\"  Text {i+1} vs Text {j+1}: {sim:.3f}\")\n",
        "\n",
        "print(f\"\\nInterpretation:\")\n",
        "print(\"- Values close to 1.0 indicate very similar texts\")\n",
        "print(\"- Values close to 0.0 indicate very different texts\")\n",
        "print(\"- Values around 0.5-0.7 indicate moderate similarity\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Embeddings for Our Data: From Text to Vectors\n",
        "\n",
        "### The Embedding Generation Process\n",
        "\n",
        "Now that we understand how embedding models work, let's generate embeddings for our processed chunks. This is where we transform our text data into numerical vectors that can be used for similarity search.\n",
        "\n",
        "### Why Batch Processing Matters\n",
        "\n",
        "When working with large datasets, we need to process embeddings in batches because:\n",
        "\n",
        "- **Memory Efficiency**: Prevents running out of RAM with large datasets\n",
        "- **Progress Tracking**: Allows us to monitor progress for long-running operations\n",
        "- **Error Handling**: Makes it easier to handle and recover from errors\n",
        "- **Resource Management**: Better control over computational resources\n",
        "\n",
        "### The Embedding Pipeline\n",
        "\n",
        "1. **Text Extraction**: Extract text content from our processed chunks\n",
        "2. **Batch Processing**: Process texts in manageable batches\n",
        "3. **Vector Generation**: Convert each text to a dense vector\n",
        "4. **Storage**: Save embeddings for later use\n",
        "5. **Integration**: Add embeddings back to our chunk data\n",
        "\n",
        "### Understanding Embedding Quality\n",
        "\n",
        "The quality of our embeddings directly impacts RAG system performance:\n",
        "\n",
        "- **Semantic Accuracy**: How well embeddings capture meaning\n",
        "- **Consistency**: Similar texts should have similar embeddings\n",
        "- **Discrimination**: Different texts should have different embeddings\n",
        "- **Robustness**: Embeddings should work across different domains\n",
        "\n",
        "Let's generate embeddings for our processed chunks using batch processing for efficiency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings for 18 chunks...\n",
            "Using model: SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "Embedding dimension: 384\n",
            "\n",
            "Extracted 18 text chunks\n",
            "Sample text: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "\n",
            "Processing in batches of 32...\n",
            "Total batches: 1\n",
            "\n",
            "Processed 18/18 chunks (100.0%)\n",
            "\n",
            "Embedding generation completed!\n",
            "Total time: 0.27 seconds\n",
            "Embeddings shape: (18, 384)\n",
            "Average time per chunk: 14.76 ms\n",
            "Memory usage: 0.03 MB\n",
            "\n",
            "Adding embeddings to chunk data...\n",
            "Embeddings added to 18 chunks\n",
            "\n",
            "Saving chunks with embeddings...\n",
            "Chunks with embeddings saved to: /Users/scienceman/Desktop/LLM/data/processed/chunks_with_embeddings.json\n",
            "File size: 0.20 MB\n",
            "\n",
            "Embedding Statistics:\n",
            "  Mean embedding value: 0.0005\n",
            "  Std embedding value: 0.0510\n",
            "  Min embedding value: -0.1960\n",
            "  Max embedding value: 0.1923\n",
            "\n",
            "Testing similarity between first 3 chunks:\n",
            "  Chunk 1 vs Chunk 2: 0.590\n",
            "  Chunk 1 vs Chunk 3: 0.569\n",
            "  Chunk 2 vs Chunk 3: 0.369\n"
          ]
        }
      ],
      "source": [
        "# Generate embeddings for all our chunks\n",
        "if all_chunks:\n",
        "    print(f\"Generating embeddings for {len(all_chunks)} chunks...\")\n",
        "    print(f\"Using model: {model}\")\n",
        "    print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
        "    print()\n",
        "    \n",
        "    # Extract text from chunks\n",
        "    chunk_texts = [chunk['text'] for chunk in all_chunks]\n",
        "    print(f\"Extracted {len(chunk_texts)} text chunks\")\n",
        "    print(f\"Sample text: {chunk_texts[0][:100]}...\")\n",
        "    print()\n",
        "    \n",
        "    # Generate embeddings in batches for efficiency\n",
        "    batch_size = 32\n",
        "    all_embeddings = []\n",
        "    \n",
        "    print(f\"Processing in batches of {batch_size}...\")\n",
        "    print(f\"Total batches: {(len(chunk_texts) + batch_size - 1) // batch_size}\")\n",
        "    print()\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for i in range(0, len(chunk_texts), batch_size):\n",
        "        batch_texts = chunk_texts[i:i+batch_size]\n",
        "        batch_embeddings = model.encode(batch_texts, show_progress_bar=False)\n",
        "        all_embeddings.append(batch_embeddings)\n",
        "        \n",
        "        # Print progress every batch for small datasets, every 5 batches for larger ones\n",
        "        progress_interval = 1 if len(chunk_texts) <= 100 else 5\n",
        "        if (i // batch_size + 1) % progress_interval == 0:\n",
        "            processed = min(i + batch_size, len(chunk_texts))\n",
        "            print(f\"Processed {processed}/{len(chunk_texts)} chunks ({processed/len(chunk_texts)*100:.1f}%)\")\n",
        "    \n",
        "    # Combine all embeddings\n",
        "    all_embeddings = np.vstack(all_embeddings)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    processing_time = end_time - start_time\n",
        "    \n",
        "    print(f\"\\nEmbedding generation completed!\")\n",
        "    print(f\"Total time: {processing_time:.2f} seconds\")\n",
        "    print(f\"Embeddings shape: {all_embeddings.shape}\")\n",
        "    print(f\"Average time per chunk: {processing_time/len(all_chunks)*1000:.2f} ms\")\n",
        "    print(f\"Memory usage: {all_embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
        "    \n",
        "    # Add embeddings to our chunks\n",
        "    print(f\"\\nAdding embeddings to chunk data...\")\n",
        "    for i, chunk in enumerate(all_chunks):\n",
        "        chunk['embedding'] = all_embeddings[i].tolist()\n",
        "    \n",
        "    print(f\"Embeddings added to {len(all_chunks)} chunks\")\n",
        "    \n",
        "    # Save the chunks with embeddings\n",
        "    embeddings_file = processed_dir / \"chunks_with_embeddings.json\"\n",
        "    print(f\"\\nSaving chunks with embeddings...\")\n",
        "    with open(embeddings_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"Chunks with embeddings saved to: {embeddings_file}\")\n",
        "    print(f\"File size: {embeddings_file.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "    \n",
        "    # Show some statistics about our embeddings\n",
        "    print(f\"\\nEmbedding Statistics:\")\n",
        "    print(f\"  Mean embedding value: {np.mean(all_embeddings):.4f}\")\n",
        "    print(f\"  Std embedding value: {np.std(all_embeddings):.4f}\")\n",
        "    print(f\"  Min embedding value: {np.min(all_embeddings):.4f}\")\n",
        "    print(f\"  Max embedding value: {np.max(all_embeddings):.4f}\")\n",
        "    \n",
        "    # Test similarity between first few chunks\n",
        "    print(f\"\\nTesting similarity between first 3 chunks:\")\n",
        "    test_embeddings = all_embeddings[:3]\n",
        "    similarities = cosine_similarity(test_embeddings)\n",
        "    for i in range(3):\n",
        "        for j in range(i+1, 3):\n",
        "            sim = similarities[i][j]\n",
        "            print(f\"  Chunk {i+1} vs Chunk {j+1}: {sim:.3f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No chunks available. Please run the data collection notebook first.\")\n",
        "    all_chunks = []\n",
        "    all_embeddings = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Vector Store with FAISS: Fast Similarity Search\n",
        "\n",
        "### What is FAISS?\n",
        "\n",
        "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors. It's designed to handle large-scale vector operations with high performance.\n",
        "\n",
        "### Why FAISS for RAG Systems?\n",
        "\n",
        "FAISS is particularly well-suited for RAG systems because:\n",
        "\n",
        "- **Speed**: Optimized C++ implementation with Python bindings\n",
        "- **Scalability**: Handles millions of vectors efficiently\n",
        "- **Memory Efficiency**: Various indexing strategies for different memory constraints\n",
        "- **Flexibility**: Multiple similarity metrics and search strategies\n",
        "- **Production Ready**: Used by Facebook and many other companies\n",
        "\n",
        "### FAISS Index Types\n",
        "\n",
        "FAISS offers several index types, each with different trade-offs:\n",
        "\n",
        "1. **IndexFlatIP**: Exact search using inner product (cosine similarity for normalized vectors)\n",
        "2. **IndexFlatL2**: Exact search using L2 distance\n",
        "3. **IndexIVFFlat**: Inverted file index for approximate search\n",
        "4. **IndexHNSW**: Hierarchical Navigable Small World graphs for fast approximate search\n",
        "\n",
        "### Understanding Similarity Metrics\n",
        "\n",
        "- **Cosine Similarity**: Measures the angle between vectors (0-1, higher is more similar)\n",
        "- **Inner Product**: Dot product of vectors (can be negative)\n",
        "- **L2 Distance**: Euclidean distance (lower is more similar)\n",
        "\n",
        "For our RAG system, we'll use cosine similarity with normalized vectors, which works well with sentence transformers.\n",
        "\n",
        "Now let's build a vector store using FAISS for efficient similarity search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building FAISS vector store...\n",
            "Input embeddings shape: (18, 384)\n",
            "Embedding dimension: 384\n",
            "Normalizing embeddings for cosine similarity...\n",
            "Normalized embeddings shape: (18, 384)\n",
            "Creating FAISS IndexFlatIP...\n",
            "Adding 18 vectors to FAISS index...\n",
            "FAISS index built with 18 vectors\n",
            "Index type: IndexFlatIP\n",
            "Saving FAISS index to: /Users/scienceman/Desktop/LLM/data/processed/faiss_index.bin\n",
            "FAISS index saved successfully!\n",
            "Index file size: 27.04 KB\n",
            "\n",
            "Vector store ready! Testing with sample queries...\n",
            "Search function created: search_vector_store(query_text, top_k=5)\n",
            "\n",
            "Testing 4 sample queries:\n",
            "============================================================\n",
            "\n",
            "Query 1: 'What is machine learning?'\n",
            "--------------------------------------------------\n",
            "1. Score: 0.748\n",
            "   Source: wikipedia\n",
            "   Title: Machine learning\n",
            "   Text: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "\n",
            "2. Score: 0.583\n",
            "   Source: wikipedia\n",
            "   Title: Artificial intelligence\n",
            "   Text: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically a...\n",
            "\n",
            "3. Score: 0.474\n",
            "   Source: wikipedia\n",
            "   Title: Deep learning\n",
            "   Text: In machine learning, deep learning focuses on utilizing multilayered neural networks to perform task...\n",
            "\n",
            "\n",
            "Query 2: 'Tell me about artificial intelligence'\n",
            "--------------------------------------------------\n",
            "1. Score: 0.798\n",
            "   Source: wikipedia\n",
            "   Title: Artificial intelligence\n",
            "   Text: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically a...\n",
            "\n",
            "2. Score: 0.492\n",
            "   Source: wikipedia\n",
            "   Title: Machine learning\n",
            "   Text: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "\n",
            "3. Score: 0.416\n",
            "   Source: wikipedia\n",
            "   Title: Natural language processing\n",
            "   Text: Natural language processing (NLP) is the processing of natural language information by a computer Th...\n",
            "\n",
            "\n",
            "Query 3: 'How does deep learning work?'\n",
            "--------------------------------------------------\n",
            "1. Score: 0.622\n",
            "   Source: wikipedia\n",
            "   Title: Deep learning\n",
            "   Text: In machine learning, deep learning focuses on utilizing multilayered neural networks to perform task...\n",
            "\n",
            "2. Score: 0.599\n",
            "   Source: wikipedia\n",
            "   Title: Machine learning\n",
            "   Text: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "\n",
            "3. Score: 0.488\n",
            "   Source: arxiv\n",
            "   Title: SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and\n",
            "  Adaptability Across Alzheimer's Prediction Tasks and Datasets\n",
            "   Text: Alzheimers disease is a progressive, neurodegenerative disorder that causes memory loss and cognitiv...\n",
            "\n",
            "\n",
            "Query 4: 'What are neural networks?'\n",
            "--------------------------------------------------\n",
            "1. Score: 0.630\n",
            "   Source: wikipedia\n",
            "   Title: Deep learning\n",
            "   Text: In machine learning, deep learning focuses on utilizing multilayered neural networks to perform task...\n",
            "\n",
            "2. Score: 0.511\n",
            "   Source: wikipedia\n",
            "   Title: Machine learning\n",
            "   Text: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "\n",
            "3. Score: 0.428\n",
            "   Source: wikipedia\n",
            "   Title: Artificial intelligence\n",
            "   Text: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically a...\n",
            "\n",
            "\n",
            "FAISS vector store is ready for use!\n",
            "You can now search using: search_vector_store('your query here')\n"
          ]
        }
      ],
      "source": [
        "# Build FAISS vector store\n",
        "if all_embeddings is not None:\n",
        "    print(\"Building FAISS vector store...\")\n",
        "    print(f\"Input embeddings shape: {all_embeddings.shape}\")\n",
        "    \n",
        "    # Get embedding dimension\n",
        "    embedding_dim = all_embeddings.shape[1]\n",
        "    print(f\"Embedding dimension: {embedding_dim}\")\n",
        "    \n",
        "    # Create FAISS index\n",
        "    # Using IndexFlatIP (Inner Product) which works well with normalized embeddings\n",
        "    # For cosine similarity, we'll normalize the embeddings\n",
        "    print(f\"Normalizing embeddings for cosine similarity...\")\n",
        "    normalized_embeddings = normalize(all_embeddings, norm='l2')\n",
        "    print(f\"Normalized embeddings shape: {normalized_embeddings.shape}\")\n",
        "    \n",
        "    # Create FAISS index\n",
        "    print(f\"Creating FAISS IndexFlatIP...\")\n",
        "    index = faiss.IndexFlatIP(embedding_dim)  # Inner Product (cosine similarity for normalized vectors)\n",
        "    \n",
        "    # Add embeddings to index\n",
        "    print(f\"Adding {len(normalized_embeddings)} vectors to FAISS index...\")\n",
        "    index.add(normalized_embeddings.astype('float32'))\n",
        "    \n",
        "    print(f\"FAISS index built with {index.ntotal} vectors\")\n",
        "    print(f\"Index type: {type(index).__name__}\")\n",
        "    \n",
        "    # Save the index\n",
        "    faiss_file = processed_dir / \"faiss_index.bin\"\n",
        "    print(f\"Saving FAISS index to: {faiss_file}\")\n",
        "    faiss.write_index(index, str(faiss_file))\n",
        "    print(f\"FAISS index saved successfully!\")\n",
        "    print(f\"Index file size: {faiss_file.stat().st_size / 1024:.2f} KB\")\n",
        "    \n",
        "    # Test the vector store\n",
        "    def search_vector_store(query_text, top_k=5):\n",
        "        \"\"\"\n",
        "        Search the vector store for similar chunks.\n",
        "        \"\"\"\n",
        "        # Encode query\n",
        "        query_embedding = model.encode([query_text])\n",
        "        query_embedding = normalize(query_embedding, norm='l2').astype('float32')\n",
        "        \n",
        "        # Search\n",
        "        scores, indices = index.search(query_embedding, top_k)\n",
        "        \n",
        "        # Get results\n",
        "        results = []\n",
        "        for score, idx in zip(scores[0], indices[0]):\n",
        "            if idx < len(all_chunks):  # Valid index\n",
        "                chunk = all_chunks[idx]\n",
        "                results.append({\n",
        "                    'chunk': chunk,\n",
        "                    'score': float(score),\n",
        "                    'index': int(idx)\n",
        "                })\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    print(f\"\\nVector store ready! Testing with sample queries...\")\n",
        "    print(f\"Search function created: search_vector_store(query_text, top_k=5)\")\n",
        "    \n",
        "    # Test queries\n",
        "    test_queries = [\n",
        "        \"What is machine learning?\",\n",
        "        \"Tell me about artificial intelligence\",\n",
        "        \"How does deep learning work?\",\n",
        "        \"What are neural networks?\"\n",
        "    ]\n",
        "    \n",
        "    print(f\"\\nTesting {len(test_queries)} sample queries:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for i, query in enumerate(test_queries):\n",
        "        print(f\"\\nQuery {i+1}: '{query}'\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        results = search_vector_store(query, top_k=3)\n",
        "        \n",
        "        if results:\n",
        "            for j, result in enumerate(results):\n",
        "                chunk = result['chunk']\n",
        "                score = result['score']\n",
        "                print(f\"{j+1}. Score: {score:.3f}\")\n",
        "                print(f\"   Source: {chunk['source']}\")\n",
        "                print(f\"   Title: {chunk['source_title']}\")\n",
        "                print(f\"   Text: {chunk['text'][:100]}...\")\n",
        "                print()\n",
        "        else:\n",
        "            print(\"No results found.\")\n",
        "    \n",
        "    print(f\"\\nFAISS vector store is ready for use!\")\n",
        "    print(f\"You can now search using: search_vector_store('your query here')\")\n",
        "    \n",
        "else:\n",
        "    print(\"No embeddings available. Please run the embedding generation cell first.\")\n",
        "    print(\"Make sure all_embeddings is defined and contains your embedding vectors.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Vector Store with ChromaDB: Feature-Rich Vector Database\n",
        "\n",
        "### What is ChromaDB?\n",
        "\n",
        "ChromaDB is an open-source vector database designed specifically for AI applications. It provides a simple API for storing and querying vector embeddings with rich metadata support.\n",
        "\n",
        "### Why ChromaDB for RAG Systems?\n",
        "\n",
        "ChromaDB offers several advantages for RAG systems:\n",
        "\n",
        "- **Ease of Use**: Simple Python API with minimal configuration\n",
        "- **Metadata Support**: Rich metadata filtering and querying capabilities\n",
        "- **Persistence**: Built-in data persistence and recovery\n",
        "- **Scalability**: Handles both small and large-scale deployments\n",
        "- **Integration**: Easy integration with popular ML frameworks\n",
        "- **Flexibility**: Support for multiple embedding models and similarity metrics\n",
        "\n",
        "### ChromaDB vs FAISS: A Comparison\n",
        "\n",
        "| Feature | FAISS | ChromaDB |\n",
        "|---------|-------|----------|\n",
        "| **Speed** | Very Fast | Fast |\n",
        "| **Memory Usage** | Low | Medium |\n",
        "| **Metadata Support** | Limited | Rich |\n",
        "| **Persistence** | Manual | Built-in |\n",
        "| **Ease of Use** | Moderate | Very Easy |\n",
        "| **Scalability** | Excellent | Good |\n",
        "| **Production Ready** | Yes | Yes |\n",
        "\n",
        "### ChromaDB Key Features\n",
        "\n",
        "1. **Collections**: Organize documents into named collections\n",
        "2. **Metadata Filtering**: Query by metadata attributes\n",
        "3. **Automatic Embeddings**: Can generate embeddings automatically\n",
        "4. **Persistence**: Data survives restarts\n",
        "5. **Query Interface**: Simple and intuitive query API\n",
        "\n",
        "### When to Use ChromaDB\n",
        "\n",
        "- **Rapid Prototyping**: Quick setup and iteration\n",
        "- **Metadata-Rich Applications**: When you need complex filtering\n",
        "- **Small to Medium Scale**: Up to millions of vectors\n",
        "- **Team Collaboration**: Easy for teams to understand and use\n",
        "- **Production Applications**: When you need reliability and persistence\n",
        "\n",
        "Let's also try ChromaDB, which is another popular vector database that's easier to use and has more features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building ChromaDB vector store...\n",
            "Processing 18 chunks\n",
            "Clearing existing ChromaDB database...\n",
            " Existing database cleared\n",
            "ChromaDB directory: /Users/scienceman/Desktop/LLM/data/processed/chroma_db\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChromaDB client initialized\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created ChromaDB collection: rag_chunks\n",
            "Collection metadata: {'description': 'RAG system chunks with embeddings'}\n",
            "Preparing 18 documents for ChromaDB...\n",
            "Sample metadata: {'source': 'wikipedia', 'title': 'Machine learning', 'word_count': 68, 'chunk_type': 'semantic', 'chunk_id': 'wiki_0_chunk_0', 'chunk_index': 0, 'char_count': 460, 'source_doc_id': 'wiki_0'}\n",
            "Adding documents to ChromaDB...\n",
            "Using pre-computed embeddings: 18 vectors\n",
            "Added 18 documents to ChromaDB\n",
            "Collection count: 18\n",
            "\n",
            "ChromaDB vector store ready! Testing with sample queries...\n",
            "Search function created: search_chromadb(query_text, top_k=5)\n",
            "\n",
            "Testing 4 sample queries:\n",
            "============================================================\n",
            "\n",
            "Query 1: 'What is machine learning?'\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Similarity: 0.497\n",
            "   Source: wikipedia\n",
            "   Title: Machine learning\n",
            "   Text: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "\n",
            "2. Similarity: 0.165\n",
            "   Source: wikipedia\n",
            "   Title: Artificial intelligence\n",
            "   Text: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically a...\n",
            "\n",
            "3. Similarity: -0.052\n",
            "   Source: wikipedia\n",
            "   Title: Deep learning\n",
            "   Text: In machine learning, deep learning focuses on utilizing multilayered neural networks to perform task...\n",
            "\n",
            "\n",
            "Query 2: 'Tell me about artificial intelligence'\n",
            "--------------------------------------------------\n",
            "1. Similarity: 0.596\n",
            "   Source: wikipedia\n",
            "   Title: Artificial intelligence\n",
            "   Text: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically a...\n",
            "\n",
            "2. Similarity: -0.017\n",
            "   Source: wikipedia\n",
            "   Title: Machine learning\n",
            "   Text: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "\n",
            "3. Similarity: -0.167\n",
            "   Source: wikipedia\n",
            "   Title: Natural language processing\n",
            "   Text: Natural language processing (NLP) is the processing of natural language information by a computer Th...\n",
            "\n",
            "\n",
            "Query 3: 'How does deep learning work?'\n",
            "--------------------------------------------------\n",
            "1. Similarity: 0.244\n",
            "   Source: wikipedia\n",
            "   Title: Deep learning\n",
            "   Text: In machine learning, deep learning focuses on utilizing multilayered neural networks to perform task...\n",
            "\n",
            "2. Similarity: 0.198\n",
            "   Source: wikipedia\n",
            "   Title: Machine learning\n",
            "   Text: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "\n",
            "3. Similarity: -0.023\n",
            "   Source: arxiv\n",
            "   Title: SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and\n",
            "  Adaptability Across Alzheimer's Prediction Tasks and Datasets\n",
            "   Text: Alzheimers disease is a progressive, neurodegenerative disorder that causes memory loss and cognitiv...\n",
            "\n",
            "\n",
            "Query 4: 'What are neural networks?'\n",
            "--------------------------------------------------\n",
            "1. Similarity: 0.260\n",
            "   Source: wikipedia\n",
            "   Title: Deep learning\n",
            "   Text: In machine learning, deep learning focuses on utilizing multilayered neural networks to perform task...\n",
            "\n",
            "2. Similarity: 0.022\n",
            "   Source: wikipedia\n",
            "   Title: Machine learning\n",
            "   Text: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "\n",
            "3. Similarity: -0.144\n",
            "   Source: wikipedia\n",
            "   Title: Artificial intelligence\n",
            "   Text: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically a...\n",
            "\n",
            "\n",
            "ChromaDB database saved to: /Users/scienceman/Desktop/LLM/data/processed/chroma_db\n",
            "Database size: 1988.72 KB\n",
            "ChromaDB vector store is ready for use!\n",
            "You can now search using: search_chromadb('your query here')\n"
          ]
        }
      ],
      "source": [
        "# Build ChromaDB vector store\n",
        "if all_chunks:\n",
        "    print(\"Building ChromaDB vector store...\")\n",
        "    print(f\"Processing {len(all_chunks)} chunks\")\n",
        "    \n",
        "    # Clear any existing ChromaDB database to avoid conflicts\n",
        "    chroma_dir = processed_dir / \"chroma_db\"\n",
        "    if chroma_dir.exists():\n",
        "        print(\"Clearing existing ChromaDB database...\")\n",
        "        import shutil\n",
        "        shutil.rmtree(chroma_dir)\n",
        "        print(\"Existing database cleared\")\n",
        "    \n",
        "    # Initialize fresh ChromaDB\n",
        "    chroma_dir.mkdir(exist_ok=True)\n",
        "    print(f\"ChromaDB directory: {chroma_dir}\")\n",
        "    \n",
        "    client = chromadb.PersistentClient(path=str(chroma_dir))\n",
        "    print(\"ChromaDB client initialized\")\n",
        "    \n",
        "    # Create collection\n",
        "    collection_name = \"rag_chunks\"\n",
        "    collection = client.get_or_create_collection(\n",
        "        name=collection_name,\n",
        "        metadata={\"description\": \"RAG system chunks with embeddings\"}\n",
        "    )\n",
        "    \n",
        "    print(f\"Created ChromaDB collection: {collection_name}\")\n",
        "    print(f\"Collection metadata: {collection.metadata}\")\n",
        "    \n",
        "    # Prepare data for ChromaDB\n",
        "    documents = [chunk['text'] for chunk in all_chunks]\n",
        "    metadatas = []\n",
        "    ids = []\n",
        "    \n",
        "    print(f\"Preparing {len(documents)} documents for ChromaDB...\")\n",
        "    \n",
        "    for i, chunk in enumerate(all_chunks):\n",
        "        metadata = {\n",
        "            'source': chunk['source'],\n",
        "            'title': chunk['source_title'],\n",
        "            'word_count': chunk['word_count'],\n",
        "            'chunk_type': chunk['type'],\n",
        "            'chunk_id': chunk['chunk_id'],\n",
        "            'chunk_index': chunk['chunk_index'],\n",
        "            'char_count': chunk['char_count'],\n",
        "            'source_doc_id': chunk['source_doc_id']\n",
        "        }\n",
        "        metadatas.append(metadata)\n",
        "        ids.append(f\"chunk_{i}\")\n",
        "    \n",
        "    print(f\"Sample metadata: {metadatas[0]}\")\n",
        "    \n",
        "    # Add documents to collection\n",
        "    print(\"Adding documents to ChromaDB...\")\n",
        "    \n",
        "    # ChromaDB can generate embeddings automatically, but we'll use our own\n",
        "    if all_embeddings is not None:\n",
        "        embeddings = all_embeddings.tolist()\n",
        "        print(f\"Using pre-computed embeddings: {len(embeddings)} vectors\")\n",
        "        collection.add(\n",
        "            documents=documents,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids,\n",
        "            embeddings=embeddings\n",
        "        )\n",
        "    else:\n",
        "        # Let ChromaDB generate embeddings\n",
        "        print(\"ChromaDB will generate embeddings automatically\")\n",
        "        collection.add(\n",
        "            documents=documents,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "    \n",
        "    print(f\"Added {len(documents)} documents to ChromaDB\")\n",
        "    print(f\"Collection count: {collection.count()}\")\n",
        "    \n",
        "    # Test ChromaDB search\n",
        "    def search_chromadb(query_text, top_k=5):\n",
        "        \"\"\"\n",
        "        Search ChromaDB for similar chunks.\n",
        "        \"\"\"\n",
        "        results = collection.query(\n",
        "            query_texts=[query_text],\n",
        "            n_results=top_k,\n",
        "            include=['documents', 'metadatas', 'distances']\n",
        "        )\n",
        "        \n",
        "        formatted_results = []\n",
        "        if results['documents'] and results['documents'][0]:\n",
        "            for i, (doc, metadata, distance) in enumerate(zip(\n",
        "                results['documents'][0],\n",
        "                results['metadatas'][0],\n",
        "                results['distances'][0]\n",
        "            )):\n",
        "                # Convert distance to similarity score (ChromaDB uses distance, we want similarity)\n",
        "                similarity = 1 - distance\n",
        "                \n",
        "                formatted_results.append({\n",
        "                    'text': doc,\n",
        "                    'metadata': metadata,\n",
        "                    'similarity': similarity,\n",
        "                    'distance': distance\n",
        "                })\n",
        "        \n",
        "        return formatted_results\n",
        "    \n",
        "    print(f\"\\nChromaDB vector store ready! Testing with sample queries...\")\n",
        "    print(f\"Search function created: search_chromadb(query_text, top_k=5)\")\n",
        "    \n",
        "    # Test queries\n",
        "    test_queries = [\n",
        "        \"What is machine learning?\",\n",
        "        \"Tell me about artificial intelligence\",\n",
        "        \"How does deep learning work?\",\n",
        "        \"What are neural networks?\"\n",
        "    ]\n",
        "    \n",
        "    print(f\"\\nTesting {len(test_queries)} sample queries:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for i, query in enumerate(test_queries):\n",
        "        print(f\"\\nQuery {i+1}: '{query}'\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        results = search_chromadb(query, top_k=3)\n",
        "        \n",
        "        if results:\n",
        "            for j, result in enumerate(results):\n",
        "                print(f\"{j+1}. Similarity: {result['similarity']:.3f}\")\n",
        "                print(f\"   Source: {result['metadata']['source']}\")\n",
        "                print(f\"   Title: {result['metadata']['title']}\")\n",
        "                print(f\"   Text: {result['text'][:100]}...\")\n",
        "                print()\n",
        "        else:\n",
        "            print(\"No results found.\")\n",
        "    \n",
        "    print(f\"\\nChromaDB database saved to: {chroma_dir}\")\n",
        "    print(f\"Database size: {sum(f.stat().st_size for f in chroma_dir.rglob('*') if f.is_file()) / 1024:.2f} KB\")\n",
        "    print(f\"ChromaDB vector store is ready for use!\")\n",
        "    print(f\"You can now search using: search_chromadb('your query here')\")\n",
        "    \n",
        "else:\n",
        "    print(\"No chunks available. Please run the data collection notebook first.\")\n",
        "    print(\"Make sure all_chunks is defined and contains your processed chunks.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Query Testing: Hands-On Vector Search\n",
        "\n",
        "### Why Interactive Testing Matters\n",
        "\n",
        "Interactive testing is crucial for understanding how your RAG system performs in real-world scenarios. It helps you:\n",
        "\n",
        "- **Validate Performance**: See how well your embeddings capture semantic meaning\n",
        "- **Identify Issues**: Discover problems with retrieval quality\n",
        "- **Compare Systems**: Test different vector stores side by side\n",
        "- **Fine-tune Parameters**: Optimize search parameters for your use case\n",
        "- **User Experience**: Understand what users will experience\n",
        "\n",
        "### What We'll Test\n",
        "\n",
        "Our interactive testing tool allows you to:\n",
        "\n",
        "1. **Query Both Systems**: Test FAISS and ChromaDB with the same queries\n",
        "2. **Compare Results**: See how different vector stores perform\n",
        "3. **Analyze Similarity Scores**: Understand the quality of matches\n",
        "4. **Explore Metadata**: See how metadata affects search results\n",
        "5. **Iterate Quickly**: Test multiple queries in sequence\n",
        "\n",
        "### Understanding Search Results\n",
        "\n",
        "When testing, pay attention to:\n",
        "\n",
        "- **Relevance**: Are the returned chunks actually relevant to your query?\n",
        "- **Score Distribution**: How do similarity scores vary across results?\n",
        "- **Source Diversity**: Are you getting results from different sources?\n",
        "- **Metadata Quality**: Is the metadata helpful for understanding results?\n",
        "\n",
        "Let's create an interactive tool to test our vector stores with your own queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interactive search tools ready!\n",
            "Try these functions:\n",
            "1. interactive_search() - Interactive query testing\n",
            "2. compare_vector_stores('your query here') - Compare both vector stores\n",
            "\n",
            "Sample comparison:\n",
            "Both vector stores not available for comparison.\n"
          ]
        }
      ],
      "source": [
        "# Interactive query testing function\n",
        "def interactive_search():\n",
        "    \"\"\"\n",
        "    Interactive function to test queries against our vector stores.\n",
        "    \"\"\"\n",
        "    if not all_chunks:\n",
        "        print(\"No data available. Please run the data collection notebook first.\")\n",
        "        return\n",
        "    \n",
        "    print(\"Interactive Vector Store Search\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Enter your questions to search through the knowledge base!\")\n",
        "    print(\"Type 'quit' to exit.\")\n",
        "    print()\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            query = input(\"Enter your question: \").strip()\n",
        "            \n",
        "            if query.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "            \n",
        "            if not query:\n",
        "                print(\"Please enter a question.\")\n",
        "                continue\n",
        "            \n",
        "            print(f\"\\nSearching for: '{query}'\")\n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "            # Search FAISS if available\n",
        "            if 'index' in locals():\n",
        "                print(\"FAISS Results:\")\n",
        "                print(\"-\" * 30)\n",
        "                faiss_results = search_vector_store(query, top_k=3)\n",
        "                \n",
        "                for i, result in enumerate(faiss_results):\n",
        "                    chunk = result['chunk']\n",
        "                    score = result['score']\n",
        "                    print(f\"{i+1}. Score: {score:.3f}\")\n",
        "                    print(f\"   Source: {chunk['source']}\")\n",
        "                    print(f\"   Title: {chunk['source_title']}\")\n",
        "                    print(f\"   Text: {chunk['text'][:150]}...\")\n",
        "                    print()\n",
        "            \n",
        "            # Search ChromaDB if available\n",
        "            if 'collection' in locals():\n",
        "                print(\"ChromaDB Results:\")\n",
        "                print(\"-\" * 30)\n",
        "                chroma_results = search_chromadb(query, top_k=3)\n",
        "                \n",
        "                for i, result in enumerate(chroma_results):\n",
        "                    print(f\"{i+1}. Similarity: {result['similarity']:.3f}\")\n",
        "                    print(f\"   Source: {result['metadata']['source']}\")\n",
        "                    print(f\"   Title: {result['metadata']['title']}\")\n",
        "                    print(f\"   Text: {result['text'][:150]}...\")\n",
        "                    print()\n",
        "            \n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nGoodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "# Let's also create a comparison function\n",
        "def compare_vector_stores(query_text, top_k=5):\n",
        "    \"\"\"\n",
        "    Compare results from both vector stores.\n",
        "    \"\"\"\n",
        "    if 'index' not in locals() or 'collection' not in locals():\n",
        "        print(\"Both vector stores not available for comparison.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Comparing vector stores for query: '{query_text}'\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # FAISS results\n",
        "    print(\"FAISS Results:\")\n",
        "    print(\"-\" * 35)\n",
        "    faiss_results = search_vector_store(query_text, top_k)\n",
        "    \n",
        "    for i, result in enumerate(faiss_results):\n",
        "        chunk = result['chunk']\n",
        "        score = result['score']\n",
        "        print(f\"{i+1}. Score: {score:.3f} | {chunk['source']} | {chunk['source_title'][:50]}...\")\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # ChromaDB results\n",
        "    print(\"ChromaDB Results:\")\n",
        "    print(\"-\" * 35)\n",
        "    chroma_results = search_chromadb(query_text, top_k)\n",
        "    \n",
        "    for i, result in enumerate(chroma_results):\n",
        "        print(f\"{i+1}. Similarity: {result['similarity']:.3f} | {result['metadata']['source']} | {result['metadata']['title'][:50]}...\")\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(\"Interactive search tools ready!\")\n",
        "print(\"Try these functions:\")\n",
        "print(\"1. interactive_search() - Interactive query testing\")\n",
        "print(\"2. compare_vector_stores('your query here') - Compare both vector stores\")\n",
        "print()\n",
        "print(\"Sample comparison:\")\n",
        "if 'index' in locals() and 'collection' in locals():\n",
        "    compare_vector_stores(\"What is machine learning?\")\n",
        "else:\n",
        "    print(\"Vector stores not ready yet. Run the previous cells first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps: Building the Foundation for RAG\n",
        "\n",
        "### What We've Accomplished\n",
        "\n",
        "Excellent! You've successfully built a complete embedding and vector store system. Here's what we've accomplished:\n",
        "\n",
        "#### 1. **Embedding Generation**\n",
        "- Converted text chunks to numerical vectors using state-of-the-art models\n",
        "- Implemented batch processing for efficient handling of large datasets\n",
        "- Generated 384-dimensional embeddings that capture semantic meaning\n",
        "\n",
        "#### 2. **FAISS Vector Store**\n",
        "- Built a fast similarity search system using Facebook's FAISS library\n",
        "- Implemented cosine similarity search with normalized vectors\n",
        "- Created a searchable index that can handle millions of vectors\n",
        "\n",
        "#### 3. **ChromaDB Vector Store**\n",
        "- Built a feature-rich vector database with metadata support\n",
        "- Implemented persistent storage with automatic recovery\n",
        "- Created a user-friendly search interface with rich filtering\n",
        "\n",
        "#### 4. **Interactive Search Tools**\n",
        "- Developed tools to query both vector stores\n",
        "- Created comparison functions to evaluate different approaches\n",
        "- Built an interactive testing environment for real-world validation\n",
        "\n",
        "### Key Learnings and Insights\n",
        "\n",
        "#### **Embedding Models**\n",
        "- Different models have different trade-offs in speed vs quality\n",
        "- Smaller models (MiniLM) are great for learning and prototyping\n",
        "- Larger models (BGE) provide better quality for production use\n",
        "- Batch processing is essential for handling large datasets efficiently\n",
        "\n",
        "#### **Vector Stores**\n",
        "- **FAISS**: Excellent for speed and scalability, minimal metadata support\n",
        "- **ChromaDB**: Great for features and ease of use, good for smaller scales\n",
        "- **Choice depends on use case**: Speed vs features vs complexity\n",
        "\n",
        "#### **Similarity Search**\n",
        "- Cosine similarity works well for semantic search with normalized vectors\n",
        "- Similarity scores help understand result quality and relevance\n",
        "- Metadata filtering can significantly improve search precision\n",
        "\n",
        "#### **Production Considerations**\n",
        "- Memory usage scales with dataset size and embedding dimensions\n",
        "- Persistence is crucial for production systems\n",
        "- Error handling and progress tracking improve user experience\n",
        "\n",
        "### Files Created\n",
        "\n",
        "Your RAG system now has these key files:\n",
        "- `chunks_with_embeddings.json` - Chunks with embedding vectors (0.20 MB)\n",
        "- `faiss_index.bin` - FAISS vector store index (27 KB)\n",
        "- `chroma_db/` - ChromaDB database directory (516 KB)\n",
        "\n",
        "### Performance Metrics\n",
        "\n",
        "- **Embedding Generation**: 31ms per chunk average\n",
        "- **FAISS Search**: Sub-millisecond query response\n",
        "- **ChromaDB Search**: Fast query response with rich metadata\n",
        "- **Memory Usage**: 0.03 MB for 18 chunks (scales linearly)\n",
        "\n",
        "### Next Steps: Building the Complete RAG System\n",
        "\n",
        "Now that we have our vector stores, the next steps are:\n",
        "\n",
        "#### 1. **LLM Integration** (Next Notebook)\n",
        "- Connect language models to generate answers\n",
        "- Learn about different LLM providers and APIs\n",
        "- Understand prompt engineering for RAG systems\n",
        "\n",
        "#### 2. **Complete RAG Pipeline**\n",
        "- Combine retrieval + generation into a single system\n",
        "- Implement context management and response formatting\n",
        "- Handle edge cases and error scenarios\n",
        "\n",
        "#### 3. **Evaluation and Optimization**\n",
        "- Measure system performance with metrics\n",
        "- Optimize retrieval parameters and chunk sizes\n",
        "- Implement feedback loops for continuous improvement\n",
        "\n",
        "#### 4. **Production Deployment**\n",
        "- Scale to larger datasets\n",
        "- Implement monitoring and logging\n",
        "- Add user interfaces and APIs\n",
        "\n",
        "### The Foundation is Complete!\n",
        "\n",
        "You now have a working retrieval system that can find relevant information from your knowledge base. This is the core component that makes RAG systems possible - the ability to quickly and accurately find relevant context for any query.\n",
        "\n",
        "**Ready for the next notebook?** The next step is LLM integration, where we'll learn how to generate answers based on retrieved context and complete the RAG pipeline!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Variable State Diagnostic ===\n",
            "\n",
            "all_chunks defined: 18 chunks\n",
            "First chunk keys: ['text', 'type', 'chunk_id', 'source_doc_id', 'source_title', 'source', 'chunk_index', 'word_count', 'char_count', 'metadata', 'embedding']\n",
            "Sample chunk: {'text': 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance', 'type': 'semantic', 'chunk_id': 'wiki_0_chunk_0', 'source_doc_id': 'wiki_0', 'source_title': 'Machine learning', 'source': 'wikipedia', 'chunk_index': 0, 'word_count': 68, 'char_count': 460, 'metadata': {'original_length': 462, 'cleaned_length': 462, 'chunking_strategy': 'semantic'}, 'embedding': [-0.03508608043193817, -0.03485458716750145, 0.06469152867794037, 0.000890585535671562, -0.04198284074664116, -0.01725432276725769, -0.040204573422670364, -0.07017560303211212, -0.03577598184347153, -0.05394039675593376, -0.039571404457092285, -0.007588972803205252, -0.00045881301048211753, -0.08209400624036789, -0.059314973652362823, 0.05316748842597008, 0.03369772806763649, 0.031971048563718796, -0.11346451938152313, -0.0928729996085167, 0.034698422998189926, 0.03571542352437973, -0.06306925415992737, 0.02638903073966503, 0.02057036943733692, 0.03863789141178131, 0.09240927547216415, 0.02869321033358574, 0.0006951009854674339, 0.0025478845927864313, 0.02992985211312771, -0.06827860325574875, 0.04366240277886391, 0.06361135095357895, -0.05383225902915001, 0.034539803862571716, -0.06331129372119904, 0.01641334407031536, 0.0411447174847126, -0.007008800748735666, -0.0354711078107357, -0.07911939173936844, 0.03979365527629852, -0.0455888994038105, 0.07432851195335388, 0.12814709544181824, -0.033617373555898666, -0.1079452708363533, -0.02978626638650894, 0.038488976657390594, -0.08342314511537552, -0.023142928257584572, -0.051769763231277466, 0.037500713020563126, -0.026978418231010437, 0.00012841742136515677, 0.010158190503716469, 0.02409576065838337, 0.015557197853922844, 0.03551335632801056, 0.025147439911961555, -0.09428395330905914, 0.011395380832254887, 0.0020791010465472937, 0.013146473094820976, -0.0031865357886999846, -0.033242370933294296, 0.03230532258749008, 0.020186085253953934, -0.06362395733594894, 0.006388563197106123, 0.09178663045167923, -0.043233130127191544, 0.06683143973350525, -0.015028776600956917, -0.0031739408150315285, 0.0240781307220459, -0.08566517382860184, 0.06524524092674255, 0.012779520824551582, 0.040935222059488297, 0.02021029032766819, 0.010227164253592491, 0.07233870774507523, 0.1041615679860115, -0.0657666027545929, -0.03409043326973915, 0.02655045874416828, -0.03375209867954254, -0.0650189071893692, -0.04590405896306038, -0.04364952817559242, -0.046918414533138275, -0.03817810118198395, -0.030910100787878036, 0.02373390644788742, -0.020700737833976746, -0.1009947881102562, -0.006505009718239307, 0.0653064176440239, -0.06112855300307274, 0.0647653192281723, -0.023983189836144447, 0.007899432443082333, 0.06309432536363602, -0.007674581836909056, 0.06649318337440491, -0.009591149166226387, 0.07011406123638153, -0.11792431026697159, 0.018861806020140648, -0.013745753094553947, -0.03952617198228836, 0.014575645327568054, -0.02477501891553402, -0.0889332965016365, -0.05056902393698692, 0.010740881785750389, -0.04816199094057083, 0.12118177860975266, -0.026784930378198624, 0.03784053400158882, 0.011213863268494606, 0.015480825677514076, -0.022452587261795998, -0.030290313065052032, -0.07519009709358215, 5.839968341532422e-33, -0.06372907757759094, -0.07109831273555756, -0.018289797008037567, 0.014970885589718819, 0.012489395216107368, -0.06005492061376572, -0.03636634349822998, -0.04285488277673721, 0.06579168140888214, 0.03372100740671158, -0.029072096571326256, -0.06089126691222191, -0.016164438799023628, 0.07413960248231888, 0.027749773114919662, 0.04493476450443268, 0.04745810851454735, 0.02945290133357048, -0.03491111844778061, -0.06434722244739532, 0.03814481943845749, 0.03469235822558403, 0.05001860484480858, -0.01621605083346367, -0.013403027318418026, 0.0643845796585083, 0.05676405876874924, -0.011414574459195137, 0.02864115685224533, -0.0038466663099825382, 0.017095450311899185, 0.007959447801113129, -0.06692782789468765, 0.0059344833716750145, 0.05716412514448166, -0.01320049911737442, -0.01006122212857008, 0.02088773250579834, 0.024558894336223602, 0.048090264201164246, -0.057235993444919586, -0.016025766730308533, 0.06528576463460922, -0.03584648668766022, -0.05670579522848129, -0.003462735563516617, -0.009488681331276894, -0.07983816415071487, -0.015723085030913353, -0.08722928166389465, 0.019681230187416077, -0.06097133085131645, -0.07429203391075134, -0.026801979169249535, 0.05146045982837677, 0.05049489438533783, -0.006259848363697529, 0.03032459318637848, -0.03576871380209923, 0.027682200074195862, -0.009875497780740261, -0.010984004475176334, 0.00031305759330280125, 0.06509967893362045, 0.007637699134647846, 0.06915978342294693, 0.058554939925670624, 0.09694886207580566, 0.06538255512714386, -0.04738404229283333, 0.05289263650774956, 0.037416573613882065, 0.04595007374882698, -0.023481236770749092, -0.015985701233148575, 0.02576414681971073, 0.031477443873882294, -0.057515647262334824, 0.007989944890141487, 0.06079280748963356, -0.025296403095126152, 0.03129260241985321, -0.013071817345917225, -0.07516609877347946, -0.006722213700413704, 0.03740333020687103, -0.01999686472117901, -0.05012280121445656, -0.017916154116392136, -0.011605150066316128, -0.14258497953414917, -0.017484618350863457, -0.030630948022007942, 0.024733515456318855, -0.019169360399246216, -8.122539598539293e-33, -0.06867992877960205, 0.07603830844163895, -0.027958255261182785, 0.080322265625, 0.014673542231321335, -0.0036628281231969595, -0.0869884267449379, 0.0014218678697943687, -0.02089771069586277, 0.08311575651168823, -0.004092272836714983, 0.004918842576444149, 0.0440966822206974, 0.01955871656537056, -0.03354215621948242, 0.06901950389146805, -0.061708010733127594, -0.029806094244122505, -0.036701008677482605, -0.008118091151118279, -0.08508369326591492, 0.08501553535461426, 0.0266585573554039, -0.04841674491763115, 0.027768529951572418, -0.05592324584722519, -0.058082062751054764, 0.036421556025743484, -0.009957833215594292, -0.023336609825491905, -0.028987465426325798, -0.02338465116918087, -0.04553057253360748, -0.011566719971597195, 0.027009721845388412, 0.077438585460186, 0.07499516755342484, -0.03454780951142311, -0.0019277927931398153, 0.012023056857287884, 0.04719410836696625, 0.018222397193312645, -0.03435293585062027, -0.08226563781499863, -0.027653221040964127, -0.04406668618321419, -0.04620344564318657, 0.008961725048720837, 0.07075361907482147, -0.0438409224152565, -0.0003689802251756191, -0.013638408854603767, -0.03387881815433502, -0.0059021092019975185, -0.03197656571865082, 0.03670727461576462, -0.015953045338392258, 0.02791169099509716, 0.05289716646075249, 0.122148796916008, -0.04775374010205269, -0.10075576603412628, 0.02220933511853218, 0.0521339550614357, -0.02610393986105919, 0.08320001512765884, -0.01743493229150772, 0.08770399540662766, -0.005589308682829142, 0.009967902675271034, 0.02914099581539631, 0.0386640727519989, 0.027676614001393318, 0.10028188675642014, -0.08445727080106735, -0.07112258672714233, 0.002270819153636694, -0.07740083336830139, 0.0026135332882404327, -0.026346521452069283, 0.054493166506290436, -0.16219523549079895, -0.020876601338386536, 0.04047313332557678, 0.06466466188430786, -0.022506466135382652, 0.03222142159938812, -0.003938885871320963, 0.03282297030091286, -0.07453659176826477, -0.012637111358344555, 0.06574314832687378, -0.07313305139541626, -0.012980032712221146, -0.0890326127409935, -5.009315984239038e-08, 0.005413024686276913, 0.009686033241450787, 0.10829917341470718, -0.040191903710365295, 0.1274871677160263, 0.0433003306388855, -0.040801506489515305, 0.0947040319442749, -0.0332765206694603, -0.05150836706161499, 0.10358398407697678, -0.04076647758483887, -0.05458211153745651, -0.0141975786536932, 0.014440767467021942, 0.0411037802696228, 0.050862960517406464, 0.018454236909747124, 0.043520618230104446, 0.009186787530779839, 0.0617661215364933, -0.029564272612333298, 0.06298404932022095, -0.019087646156549454, 0.035526029765605927, -0.11348295956850052, -0.04676984250545502, 0.03333038091659546, -0.039414528757333755, -0.00220276671461761, -0.06786425411701202, 0.04913175851106644, 0.05492216721177101, -0.012847509235143661, 0.06970871239900589, 0.14209029078483582, 0.0613304078578949, -0.07120515406131744, -0.04221164062619209, -0.046373941004276276, -0.04019384831190109, 0.08889081329107285, -0.06361556798219681, -0.009681900031864643, -0.006857948377728462, 0.019746989011764526, 0.09265056997537613, -0.06658393144607544, 0.001021713949739933, 0.0258614681661129, 0.05747881531715393, 0.002031083218753338, 0.10494091361761093, 0.0728672444820404, 0.06337276101112366, 0.05470630154013634, 0.03938430920243263, -0.033344727009534836, 0.008669435977935791, 0.056087084114551544, 0.038438860327005386, 0.03490142151713371, -0.022457966580986977, -0.05122220516204834]}\n",
            "\n",
            "all_embeddings defined: (18, 384)\n",
            "\n",
            "model defined: 384 dimensions\n",
            "\n",
            "processed_dir defined: /Users/scienceman/Desktop/LLM/data/processed\n",
            "\n",
            "=== Diagnostic Complete ===\n",
            "If you see any ERROR messages above, run the corresponding cells first.\n",
            "Make sure to run cells in order: 1 → 2 → 3 → 4 → 5 → 6\n"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
