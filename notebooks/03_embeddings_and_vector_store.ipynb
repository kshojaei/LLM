{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embeddings and Vector Store\n",
        "\n",
        "In this notebook, we'll learn how to convert our text chunks into embeddings (numerical vectors) and build a vector store for efficient similarity search. This is the core of how RAG systems find relevant information.\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this notebook, you will:\n",
        "1. Understand different embedding models and their trade-offs\n",
        "2. Generate embeddings for your text chunks\n",
        "3. Build and query a vector database\n",
        "4. Compare different embedding approaches\n",
        "5. Learn about vector store optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Let's import the libraries we need and load our processed data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Embedding and vector store imports\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import faiss\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Add project root to path\n",
        "import sys\n",
        "sys.path.append(str(Path.cwd().parent))\n",
        "\n",
        "# Import our configuration\n",
        "from src.config import DATA_CONFIG, DATA_DIR\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "\n",
        "# Check if we have processed data\n",
        "processed_dir = DATA_DIR / \"processed\"\n",
        "chunks_file = processed_dir / \"all_chunks.json\"\n",
        "\n",
        "if chunks_file.exists():\n",
        "    print(f\"Found processed chunks: {chunks_file}\")\n",
        "    with open(chunks_file, 'r', encoding='utf-8') as f:\n",
        "        all_chunks = json.load(f)\n",
        "    print(f\"Loaded {len(all_chunks)} chunks\")\n",
        "else:\n",
        "    print(\"No processed chunks found. Please run the data collection notebook first.\")\n",
        "    all_chunks = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Embedding Models\n",
        "\n",
        "Before we generate embeddings, let's understand the different models available and their trade-offs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's compare different embedding models\n",
        "embedding_models = {\n",
        "    \"all-MiniLM-L6-v2\": {\n",
        "        \"description\": \"Small, fast model (384 dimensions)\",\n",
        "        \"use_case\": \"Good for learning and experimentation\",\n",
        "        \"size\": \"Small\"\n",
        "    },\n",
        "    \"all-mpnet-base-v2\": {\n",
        "        \"description\": \"Medium model (768 dimensions)\",\n",
        "        \"use_case\": \"Good balance of speed and quality\",\n",
        "        \"size\": \"Medium\"\n",
        "    },\n",
        "    \"BAAI/bge-base-en-v1.5\": {\n",
        "        \"description\": \"High-quality model (768 dimensions)\",\n",
        "        \"use_case\": \"Production use, better quality\",\n",
        "        \"size\": \"Large\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Available Embedding Models:\")\n",
        "print(\"=\" * 50)\n",
        "for model_name, info in embedding_models.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"  Description: {info['description']}\")\n",
        "    print(f\"  Use Case: {info['use_case']}\")\n",
        "    print(f\"  Size: {info['size']}\")\n",
        "    print()\n",
        "\n",
        "# For learning purposes, let's start with the small, fast model\n",
        "print(\"Loading embedding model...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(f\"Model loaded: {model}\")\n",
        "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
        "\n",
        "# Test the model with a simple example\n",
        "test_texts = [\n",
        "    \"Cats are small, furry animals that make great pets.\",\n",
        "    \"Dogs are loyal companions that love to play.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting embedding generation...\")\n",
        "embeddings = model.encode(test_texts)\n",
        "print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
        "print(f\"Each text is now represented by {embeddings.shape[1]} numbers\")\n",
        "\n",
        "# Show similarity between the texts\n",
        "similarities = cosine_similarity(embeddings)\n",
        "print(f\"\\nSimilarity matrix:\")\n",
        "print(\"Texts:\")\n",
        "for i, text in enumerate(test_texts):\n",
        "    print(f\"  {i}: {text[:50]}...\")\n",
        "\n",
        "print(\"\\nSimilarity scores (higher = more similar):\")\n",
        "for i in range(len(test_texts)):\n",
        "    for j in range(i+1, len(test_texts)):\n",
        "        sim = similarities[i][j]\n",
        "        print(f\"  Text {i} vs Text {j}: {sim:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Embeddings for Our Data\n",
        "\n",
        "Now let's generate embeddings for our processed chunks. We'll do this in batches to handle large amounts of data efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate embeddings for all our chunks\n",
        "if all_chunks:\n",
        "    print(f\"Generating embeddings for {len(all_chunks)} chunks...\")\n",
        "    \n",
        "    # Extract text from chunks\n",
        "    chunk_texts = [chunk['text'] for chunk in all_chunks]\n",
        "    \n",
        "    # Generate embeddings in batches for efficiency\n",
        "    batch_size = 32\n",
        "    all_embeddings = []\n",
        "    \n",
        "    print(f\"Processing in batches of {batch_size}...\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    for i in range(0, len(chunk_texts), batch_size):\n",
        "        batch_texts = chunk_texts[i:i+batch_size]\n",
        "        batch_embeddings = model.encode(batch_texts, show_progress_bar=False)\n",
        "        all_embeddings.append(batch_embeddings)\n",
        "        \n",
        "        if (i // batch_size + 1) % 5 == 0:  # Print progress every 5 batches\n",
        "            print(f\"Processed {min(i + batch_size, len(chunk_texts))}/{len(chunk_texts)} chunks\")\n",
        "    \n",
        "    # Combine all embeddings\n",
        "    all_embeddings = np.vstack(all_embeddings)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    processing_time = end_time - start_time\n",
        "    \n",
        "    print(f\"\\nEmbedding generation completed!\")\n",
        "    print(f\"Total time: {processing_time:.2f} seconds\")\n",
        "    print(f\"Embeddings shape: {all_embeddings.shape}\")\n",
        "    print(f\"Average time per chunk: {processing_time/len(all_chunks)*1000:.2f} ms\")\n",
        "    \n",
        "    # Add embeddings to our chunks\n",
        "    for i, chunk in enumerate(all_chunks):\n",
        "        chunk['embedding'] = all_embeddings[i].tolist()\n",
        "    \n",
        "    print(f\"Embeddings added to {len(all_chunks)} chunks\")\n",
        "    \n",
        "    # Save the chunks with embeddings\n",
        "    embeddings_file = processed_dir / \"chunks_with_embeddings.json\"\n",
        "    with open(embeddings_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"Chunks with embeddings saved to: {embeddings_file}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No chunks available. Please run the data collection notebook first.\")\n",
        "    all_embeddings = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Vector Store with FAISS\n",
        "\n",
        "Now let's build a vector store using FAISS (Facebook AI Similarity Search) for efficient similarity search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build FAISS vector store\n",
        "if all_embeddings is not None:\n",
        "    print(\"Building FAISS vector store...\")\n",
        "    \n",
        "    # Get embedding dimension\n",
        "    embedding_dim = all_embeddings.shape[1]\n",
        "    print(f\"Embedding dimension: {embedding_dim}\")\n",
        "    \n",
        "    # Create FAISS index\n",
        "    # Using IndexFlatIP (Inner Product) which works well with normalized embeddings\n",
        "    # For cosine similarity, we'll normalize the embeddings\n",
        "    from sklearn.preprocessing import normalize\n",
        "    \n",
        "    # Normalize embeddings for cosine similarity\n",
        "    normalized_embeddings = normalize(all_embeddings, norm='l2')\n",
        "    \n",
        "    # Create FAISS index\n",
        "    index = faiss.IndexFlatIP(embedding_dim)  # Inner Product (cosine similarity for normalized vectors)\n",
        "    \n",
        "    # Add embeddings to index\n",
        "    index.add(normalized_embeddings.astype('float32'))\n",
        "    \n",
        "    print(f\"FAISS index built with {index.ntotal} vectors\")\n",
        "    \n",
        "    # Save the index\n",
        "    faiss_file = processed_dir / \"faiss_index.bin\"\n",
        "    faiss.write_index(index, str(faiss_file))\n",
        "    print(f\"FAISS index saved to: {faiss_file}\")\n",
        "    \n",
        "    # Test the vector store\n",
        "    def search_vector_store(query_text, top_k=5):\n",
        "        \"\"\"\n",
        "        Search the vector store for similar chunks.\n",
        "        \"\"\"\n",
        "        # Encode query\n",
        "        query_embedding = model.encode([query_text])\n",
        "        query_embedding = normalize(query_embedding, norm='l2').astype('float32')\n",
        "        \n",
        "        # Search\n",
        "        scores, indices = index.search(query_embedding, top_k)\n",
        "        \n",
        "        # Get results\n",
        "        results = []\n",
        "        for score, idx in zip(scores[0], indices[0]):\n",
        "            if idx < len(all_chunks):  # Valid index\n",
        "                chunk = all_chunks[idx]\n",
        "                results.append({\n",
        "                    'chunk': chunk,\n",
        "                    'score': float(score),\n",
        "                    'index': int(idx)\n",
        "                })\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    print(\"\\nVector store ready! Testing with sample queries...\")\n",
        "    \n",
        "    # Test queries\n",
        "    test_queries = [\n",
        "        \"What is machine learning?\",\n",
        "        \"Tell me about cats\",\n",
        "        \"How does artificial intelligence work?\",\n",
        "        \"What are the benefits of pets?\"\n",
        "    ]\n",
        "    \n",
        "    for query in test_queries:\n",
        "        print(f\"\\nQuery: '{query}'\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        results = search_vector_store(query, top_k=3)\n",
        "        \n",
        "        for i, result in enumerate(results):\n",
        "            chunk = result['chunk']\n",
        "            score = result['score']\n",
        "            print(f\"{i+1}. Score: {score:.3f}\")\n",
        "            print(f\"   Source: {chunk['source']}\")\n",
        "            print(f\"   Title: {chunk['source_title']}\")\n",
        "            print(f\"   Text: {chunk['text'][:100]}...\")\n",
        "            print()\n",
        "    \n",
        "else:\n",
        "    print(\"No embeddings available. Please run the embedding generation cell first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Vector Store with ChromaDB\n",
        "\n",
        "Let's also try ChromaDB, which is another popular vector database that's easier to use and has more features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build ChromaDB vector store\n",
        "if all_chunks:\n",
        "    print(\"Building ChromaDB vector store...\")\n",
        "    \n",
        "    # Initialize ChromaDB\n",
        "    chroma_dir = processed_dir / \"chroma_db\"\n",
        "    chroma_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    client = chromadb.PersistentClient(path=str(chroma_dir))\n",
        "    \n",
        "    # Create collection\n",
        "    collection_name = \"rag_chunks\"\n",
        "    collection = client.get_or_create_collection(\n",
        "        name=collection_name,\n",
        "        metadata={\"description\": \"RAG system chunks with embeddings\"}\n",
        "    )\n",
        "    \n",
        "    print(f\"Created ChromaDB collection: {collection_name}\")\n",
        "    \n",
        "    # Prepare data for ChromaDB\n",
        "    documents = [chunk['text'] for chunk in all_chunks]\n",
        "    metadatas = []\n",
        "    ids = []\n",
        "    \n",
        "    for i, chunk in enumerate(all_chunks):\n",
        "        metadata = {\n",
        "            'source': chunk['source'],\n",
        "            'title': chunk['source_title'],\n",
        "            'word_count': chunk['word_count'],\n",
        "            'chunk_type': chunk['type'],\n",
        "            'chunk_id': chunk['chunk_id']\n",
        "        }\n",
        "        metadatas.append(metadata)\n",
        "        ids.append(f\"chunk_{i}\")\n",
        "    \n",
        "    # Add documents to collection\n",
        "    print(\"Adding documents to ChromaDB...\")\n",
        "    \n",
        "    # ChromaDB can generate embeddings automatically, but we'll use our own\n",
        "    if all_embeddings is not None:\n",
        "        embeddings = all_embeddings.tolist()\n",
        "        collection.add(\n",
        "            documents=documents,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids,\n",
        "            embeddings=embeddings\n",
        "        )\n",
        "    else:\n",
        "        # Let ChromaDB generate embeddings\n",
        "        collection.add(\n",
        "            documents=documents,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "    \n",
        "    print(f\"Added {len(documents)} documents to ChromaDB\")\n",
        "    \n",
        "    # Test ChromaDB search\n",
        "    def search_chromadb(query_text, top_k=5):\n",
        "        \"\"\"\n",
        "        Search ChromaDB for similar chunks.\n",
        "        \"\"\"\n",
        "        results = collection.query(\n",
        "            query_texts=[query_text],\n",
        "            n_results=top_k,\n",
        "            include=['documents', 'metadatas', 'distances']\n",
        "        )\n",
        "        \n",
        "        formatted_results = []\n",
        "        if results['documents'] and results['documents'][0]:\n",
        "            for i, (doc, metadata, distance) in enumerate(zip(\n",
        "                results['documents'][0],\n",
        "                results['metadatas'][0],\n",
        "                results['distances'][0]\n",
        "            )):\n",
        "                # Convert distance to similarity score (ChromaDB uses distance, we want similarity)\n",
        "                similarity = 1 - distance\n",
        "                \n",
        "                formatted_results.append({\n",
        "                    'text': doc,\n",
        "                    'metadata': metadata,\n",
        "                    'similarity': similarity,\n",
        "                    'distance': distance\n",
        "                })\n",
        "        \n",
        "        return formatted_results\n",
        "    \n",
        "    print(\"\\nChromaDB vector store ready! Testing with sample queries...\")\n",
        "    \n",
        "    # Test queries\n",
        "    test_queries = [\n",
        "        \"What is machine learning?\",\n",
        "        \"Tell me about cats\",\n",
        "        \"How does artificial intelligence work?\",\n",
        "        \"What are the benefits of pets?\"\n",
        "    ]\n",
        "    \n",
        "    for query in test_queries:\n",
        "        print(f\"\\nQuery: '{query}'\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        results = search_chromadb(query, top_k=3)\n",
        "        \n",
        "        for i, result in enumerate(results):\n",
        "            print(f\"{i+1}. Similarity: {result['similarity']:.3f}\")\n",
        "            print(f\"   Source: {result['metadata']['source']}\")\n",
        "            print(f\"   Title: {result['metadata']['title']}\")\n",
        "            print(f\"   Text: {result['text'][:100]}...\")\n",
        "            print()\n",
        "    \n",
        "    print(f\"ChromaDB database saved to: {chroma_dir}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No chunks available. Please run the data collection notebook first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Query Testing\n",
        "\n",
        "Let's create an interactive tool to test our vector stores with your own queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive query testing function\n",
        "def interactive_search():\n",
        "    \"\"\"\n",
        "    Interactive function to test queries against our vector stores.\n",
        "    \"\"\"\n",
        "    if not all_chunks:\n",
        "        print(\"No data available. Please run the data collection notebook first.\")\n",
        "        return\n",
        "    \n",
        "    print(\"Interactive Vector Store Search\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Enter your questions to search through the knowledge base!\")\n",
        "    print(\"Type 'quit' to exit.\")\n",
        "    print()\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            query = input(\"Enter your question: \").strip()\n",
        "            \n",
        "            if query.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "            \n",
        "            if not query:\n",
        "                print(\"Please enter a question.\")\n",
        "                continue\n",
        "            \n",
        "            print(f\"\\nSearching for: '{query}'\")\n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "            # Search FAISS if available\n",
        "            if 'index' in locals():\n",
        "                print(\"FAISS Results:\")\n",
        "                print(\"-\" * 30)\n",
        "                faiss_results = search_vector_store(query, top_k=3)\n",
        "                \n",
        "                for i, result in enumerate(faiss_results):\n",
        "                    chunk = result['chunk']\n",
        "                    score = result['score']\n",
        "                    print(f\"{i+1}. Score: {score:.3f}\")\n",
        "                    print(f\"   Source: {chunk['source']}\")\n",
        "                    print(f\"   Title: {chunk['source_title']}\")\n",
        "                    print(f\"   Text: {chunk['text'][:150]}...\")\n",
        "                    print()\n",
        "            \n",
        "            # Search ChromaDB if available\n",
        "            if 'collection' in locals():\n",
        "                print(\"ChromaDB Results:\")\n",
        "                print(\"-\" * 30)\n",
        "                chroma_results = search_chromadb(query, top_k=3)\n",
        "                \n",
        "                for i, result in enumerate(chroma_results):\n",
        "                    print(f\"{i+1}. Similarity: {result['similarity']:.3f}\")\n",
        "                    print(f\"   Source: {result['metadata']['source']}\")\n",
        "                    print(f\"   Title: {result['metadata']['title']}\")\n",
        "                    print(f\"   Text: {result['text'][:150]}...\")\n",
        "                    print()\n",
        "            \n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nGoodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "# Let's also create a comparison function\n",
        "def compare_vector_stores(query_text, top_k=5):\n",
        "    \"\"\"\n",
        "    Compare results from both vector stores.\n",
        "    \"\"\"\n",
        "    if 'index' not in locals() or 'collection' not in locals():\n",
        "        print(\"Both vector stores not available for comparison.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Comparing vector stores for query: '{query_text}'\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # FAISS results\n",
        "    print(\"FAISS Results:\")\n",
        "    print(\"-\" * 35)\n",
        "    faiss_results = search_vector_store(query_text, top_k)\n",
        "    \n",
        "    for i, result in enumerate(faiss_results):\n",
        "        chunk = result['chunk']\n",
        "        score = result['score']\n",
        "        print(f\"{i+1}. Score: {score:.3f} | {chunk['source']} | {chunk['source_title'][:50]}...\")\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # ChromaDB results\n",
        "    print(\"ChromaDB Results:\")\n",
        "    print(\"-\" * 35)\n",
        "    chroma_results = search_chromadb(query_text, top_k)\n",
        "    \n",
        "    for i, result in enumerate(chroma_results):\n",
        "        print(f\"{i+1}. Similarity: {result['similarity']:.3f} | {result['metadata']['source']} | {result['metadata']['title'][:50]}...\")\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(\"Interactive search tools ready!\")\n",
        "print(\"Try these functions:\")\n",
        "print(\"1. interactive_search() - Interactive query testing\")\n",
        "print(\"2. compare_vector_stores('your query here') - Compare both vector stores\")\n",
        "print()\n",
        "print(\"Sample comparison:\")\n",
        "if 'index' in locals() and 'collection' in locals():\n",
        "    compare_vector_stores(\"What is machine learning?\")\n",
        "else:\n",
        "    print(\"Vector stores not ready yet. Run the previous cells first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "Excellent! You've successfully built a complete embedding and vector store system. Here's what we've accomplished:\n",
        "\n",
        "### What We've Built:\n",
        "1. **Embedding Generation** - Converted text chunks to numerical vectors\n",
        "2. **FAISS Vector Store** - Fast similarity search using Facebook's library\n",
        "3. **ChromaDB Vector Store** - Feature-rich vector database with metadata\n",
        "4. **Interactive Search** - Tools to query your knowledge base\n",
        "5. **Comparison Tools** - Compare different vector store approaches\n",
        "\n",
        "### Key Learnings:\n",
        "- **Embedding Models** - Different models have different trade-offs in speed vs quality\n",
        "- **Vector Stores** - FAISS is fast, ChromaDB is feature-rich\n",
        "- **Similarity Search** - Cosine similarity works well for semantic search\n",
        "- **Batch Processing** - Efficient embedding generation for large datasets\n",
        "- **Metadata** - Important for filtering and understanding search results\n",
        "\n",
        "### Files Created:\n",
        "- `chunks_with_embeddings.json` - Chunks with embedding vectors\n",
        "- `faiss_index.bin` - FAISS vector store index\n",
        "- `chroma_db/` - ChromaDB database directory\n",
        "\n",
        "### Next Steps:\n",
        "Now that we have our vector stores, the next steps are:\n",
        "1. **LLM Integration** - Connect language models to generate answers\n",
        "2. **Prompt Engineering** - Design effective prompts for the LLM\n",
        "3. **Complete RAG Pipeline** - Combine retrieval + generation\n",
        "4. **Evaluation** - Measure how well the system works\n",
        "5. **Optimization** - Improve performance and accuracy\n",
        "\n",
        "The foundation is now complete - you have a working retrieval system that can find relevant information from your knowledge base!\n",
        "\n",
        "**Ready for the next notebook?** The next step is LLM integration, where we'll learn how to generate answers based on retrieved context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
