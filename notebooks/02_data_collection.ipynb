{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Collection and Exploration\n",
        "\n",
        "In this notebook, we will collect real data from Wikipedia and ArXiv to build our RAG system. This is where we start working with actual documents that our system will need to understand and retrieve from.\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this notebook, you will:\n",
        "1. Understand how to collect data using our custom DataCollector\n",
        "2. Explore the structure and characteristics of different data sources\n",
        "3. Learn about data quality and filtering strategies\n",
        "4. Get hands-on experience with real text data\n",
        "5. Understand different chunking strategies for text preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "First, let us import the libraries we will need and set up our environment.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current directory: /Users/scienceman/Desktop/LLM/notebooks\n",
            "Project root: /Users/scienceman/Desktop/LLM\n",
            "Python path: ['.', '/Users/scienceman/Desktop/LLM/notebooks', '/Users/scienceman/Desktop/LLM']\n",
            "Successfully imported from src module\n",
            "Libraries imported successfully!\n",
            "Data directory: /Users/scienceman/Desktop/LLM/data\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Add project root to path - multiple approaches for reliability\n",
        "current_dir = os.getcwd()\n",
        "project_root = os.path.dirname(current_dir) if current_dir.endswith('notebooks') else current_dir\n",
        "\n",
        "# Add both current directory and project root to path\n",
        "sys.path.insert(0, project_root)\n",
        "sys.path.insert(0, current_dir)\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "print(f\"Current directory: {current_dir}\")\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Python path: {sys.path[:3]}\")\n",
        "\n",
        "# Force reload modules to get latest changes\n",
        "import importlib\n",
        "\n",
        "# Import our custom modules with error handling\n",
        "try:\n",
        "    from src.config import DATA_CONFIG, DATA_DIR\n",
        "    from src.data.collect_data import DataCollector\n",
        "    from src.data.preprocess_data import TextPreprocessor\n",
        "    print(\"Successfully imported from src module\")\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")\n",
        "    print(\"Trying alternative import methods...\")\n",
        "    \n",
        "    # Try importing directly from the file\n",
        "    try:\n",
        "        import importlib.util\n",
        "        \n",
        "        # Import config\n",
        "        config_path = os.path.join(project_root, 'src', 'config.py')\n",
        "        spec = importlib.util.spec_from_file_location(\"config\", config_path)\n",
        "        config_module = importlib.util.module_from_spec(spec)\n",
        "        spec.loader.exec_module(config_module)\n",
        "        DATA_CONFIG = config_module.DATA_CONFIG\n",
        "        DATA_DIR = config_module.DATA_DIR\n",
        "        \n",
        "        # Import collect_data\n",
        "        collect_path = os.path.join(project_root, 'src', 'data', 'collect_data.py')\n",
        "        spec = importlib.util.spec_from_file_location(\"collect_data\", collect_path)\n",
        "        collect_module = importlib.util.module_from_spec(spec)\n",
        "        spec.loader.exec_module(collect_module)\n",
        "        DataCollector = collect_module.DataCollector\n",
        "        \n",
        "        # Import preprocess_data\n",
        "        preprocess_path = os.path.join(project_root, 'src', 'data', 'preprocess_data.py')\n",
        "        spec = importlib.util.spec_from_file_location(\"preprocess_data\", preprocess_path)\n",
        "        preprocess_module = importlib.util.module_from_spec(spec)\n",
        "        spec.loader.exec_module(preprocess_module)\n",
        "        TextPreprocessor = preprocess_module.TextPreprocessor\n",
        "        \n",
        "        print(\"Successfully imported using direct file imports\")\n",
        "        \n",
        "    except Exception as e2:\n",
        "        print(f\"Direct import also failed: {e2}\")\n",
        "        print(\"Please check that you're running this from the correct directory\")\n",
        "        raise e2\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data.collect_data:Data collector initialized. Output directory: /Users/scienceman/Desktop/LLM/data/raw\n",
            "INFO:src.data.collect_data:Collecting Wikipedia data...\n",
            "INFO:src.data.collect_data:Using sample Wikipedia data...\n",
            "INFO:src.data.collect_data:Collecting ArXiv data...\n",
            "INFO:src.data.collect_data:Using sample ArXiv data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running diagnostics...\n",
            "Current working directory: /Users/scienceman/Desktop/LLM/notebooks\n",
            "Contents of current directory: ['03_embeddings_and_vector_store.ipynb', '01_understanding_rag.ipynb', '02_data_collection.ipynb', 'test.ipynb']\n",
            "Found src directory in parent folder\n",
            "Contents of src/: ['config.py', '__init__.py', 'models', '__pycache__', 'retrieval', 'evaluation', 'data']\n",
            "Contents of src/data/: ['preprocess_data.py', '__init__.py', '__pycache__', 'collect_data.py']\n",
            "\n",
            "Testing DataCollector method signatures...\n",
            "Wikipedia method parameters: ['self', 'max_documents', 'use_real_data']\n",
            "ArXiv method parameters: ['self', 'max_documents', 'use_real_data']\n",
            "\n",
            "Testing with sample data...\n",
            "Sample Wikipedia test: 1 articles\n",
            "Sample ArXiv test: 1 papers\n",
            "DataCollector methods are working correctly!\n"
          ]
        }
      ],
      "source": [
        "# Diagnostic and Testing Cell\n",
        "print(\"Running diagnostics...\")\n",
        "\n",
        "# Check current working directory and file structure\n",
        "import os\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"Contents of current directory: {os.listdir('.')}\")\n",
        "\n",
        "# Check if src directory exists\n",
        "if os.path.exists('../src'):\n",
        "    print(\"Found src directory in parent folder\")\n",
        "    print(f\"Contents of src/: {os.listdir('../src')}\")\n",
        "    if os.path.exists('../src/data'):\n",
        "        print(f\"Contents of src/data/: {os.listdir('../src/data')}\")\n",
        "else:\n",
        "    print(\"src directory not found in parent folder\")\n",
        "\n",
        "# Test the DataCollector method signatures\n",
        "print(\"\\nTesting DataCollector method signatures...\")\n",
        "\n",
        "try:\n",
        "    import inspect\n",
        "    \n",
        "    # Check Wikipedia method\n",
        "    wiki_sig = inspect.signature(DataCollector.collect_wikipedia_data)\n",
        "    print(f\"Wikipedia method parameters: {list(wiki_sig.parameters.keys())}\")\n",
        "    \n",
        "    # Check ArXiv method  \n",
        "    arxiv_sig = inspect.signature(DataCollector.collect_arxiv_data)\n",
        "    print(f\"ArXiv method parameters: {list(arxiv_sig.parameters.keys())}\")\n",
        "    \n",
        "    # Test with sample data (should work)\n",
        "    print(\"\\nTesting with sample data...\")\n",
        "    test_collector = DataCollector()\n",
        "    test_wiki = test_collector.collect_wikipedia_data(max_documents=1, use_real_data=False)\n",
        "    print(f\"Sample Wikipedia test: {len(test_wiki)} articles\")\n",
        "    \n",
        "    test_arxiv = test_collector.collect_arxiv_data(max_documents=1, use_real_data=False)\n",
        "    print(f\"Sample ArXiv test: {len(test_arxiv)} papers\")\n",
        "    \n",
        "    print(\"DataCollector methods are working correctly!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error during testing: {e}\")\n",
        "    print(\"This might indicate an import or configuration issue.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Our Data Sources\n",
        "\n",
        "Before we start collecting data, let us understand what we are working with:\n",
        "\n",
        "- **Wikipedia**: Encyclopedia articles with structured content\n",
        "- **ArXiv**: Scientific paper abstracts with technical content\n",
        "\n",
        "Let us start by collecting a small sample of each to explore their characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data.collect_data:Data collector initialized. Output directory: /Users/scienceman/Desktop/LLM/data/raw\n",
            "INFO:src.data.collect_data:Collecting Wikipedia data...\n",
            "INFO:src.data.collect_data:Fetching real Wikipedia data using Wikipedia API...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataCollector initialized successfully!\n",
            "\n",
            "Data collection mode: Real data from APIs\n",
            "\n",
            "Collecting Wikipedia data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data.collect_data:Collected: Machine learning\n",
            "INFO:src.data.collect_data:Collected: Artificial intelligence\n",
            "INFO:src.data.collect_data:Collected: Deep learning\n",
            "INFO:src.data.collect_data:Collected: Natural language processing\n",
            "INFO:src.data.collect_data:Collected: Computer vision\n",
            "INFO:src.data.collect_data:Successfully collected 5 Wikipedia articles\n",
            "INFO:src.data.collect_data:Collecting ArXiv data...\n",
            "INFO:src.data.collect_data:Fetching real ArXiv data using ArXiv API...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collected 5 Wikipedia articles\n",
            "\n",
            "Collecting ArXiv data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data.collect_data:Collected: FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n",
            "  Dataset and Comprehensive Benchmark\n",
            "INFO:src.data.collect_data:Collected: ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n",
            "  Orthogonal Butterfly Transforms\n",
            "INFO:src.data.collect_data:Collected: ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n",
            "  Orthogonal Butterfly Transforms\n",
            "INFO:src.data.collect_data:Collected: SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning\n",
            "INFO:src.data.collect_data:Successfully collected 4 ArXiv papers\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collected 4 ArXiv papers\n",
            "\n",
            "Total documents collected: 9\n",
            "\n",
            "Sample Wikipedia article:\n",
            "  Title: Machine learning\n",
            "  Word count: 68\n",
            "  Preview: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "  URL: https://en.wikipedia.org/wiki/Machine_learning\n",
            "\n",
            "Sample ArXiv paper:\n",
            "  Title: FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n",
            "  Dataset and Comprehensive Benchmark\n",
            "  Word count: 203\n",
            "  Authors: ['Rongyao Fang', 'Aldrich Yu', 'Chengqi Duan', 'Linjiang Huang', 'Shuai Bai', 'Yuxuan Cai', 'Kun Wang', 'Si Liu', 'Xihui Liu', 'Hongsheng Li']\n",
            "  Categories: ['cs.CV', 'cs.CL']\n",
            "  Preview: The advancement of open-source text-to-image (T2I) models has been hindered\n",
            "by the absence of large-...\n",
            "  ArXiv ID: 2509.09680v1\n"
          ]
        }
      ],
      "source": [
        "# Initialize our data collector\n",
        "collector = DataCollector()\n",
        "print(\"DataCollector initialized successfully!\")\n",
        "\n",
        "# Choose data collection mode\n",
        "USE_REAL_DATA = True  # Set to False to use sample data, True to fetch real data from APIs\n",
        "\n",
        "print(f\"\\nData collection mode: {'Real data from APIs' if USE_REAL_DATA else 'Sample data'}\")\n",
        "\n",
        "# Collect Wikipedia data with error handling\n",
        "print(\"\\nCollecting Wikipedia data...\")\n",
        "try:\n",
        "    wiki_data = collector.collect_wikipedia_data(max_documents=5, use_real_data=USE_REAL_DATA)\n",
        "    print(f\"Collected {len(wiki_data)} Wikipedia articles\")\n",
        "except TypeError as e:\n",
        "    if \"unexpected keyword argument\" in str(e):\n",
        "        print(\"Using fallback method (old API signature)\")\n",
        "        wiki_data = collector.collect_wikipedia_data(max_documents=5)\n",
        "        print(f\"Collected {len(wiki_data)} Wikipedia articles (sample data)\")\n",
        "    else:\n",
        "        raise e\n",
        "except Exception as e:\n",
        "    print(f\"Error collecting Wikipedia data: {e}\")\n",
        "    print(\"Falling back to sample data...\")\n",
        "    wiki_data = collector.collect_wikipedia_data(max_documents=5, use_real_data=False)\n",
        "\n",
        "# Collect ArXiv data with error handling\n",
        "print(\"\\nCollecting ArXiv data...\")\n",
        "try:\n",
        "    arxiv_data = collector.collect_arxiv_data(max_documents=5, use_real_data=USE_REAL_DATA)\n",
        "    print(f\"Collected {len(arxiv_data)} ArXiv papers\")\n",
        "except TypeError as e:\n",
        "    if \"unexpected keyword argument\" in str(e):\n",
        "        print(\"Using fallback method (old API signature)\")\n",
        "        arxiv_data = collector.collect_arxiv_data(max_documents=5)\n",
        "        print(f\"Collected {len(arxiv_data)} ArXiv papers (sample data)\")\n",
        "    else:\n",
        "        raise e\n",
        "except Exception as e:\n",
        "    print(f\"Error collecting ArXiv data: {e}\")\n",
        "    print(\"Falling back to sample data...\")\n",
        "    arxiv_data = collector.collect_arxiv_data(max_documents=5, use_real_data=False)\n",
        "\n",
        "print(f\"\\nTotal documents collected: {len(wiki_data) + len(arxiv_data)}\")\n",
        "\n",
        "# Display sample data to verify collection\n",
        "if wiki_data:\n",
        "    print(f\"\\nSample Wikipedia article:\")\n",
        "    print(f\"  Title: {wiki_data[0]['title']}\")\n",
        "    print(f\"  Word count: {wiki_data[0]['word_count']}\")\n",
        "    print(f\"  Preview: {wiki_data[0]['text'][:100]}...\")\n",
        "    if 'url' in wiki_data[0]:\n",
        "        print(f\"  URL: {wiki_data[0]['url']}\")\n",
        "\n",
        "if arxiv_data:\n",
        "    print(f\"\\nSample ArXiv paper:\")\n",
        "    print(f\"  Title: {arxiv_data[0]['title']}\")\n",
        "    print(f\"  Word count: {arxiv_data[0]['word_count']}\")\n",
        "    print(f\"  Authors: {arxiv_data[0]['authors']}\")\n",
        "    print(f\"  Categories: {arxiv_data[0]['categories']}\")\n",
        "    print(f\"  Preview: {arxiv_data[0]['abstract'][:100]}...\")\n",
        "    if 'arxiv_id' in arxiv_data[0]:\n",
        "        print(f\"  ArXiv ID: {arxiv_data[0]['arxiv_id']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative Data Collection Methods\n",
        "\n",
        "Since HuggingFace datasets no longer support `trust_remote_code`, we've implemented several alternative methods:\n",
        "\n",
        "### 1. **Wikipedia API** (Recommended)\n",
        "- **Free and reliable**: No authentication required\n",
        "- **Real-time data**: Always up-to-date articles\n",
        "- **Rich metadata**: Includes URLs, descriptions, and full content\n",
        "- **Rate limiting**: Built-in delays to be respectful to Wikipedia\n",
        "\n",
        "### 2. **ArXiv API**\n",
        "- **Scientific papers**: Real research papers from ArXiv\n",
        "- **Recent publications**: Can fetch latest papers\n",
        "- **Rich metadata**: Authors, categories, publication dates\n",
        "- **XML format**: Parses ArXiv's Atom feed\n",
        "\n",
        "### 3. **Sample Data Fallback**\n",
        "- **Offline mode**: Works without internet connection\n",
        "- **Consistent data**: Same data for reproducible results\n",
        "- **Fast execution**: No API delays\n",
        "\n",
        "### Usage Options:\n",
        "- Set `USE_REAL_DATA = True` to fetch real data from APIs\n",
        "- Set `USE_REAL_DATA = False` to use sample data\n",
        "- The system automatically falls back to sample data if API calls fail\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring Wikipedia Data Structure\n",
        "\n",
        "Let us examine the structure and characteristics of our Wikipedia articles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wikipedia Data Structure:\n",
            "==================================================\n",
            "Article keys: ['id', 'title', 'text', 'source', 'length', 'word_count', 'url', 'description']\n",
            "\n",
            "First article:\n",
            "  Title: Machine learning\n",
            "  Source: wikipedia\n",
            "  Length: 462 characters\n",
            "  Word count: 68 words\n",
            "  Text preview: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus...\n",
            "\n",
            "All Wikipedia articles:\n",
            "  1. Machine learning (68 words)\n",
            "  2. Artificial intelligence (64 words)\n",
            "  3. Deep learning (64 words)\n",
            "  4. Natural language processing (44 words)\n",
            "  5. Computer vision (90 words)\n",
            "\n",
            "Wikipedia Statistics:\n",
            "  Total articles: 5\n",
            "  Average words per article: 66.0\n",
            "  Min words: 44\n",
            "  Max words: 90\n"
          ]
        }
      ],
      "source": [
        "print(\"Wikipedia Data Structure:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if wiki_data:\n",
        "    # Show structure of first article\n",
        "    first_article = wiki_data[0]\n",
        "    print(f\"Article keys: {list(first_article.keys())}\")\n",
        "    print(f\"\\nFirst article:\")\n",
        "    print(f\"  Title: {first_article['title']}\")\n",
        "    print(f\"  Source: {first_article['source']}\")\n",
        "    print(f\"  Length: {first_article['length']} characters\")\n",
        "    print(f\"  Word count: {first_article['word_count']} words\")\n",
        "    print(f\"  Text preview: {first_article['text'][:200]}...\")\n",
        "    \n",
        "    # Show all articles\n",
        "    print(f\"\\nAll Wikipedia articles:\")\n",
        "    for i, article in enumerate(wiki_data):\n",
        "        print(f\"  {i+1}. {article['title']} ({article['word_count']} words)\")\n",
        "    \n",
        "    # Show statistics\n",
        "    word_counts = [article['word_count'] for article in wiki_data]\n",
        "    print(f\"\\nWikipedia Statistics:\")\n",
        "    print(f\"  Total articles: {len(wiki_data)}\")\n",
        "    print(f\"  Average words per article: {sum(word_counts) / len(word_counts):.1f}\")\n",
        "    print(f\"  Min words: {min(word_counts)}\")\n",
        "    print(f\"  Max words: {max(word_counts)}\")\n",
        "else:\n",
        "    print(\"No Wikipedia data collected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring ArXiv Data Structure\n",
        "\n",
        "Now let us examine the structure and characteristics of our ArXiv papers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ArXiv Data Structure:\n",
            "==================================================\n",
            "Paper keys: ['id', 'title', 'abstract', 'source', 'length', 'word_count', 'authors', 'categories', 'published', 'arxiv_id']\n",
            "\n",
            "First paper:\n",
            "  Title: FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n",
            "  Dataset and Comprehensive Benchmark\n",
            "  Source: arxiv\n",
            "  Length: 1595 characters\n",
            "  Word count: 203 words\n",
            "  Authors: ['Rongyao Fang', 'Aldrich Yu', 'Chengqi Duan', 'Linjiang Huang', 'Shuai Bai', 'Yuxuan Cai', 'Kun Wang', 'Si Liu', 'Xihui Liu', 'Hongsheng Li']\n",
            "  Categories: ['cs.CV', 'cs.CL']\n",
            "  Abstract preview: The advancement of open-source text-to-image (T2I) models has been hindered\n",
            "by the absence of large-scale, reasoning-focused datasets and comprehensive\n",
            "evaluation benchmarks, resulting in a performanc...\n",
            "\n",
            "All ArXiv papers:\n",
            "  1. FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n",
            "  Dataset and Comprehensive Benchmark (203 words)\n",
            "  2. ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n",
            "  Orthogonal Butterfly Transforms (208 words)\n",
            "  3. ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n",
            "  Orthogonal Butterfly Transforms (208 words)\n",
            "  4. SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning (187 words)\n",
            "\n",
            "ArXiv Statistics:\n",
            "  Total papers: 4\n",
            "  Average words per abstract: 201.5\n",
            "  Min words: 187\n",
            "  Max words: 208\n"
          ]
        }
      ],
      "source": [
        "print(\"ArXiv Data Structure:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if arxiv_data:\n",
        "    # Show structure of first paper\n",
        "    first_paper = arxiv_data[0]\n",
        "    print(f\"Paper keys: {list(first_paper.keys())}\")\n",
        "    print(f\"\\nFirst paper:\")\n",
        "    print(f\"  Title: {first_paper['title']}\")\n",
        "    print(f\"  Source: {first_paper['source']}\")\n",
        "    print(f\"  Length: {first_paper['length']} characters\")\n",
        "    print(f\"  Word count: {first_paper['word_count']} words\")\n",
        "    print(f\"  Authors: {first_paper['authors']}\")\n",
        "    print(f\"  Categories: {first_paper['categories']}\")\n",
        "    print(f\"  Abstract preview: {first_paper['abstract'][:200]}...\")\n",
        "    \n",
        "    # Show all papers\n",
        "    print(f\"\\nAll ArXiv papers:\")\n",
        "    for i, paper in enumerate(arxiv_data):\n",
        "        print(f\"  {i+1}. {paper['title']} ({paper['word_count']} words)\")\n",
        "    \n",
        "    # Show statistics\n",
        "    word_counts = [paper['word_count'] for paper in arxiv_data]\n",
        "    print(f\"\\nArXiv Statistics:\")\n",
        "    print(f\"  Total papers: {len(arxiv_data)}\")\n",
        "    print(f\"  Average words per abstract: {sum(word_counts) / len(word_counts):.1f}\")\n",
        "    print(f\"  Min words: {min(word_counts)}\")\n",
        "    print(f\"  Max words: {max(word_counts)}\")\n",
        "else:\n",
        "    print(\"No ArXiv data collected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Preprocessing and Chunking\n",
        "\n",
        "Now let us learn about different chunking strategies for preprocessing our text data. Chunking is crucial for RAG systems as it determines how we break down large documents into manageable pieces for retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data.preprocess_data:Text preprocessor initialized. Output directory: /Users/scienceman/Desktop/LLM/data/processed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TextPreprocessor initialized successfully!\n",
            "\n",
            "Demo document: Machine learning\n",
            "Original text length: 462 characters\n",
            "Original word count: 68 words\n",
            "\n",
            "Original text preview:\n",
            "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances i...\n"
          ]
        }
      ],
      "source": [
        "# Initialize our text preprocessor\n",
        "preprocessor = TextPreprocessor()\n",
        "print(\"TextPreprocessor initialized successfully!\")\n",
        "\n",
        "# Let us use one of our documents for demonstration\n",
        "if wiki_data:\n",
        "    demo_doc = wiki_data[0]\n",
        "    print(f\"\\nDemo document: {demo_doc['title']}\")\n",
        "    print(f\"Original text length: {len(demo_doc['text'])} characters\")\n",
        "    print(f\"Original word count: {demo_doc['word_count']} words\")\n",
        "    print(f\"\\nOriginal text preview:\")\n",
        "    print(demo_doc['text'][:300] + \"...\")\n",
        "elif arxiv_data:\n",
        "    demo_doc = arxiv_data[0]\n",
        "    print(f\"\\nDemo document: {demo_doc['title']}\")\n",
        "    print(f\"Original text length: {len(demo_doc['abstract'])} characters\")\n",
        "    print(f\"Original word count: {demo_doc['word_count']} words\")\n",
        "    print(f\"\\nOriginal text preview:\")\n",
        "    print(demo_doc['abstract'][:300] + \"...\")\n",
        "else:\n",
        "    print(\"No data available for demonstration\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Wikipedia article: Machine learning\n",
            "Comparing Chunking Strategies:\n",
            "==================================================\n",
            "\n",
            "FIXED Chunking:\n",
            "  Number of chunks: 1\n",
            "  First chunk length: 462 characters\n",
            "  First chunk preview: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn...\n",
            "  Chunk length stats: min=462, max=462, avg=462.0\n",
            "\n",
            "SEMANTIC Chunking:\n",
            "  Number of chunks: 1\n",
            "  First chunk length: 460 characters\n",
            "  First chunk preview: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn...\n",
            "  Chunk length stats: min=460, max=460, avg=460.0\n",
            "\n",
            "HIERARCHICAL Chunking:\n",
            "  Number of chunks: 1\n",
            "  First chunk length: 462 characters\n",
            "  First chunk preview: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn...\n",
            "  Chunk length stats: min=462, max=462, avg=462.0\n"
          ]
        }
      ],
      "source": [
        "# Compare different chunking strategies\n",
        "if wiki_data or arxiv_data:\n",
        "    # Use Wikipedia data if available, otherwise ArXiv\n",
        "    if wiki_data:\n",
        "        demo_doc = wiki_data[0]\n",
        "        print(f\"Using Wikipedia article: {demo_doc['title']}\")\n",
        "    else:\n",
        "        demo_doc = arxiv_data[0]\n",
        "        print(f\"Using ArXiv paper: {demo_doc['title']}\")\n",
        "    \n",
        "    print(\"Comparing Chunking Strategies:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    strategies = ['fixed', 'semantic', 'hierarchical']\n",
        "    \n",
        "    for strategy in strategies:\n",
        "        print(f\"\\n{strategy.upper()} Chunking:\")\n",
        "        chunks = preprocessor.chunk_document(demo_doc, strategy=strategy)\n",
        "        print(f\"  Number of chunks: {len(chunks)}\")\n",
        "        \n",
        "        if chunks:\n",
        "            # Show first chunk\n",
        "            first_chunk = chunks[0]\n",
        "            print(f\"  First chunk length: {len(first_chunk['text'])} characters\")\n",
        "            print(f\"  First chunk preview: {first_chunk['text'][:150]}...\")\n",
        "            \n",
        "            # Show chunk statistics\n",
        "            chunk_lengths = [len(chunk['text']) for chunk in chunks]\n",
        "            print(f\"  Chunk length stats: min={min(chunk_lengths)}, max={max(chunk_lengths)}, avg={np.mean(chunk_lengths):.1f}\")\n",
        "        else:\n",
        "            print(\"  No chunks created (document too short)\")\n",
        "else:\n",
        "    print(\"No data available for chunking demonstration\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing All Collected Data\n",
        "\n",
        "Now let us process all our collected data using our preferred chunking strategy and save it for use in the next notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing all collected documents...\n",
            "\n",
            "Processing 5 Wikipedia articles...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Wikipedia: 100%|██████████| 5/5 [00:00<00:00, 5814.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing 4 ArXiv papers...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ArXiv: 100%|██████████| 4/4 [00:00<00:00, 336.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total chunks created: 22\n",
            "Chunk length statistics:\n",
            "  Min: 44 characters\n",
            "  Max: 510 characters\n",
            "  Average: 400.7 characters\n",
            "  Median: 435.0 characters\n",
            "\n",
            "Chunks by source:\n",
            "  wikipedia: 6 chunks\n",
            "  arxiv: 16 chunks\n",
            "\n",
            "Sample chunks:\n",
            "  Chunk 1 (wikipedia): Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "  Chunk 2 (wikipedia): Artificial intelligence (AI) is the capability of computational systems to perform tasks typically a...\n",
            "  Chunk 3 (wikipedia): In machine learning, deep learning focuses on utilizing multilayered neural networks to perform task...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Process all documents\n",
        "print(\"Processing all collected documents...\")\n",
        "\n",
        "all_chunks = []\n",
        "\n",
        "# Process Wikipedia articles\n",
        "print(f\"\\nProcessing {len(wiki_data)} Wikipedia articles...\")\n",
        "for i, article in enumerate(tqdm(wiki_data, desc=\"Wikipedia\")):\n",
        "    chunks = preprocessor.chunk_document(article, strategy='semantic')\n",
        "    all_chunks.extend(chunks)\n",
        "\n",
        "# Process ArXiv papers\n",
        "print(f\"\\nProcessing {len(arxiv_data)} ArXiv papers...\")\n",
        "for i, paper in enumerate(tqdm(arxiv_data, desc=\"ArXiv\")):\n",
        "    # Convert ArXiv format to expected format\n",
        "    doc = {\n",
        "        'id': paper['id'],\n",
        "        'title': paper['title'],\n",
        "        'text': paper['abstract'],\n",
        "        'source': paper['source']\n",
        "    }\n",
        "    chunks = preprocessor.chunk_document(doc, strategy='semantic')\n",
        "    all_chunks.extend(chunks)\n",
        "\n",
        "print(f\"\\nTotal chunks created: {len(all_chunks)}\")\n",
        "\n",
        "# Show chunk statistics\n",
        "if all_chunks:\n",
        "    chunk_lengths = [len(chunk['text']) for chunk in all_chunks]\n",
        "    print(f\"Chunk length statistics:\")\n",
        "    print(f\"  Min: {min(chunk_lengths)} characters\")\n",
        "    print(f\"  Max: {max(chunk_lengths)} characters\")\n",
        "    print(f\"  Average: {np.mean(chunk_lengths):.1f} characters\")\n",
        "    print(f\"  Median: {np.median(chunk_lengths):.1f} characters\")\n",
        "    \n",
        "    # Show sources\n",
        "    sources = [chunk['source'] for chunk in all_chunks]\n",
        "    source_counts = Counter(sources)\n",
        "    print(f\"\\nChunks by source:\")\n",
        "    for source, count in source_counts.items():\n",
        "        print(f\"  {source}: {count} chunks\")\n",
        "    \n",
        "    # Show sample chunks\n",
        "    print(f\"\\nSample chunks:\")\n",
        "    for i, chunk in enumerate(all_chunks[:3]):  # Show first 3 chunks\n",
        "        print(f\"  Chunk {i+1} ({chunk['source']}): {chunk['text'][:100]}...\")\n",
        "else:\n",
        "    print(\"No chunks were created. This might indicate an issue with the chunking process.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving processed data...\n",
            "Saved 22 chunks to: /Users/scienceman/Desktop/LLM/data/processed/all_chunks.json\n",
            "\n",
            "Data processing complete! Ready for the next notebook.\n"
          ]
        }
      ],
      "source": [
        "# Save processed data\n",
        "print(\"Saving processed data...\")\n",
        "\n",
        "# Save chunks\n",
        "chunks_file = DATA_DIR / \"processed\" / \"all_chunks.json\"\n",
        "chunks_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(chunks_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(all_chunks, f, indent=2)\n",
        "\n",
        "print(f\"Saved {len(all_chunks)} chunks to: {chunks_file}\")\n",
        "\n",
        "print(\"\\nData processing complete! Ready for the next notebook.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we have learned:\n",
        "\n",
        "1. **Data Collection**: How to collect data from different sources using our custom DataCollector\n",
        "2. **Data Exploration**: How to analyze the structure and characteristics of our collected data\n",
        "3. **Text Preprocessing**: How to chunk documents using different strategies (fixed, semantic, hierarchical)\n",
        "4. **Data Processing**: How to process and save data for use in subsequent notebooks\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "- **Wikipedia articles** tend to be longer and more comprehensive\n",
        "- **ArXiv abstracts** are more concise and technical\n",
        "- **Semantic chunking** works well for preserving meaning while creating manageable pieces\n",
        "- **Data preprocessing** is crucial for effective RAG systems\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "In the next notebook, we will learn how to convert these text chunks into embeddings and build vector stores for efficient similarity search.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
