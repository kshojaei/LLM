{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Collection and Exploration\n",
        "\n",
        "In this notebook, we'll collect real data from Wikipedia and ArXiv to build our RAG system. This is where we start working with actual documents that our system will need to understand and retrieve from.\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this notebook, you will:\n",
        "1. Understand how to collect data from HuggingFace datasets\n",
        "2. Explore the structure and characteristics of different data sources\n",
        "3. Learn about data quality and filtering strategies\n",
        "4. Get hands-on experience with real text data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "First, let's import the libraries we'll need and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Add project root to path\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "# Import our custom modules\n",
        "from src.config import DATA_CONFIG, DATA_DIR\n",
        "from src.data.collect_data import DataCollector\n",
        "from src.data.preprocess_data import TextPreprocessor\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "\n",
        "# Use our DataCollector instead of direct load_dataset\n",
        "collector = DataCollector()\n",
        "wiki_sample_data = collector.collect_wikipedia_data(max_documents=5)\n",
        "print(f'Collected {len(wiki_sample_data)} Wikipedia articles')\n",
        "\n",
        "# Convert to the format expected by the rest of the notebook\n",
        "wiki_sample = []\n",
        "for article in wiki_sample_data:\n",
        "    wiki_sample.append({\n",
        "        'title': article['title'],\n",
        "        'text': article['text']\n",
        "    })\n",
        "\n",
        "print(f'Wikipedia sample structure: {len(wiki_sample)} articles')\n",
        "if wiki_sample:\n",
        "    print(f'First article title: {wiki_sample[0][\"title\"]}')\n",
        "    print(f'First article length: {len(wiki_sample[0][\"text\"])} characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Our Data Sources\n",
        "\n",
        "Before we start collecting data, let's understand what we're working with:\n",
        "\n",
        "### Wikipedia\n",
        "- **What it is**: General knowledge articles on a wide range of topics\n",
        "- **Why it's good for RAG**: Diverse topics, well-structured text, factual content\n",
        "- **Challenges**: Variable length, some articles are very long or very short\n",
        "\n",
        "### ArXiv\n",
        "- **What it is**: Academic paper abstracts from various scientific fields\n",
        "- **Why it's good for RAG**: Technical content, structured abstracts, domain-specific knowledge\n",
        "- **Challenges**: Technical jargon, variable quality, specialized vocabulary\n",
        "\n",
        "Let's start by exploring these datasets without downloading the full thing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use our DataCollector instead of direct load_dataset\n",
        "collector = DataCollector()\n",
        "wiki_sample_data = collector.collect_wikipedia_data(max_documents=5)\n",
        "print(f'Collected {len(wiki_sample_data)} Wikipedia articles')\n",
        "\n",
        "# Convert to the format expected by the rest of the notebook\n",
        "wiki_sample = []\n",
        "for article in wiki_sample_data:\n",
        "    wiki_sample.append({\n",
        "        'title': article['title'],\n",
        "        'text': article['text']\n",
        "    })\n",
        "\n",
        "print(f'Wikipedia sample structure: {len(wiki_sample)} articles')\n",
        "if wiki_sample:\n",
        "    print(f'First article title: {wiki_sample[0][\"title\"]}')\n",
        "    print(f'First article length: {len(wiki_sample[0][\"text\"])} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exploring ArXiv dataset structure...\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Dataset scripts are no longer supported, but found scientific_papers.py",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[50], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now let's look at ArXiv data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExploring ArXiv dataset structure...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m arxiv_sample \u001b[38;5;241m=\u001b[39m load_dataset(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscientific_papers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marxiv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain[:5]\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Just 5 samples\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mArXiv sample contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(arxiv_sample)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m papers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mKeys in each paper: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marxiv_sample\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1392\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   1388\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1389\u001b[0m )\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1393\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   1394\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   1395\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1396\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1397\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1398\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m   1399\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1400\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1401\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1402\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1403\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[1;32m   1405\u001b[0m )\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1132\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     features \u001b[38;5;241m=\u001b[39m _fix_for_backward_compatible_features(features)\n\u001b[0;32m-> 1132\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1133\u001b[0m     path,\n\u001b[1;32m   1134\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1135\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1136\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1137\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1138\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1139\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1140\u001b[0m )\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1031\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1027\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1028\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1029\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1030\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1031\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:989\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     api\u001b[38;5;241m.\u001b[39mhf_hub_download(\n\u001b[1;32m    983\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m    984\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[1;32m    988\u001b[0m     )\n\u001b[0;32m--> 989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Dataset scripts are no longer supported, but found scientific_papers.py"
          ]
        }
      ],
      "source": [
        "# Use our DataCollector instead of direct load_dataset\n",
        "collector = DataCollector()\n",
        "arxiv_sample_data = collector.collect_arxiv_data(max_documents=5)\n",
        "print(f'Collected {len(arxiv_sample_data)} ArXiv papers')\n",
        "\n",
        "# Convert to the format expected by the rest of the notebook\n",
        "arxiv_sample = []\n",
        "for paper in arxiv_sample_data:\n",
        "    arxiv_sample.append({\n",
        "        'title': paper['title'],\n",
        "        'abstract': paper['abstract']\n",
        "    })\n",
        "\n",
        "print(f'ArXiv sample structure: {len(arxiv_sample)} papers')\n",
        "if arxiv_sample:\n",
        "    print(f'First paper title: {arxiv_sample[0][\"title\"]}')\n",
        "    print(f'First paper abstract length: {len(arxiv_sample[0][\"abstract\"])} characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collecting Real Data\n",
        "\n",
        "Now let's use our data collection module to get some real data. We'll start with a small sample to test our approach, then you can decide if you want to collect more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's start with a small collection to test our approach\n",
        "print(\"Collecting a small sample of Wikipedia data...\")\n",
        "wiki_data = collector.collect_wikipedia_data(max_documents=50)\n",
        "\n",
        "print(f\"\\nCollected {len(wiki_data)} Wikipedia articles\")\n",
        "if wiki_data:\n",
        "    print(f\"\\nSample article titles:\")\n",
        "    for i, article in enumerate(wiki_data[:5]):\n",
        "        print(f\"  {i+1}. {article['title']} ({article['word_count']} words)\")\n",
        "    \n",
        "    # Show some statistics\n",
        "    word_counts = [article['word_count'] for article in wiki_data]\n",
        "    print(f\"\\nWikipedia Statistics:\")\n",
        "    print(f\"  Average words: {np.mean(word_counts):.1f}\")\n",
        "    print(f\"  Word range: {min(word_counts)} - {max(word_counts)}\")\n",
        "    print(f\"  Total words: {sum(word_counts):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now let's collect some ArXiv data\n",
        "print(\"Collecting a small sample of ArXiv data...\")\n",
        "arxiv_data = collector.collect_arxiv_data(max_documents=25)\n",
        "\n",
        "print(f\"\\nCollected {len(arxiv_data)} ArXiv abstracts\")\n",
        "if arxiv_data:\n",
        "    print(f\"\\nSample paper titles:\")\n",
        "    for i, paper in enumerate(arxiv_data[:5]):\n",
        "        print(f\"  {i+1}. {paper['title']} ({paper['word_count']} words)\")\n",
        "        if 'categories' in paper and paper['categories']:\n",
        "            print(f\"     Categories: {paper['categories'][:3]}\")  # Show first 3 categories\n",
        "    \n",
        "    # Show some statistics\n",
        "    word_counts = [paper['word_count'] for paper in arxiv_data]\n",
        "    print(f\"\\nArXiv Statistics:\")\n",
        "    print(f\"  Average words: {np.mean(word_counts):.1f}\")\n",
        "    print(f\"  Word range: {min(word_counts)} - {max(word_counts)}\")\n",
        "    print(f\"  Total words: {sum(word_counts):,}\")\n",
        "    \n",
        "    # Analyze categories\n",
        "    all_categories = []\n",
        "    for paper in arxiv_data:\n",
        "        if 'categories' in paper and paper['categories']:\n",
        "            all_categories.extend(paper['categories'])\n",
        "    \n",
        "    if all_categories:\n",
        "        category_counts = Counter(all_categories)\n",
        "        print(f\"\\nTop categories:\")\n",
        "        for cat, count in category_counts.most_common(5):\n",
        "            print(f\"  {cat}: {count} papers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Visualization and Analysis\n",
        "\n",
        "Let's create some visualizations to understand our data better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data collected yet. Run the previous cells to collect data first.\n"
          ]
        }
      ],
      "source": [
        "# Create visualizations of our collected data\n",
        "if wiki_data and arxiv_data:\n",
        "    # Combine data for comparison\n",
        "    comparison_data = []\n",
        "    \n",
        "    for article in wiki_data:\n",
        "        comparison_data.append({\n",
        "            'source': 'Wikipedia',\n",
        "            'word_count': article['word_count'],\n",
        "            'title': article['title']\n",
        "        })\n",
        "    \n",
        "    for paper in arxiv_data:\n",
        "        comparison_data.append({\n",
        "            'source': 'ArXiv',\n",
        "            'word_count': paper['word_count'],\n",
        "            'title': paper['title']\n",
        "        })\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    # Create comparison plots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Word count distribution\n",
        "    axes[0, 0].hist(comparison_df[comparison_df['source'] == 'Wikipedia']['word_count'], \n",
        "                   bins=20, alpha=0.7, label='Wikipedia', color='blue')\n",
        "    axes[0, 0].hist(comparison_df[comparison_df['source'] == 'ArXiv']['word_count'], \n",
        "                   bins=20, alpha=0.7, label='ArXiv', color='orange')\n",
        "    axes[0, 0].set_title('Word Count Distribution')\n",
        "    axes[0, 0].set_xlabel('Word Count')\n",
        "    axes[0, 0].set_ylabel('Number of Documents')\n",
        "    axes[0, 0].legend()\n",
        "    \n",
        "    # Box plot comparison\n",
        "    comparison_df.boxplot(column='word_count', by='source', ax=axes[0, 1])\n",
        "    axes[0, 1].set_title('Word Count by Source')\n",
        "    axes[0, 1].set_xlabel('Source')\n",
        "    axes[0, 1].set_ylabel('Word Count')\n",
        "    \n",
        "    # Scatter plot\n",
        "    wiki_counts = comparison_df[comparison_df['source'] == 'Wikipedia']['word_count']\n",
        "    arxiv_counts = comparison_df[comparison_df['source'] == 'ArXiv']['word_count']\n",
        "    \n",
        "    axes[1, 0].scatter(range(len(wiki_counts)), wiki_counts, alpha=0.6, label='Wikipedia', color='blue')\n",
        "    axes[1, 0].scatter(range(len(arxiv_counts)), arxiv_counts, alpha=0.6, label='ArXiv', color='orange')\n",
        "    axes[1, 0].set_title('Individual Document Word Counts')\n",
        "    axes[1, 0].set_xlabel('Document Index')\n",
        "    axes[1, 0].set_ylabel('Word Count')\n",
        "    axes[1, 0].legend()\n",
        "    \n",
        "    # Summary statistics\n",
        "    summary_stats = comparison_df.groupby('source')['word_count'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
        "    axes[1, 1].axis('off')\n",
        "    \n",
        "    # Create a text table\n",
        "    table_text = \"Summary Statistics:\\n\\n\"\n",
        "    for source in summary_stats.index:\n",
        "        stats = summary_stats.loc[source]\n",
        "        table_text += f\"{source}:\\n\"\n",
        "        table_text += f\"  Count: {stats['count']}\\n\"\n",
        "        table_text += f\"  Mean: {stats['mean']:.1f}\\n\"\n",
        "        table_text += f\"  Std: {stats['std']:.1f}\\n\"\n",
        "        table_text += f\"  Range: {stats['min']}-{stats['max']}\\n\\n\"\n",
        "    \n",
        "    axes[1, 1].text(0.1, 0.9, table_text, transform=axes[1, 1].transAxes, \n",
        "                   fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Key Insights:\")\n",
        "    wiki_mean = comparison_df[comparison_df['source'] == 'Wikipedia']['word_count'].mean()\n",
        "    arxiv_mean = comparison_df[comparison_df['source'] == 'ArXiv']['word_count'].mean()\n",
        "    \n",
        "    print(f\"\u2022 Wikipedia articles are {wiki_mean/arxiv_mean:.1f}x longer on average\")\n",
        "    print(f\"\u2022 Wikipedia: {len(wiki_data)} articles, average {wiki_mean:.0f} words\")\n",
        "    print(f\"\u2022 ArXiv: {len(arxiv_data)} abstracts, average {arxiv_mean:.0f} words\")\n",
        "    print(f\"\u2022 Wikipedia provides broader, general knowledge\")\n",
        "    print(f\"\u2022 ArXiv provides focused, technical content\")\n",
        "    print(f\"\u2022 Together, they offer both breadth and depth for RAG systems\")\n",
        "else:\n",
        "    print(\"No data collected yet. Run the previous cells to collect data first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Preprocessing and Chunking\n",
        "\n",
        "Now let's use our preprocessing module to clean and chunk the data we collected. This is a crucial step because raw text needs to be prepared before we can create embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No Wikipedia data available. Run the data collection cells first.\n"
          ]
        }
      ],
      "source": [
        "# Let's test different chunking strategies on a sample document\n",
        "if wiki_data:\n",
        "    # Pick a longer article to demonstrate chunking\n",
        "    sample_article = max(wiki_data, key=lambda x: x['word_count'])\n",
        "    \n",
        "    print(f\"Testing chunking on: '{sample_article['title']}'\")\n",
        "    print(f\"Original length: {sample_article['word_count']} words\")\n",
        "    print(f\"Original text (first 300 chars): {sample_article['text'][:300]}...\")\n",
        "    \n",
        "    # Test different chunking strategies\n",
        "    strategies = ['fixed', 'semantic', 'hierarchical']\n",
        "    \n",
        "    for strategy in strategies:\n",
        "        print(f\"\\n--- {strategy.upper()} CHUNKING ---\")\n",
        "        chunks = preprocessor.chunk_document(sample_article, strategy)\n",
        "        \n",
        "        print(f\"Number of chunks: {len(chunks)}\")\n",
        "        if chunks:\n",
        "            print(f\"Average words per chunk: {np.mean([c['word_count'] for c in chunks]):.1f}\")\n",
        "            print(f\"Chunk sizes: {[c['word_count'] for c in chunks]}\")\n",
        "            \n",
        "            # Show first chunk\n",
        "            print(f\"First chunk ({chunks[0]['word_count']} words):\")\n",
        "            print(f\"  {chunks[0]['text'][:150]}...\")\n",
        "            \n",
        "            if len(chunks) > 1:\n",
        "                print(f\"Second chunk ({chunks[1]['word_count']} words):\")\n",
        "                print(f\"  {chunks[1]['text'][:150]}...\")\n",
        "        \n",
        "        print(\"-\" * 50)\n",
        "else:\n",
        "    print(\"No Wikipedia data available. Run the data collection cells first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing All Collected Data\n",
        "\n",
        "Now let's process all our collected data using the best chunking strategy. We'll use semantic chunking as it tends to work well for most use cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data collected yet. Run the data collection cells first.\n"
          ]
        }
      ],
      "source": [
        "# Process all our collected data\n",
        "if wiki_data and arxiv_data:\n",
        "    print(\"Processing all collected data...\")\n",
        "    \n",
        "    # Process Wikipedia data\n",
        "    print(\"\\nProcessing Wikipedia data with semantic chunking...\")\n",
        "    wiki_chunks = preprocessor.process_documents(wiki_data, strategy=\"semantic\")\n",
        "    \n",
        "    # Process ArXiv data\n",
        "    print(\"\\nProcessing ArXiv data with semantic chunking...\")\n",
        "    arxiv_chunks = preprocessor.process_documents(arxiv_data, strategy=\"semantic\")\n",
        "    \n",
        "    # Combine all chunks\n",
        "    all_chunks = wiki_chunks + arxiv_chunks\n",
        "    \n",
        "    print(f\"\\nProcessing Summary:\")\n",
        "    print(f\"  Wikipedia: {len(wiki_data)} documents -> {len(wiki_chunks)} chunks\")\n",
        "    print(f\"  ArXiv: {len(arxiv_data)} documents -> {len(arxiv_chunks)} chunks\")\n",
        "    print(f\"  Total: {len(all_chunks)} chunks\")\n",
        "    \n",
        "    if all_chunks:\n",
        "        # Analyze chunk characteristics\n",
        "        word_counts = [chunk['word_count'] for chunk in all_chunks]\n",
        "        char_counts = [chunk['char_count'] for chunk in all_chunks]\n",
        "        \n",
        "        print(f\"\\nChunk Statistics:\")\n",
        "        print(f\"  Average words per chunk: {np.mean(word_counts):.1f}\")\n",
        "        print(f\"  Average chars per chunk: {np.mean(char_counts):.1f}\")\n",
        "        print(f\"  Word count range: {min(word_counts)} - {max(word_counts)}\")\n",
        "        print(f\"  Total words in chunks: {sum(word_counts):,}\")\n",
        "        \n",
        "        # Show sample chunks\n",
        "        print(f\"\\nSample chunks:\")\n",
        "        for i, chunk in enumerate(all_chunks[:3]):\n",
        "            print(f\"\\nChunk {i+1} (Source: {chunk['source']}, {chunk['word_count']} words):\")\n",
        "            print(f\"  Title: {chunk['source_title']}\")\n",
        "            print(f\"  Text: {chunk['text'][:200]}...\")\n",
        "    \n",
        "    # Save the processed data\n",
        "    print(f\"\\nSaving processed data...\")\n",
        "    preprocessor.save_processed_data(wiki_chunks, \"wikipedia_chunks.json\")\n",
        "    preprocessor.save_processed_data(arxiv_chunks, \"arxiv_chunks.json\")\n",
        "    preprocessor.save_processed_data(all_chunks, \"all_chunks.json\")\n",
        "    \n",
        "    print(f\"Processed data saved to: {preprocessor.output_dir}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No data collected yet. Run the data collection cells first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Data Exploration\n",
        "\n",
        "Let's create some interactive tools to explore our data and understand what we've collected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Try these exploration functions:\n",
            "1. explore_documents(wiki_data, 'Wikipedia article')\n",
            "2. explore_documents(arxiv_data, 'ArXiv abstract')\n"
          ]
        }
      ],
      "source": [
        "# Interactive exploration functions\n",
        "def explore_documents(documents, doc_type=\"document\"):\n",
        "    \"\"\"\n",
        "    Interactive function to explore individual documents.\n",
        "    \"\"\"\n",
        "    if not documents:\n",
        "        print(\"No documents available\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\n=== Exploring {doc_type}s ===\")\n",
        "    print(f\"Total {doc_type}s: {len(documents)}\")\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            choice = input(f\"\\nEnter {doc_type} number (0-{len(documents)-1}) or 'q' to quit: \")\n",
        "            \n",
        "            if choice.lower() == 'q':\n",
        "                break\n",
        "            \n",
        "            idx = int(choice)\n",
        "            if 0 <= idx < len(documents):\n",
        "                doc = documents[idx]\n",
        "                print(f\"\\n--- {doc_type.upper()} {idx} ---\")\n",
        "                print(f\"Title: {doc['title']}\")\n",
        "                print(f\"Word count: {doc['word_count']}\")\n",
        "                print(f\"Character count: {doc['length']}\")\n",
        "                \n",
        "                # Show the text content\n",
        "                text_key = 'text' if 'text' in doc else 'abstract'\n",
        "                text = doc[text_key]\n",
        "                \n",
        "                print(f\"\\n{text_key.upper()} (first 500 characters):\")\n",
        "                print(\"-\" * 50)\n",
        "                print(text[:500])\n",
        "                if len(text) > 500:\n",
        "                    print(\"...\")\n",
        "                print(\"-\" * 50)\n",
        "                \n",
        "                # Show additional info if available\n",
        "                if 'categories' in doc and doc['categories']:\n",
        "                    print(f\"Categories: {doc['categories']}\")\n",
        "                if 'authors' in doc and doc['authors']:\n",
        "                    print(f\"Authors: {doc['authors'][:3]}\")\n",
        "            else:\n",
        "                print(f\"Please enter a number between 0 and {len(documents)-1}\")\n",
        "        except ValueError:\n",
        "            print(\"Please enter a valid number or 'q'\")\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nExiting exploration...\")\n",
        "            break\n",
        "\n",
        "def explore_chunks(chunks):\n",
        "    \"\"\"\n",
        "    Interactive function to explore individual chunks.\n",
        "    \"\"\"\n",
        "    if not chunks:\n",
        "        print(\"No chunks available\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\n=== Exploring Chunks ===\")\n",
        "    print(f\"Total chunks: {len(chunks)}\")\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            choice = input(f\"\\nEnter chunk number (0-{len(chunks)-1}) or 'q' to quit: \")\n",
        "            \n",
        "            if choice.lower() == 'q':\n",
        "                break\n",
        "            \n",
        "            idx = int(choice)\n",
        "            if 0 <= idx < len(chunks):\n",
        "                chunk = chunks[idx]\n",
        "                print(f\"\\n--- CHUNK {idx} ---\")\n",
        "                print(f\"Source: {chunk['source']}\")\n",
        "                print(f\"Title: {chunk['source_title']}\")\n",
        "                print(f\"Word count: {chunk['word_count']}\")\n",
        "                print(f\"Chunk type: {chunk['type']}\")\n",
        "                \n",
        "                print(f\"\\nCHUNK TEXT:\")\n",
        "                print(\"-\" * 50)\n",
        "                print(chunk['text'])\n",
        "                print(\"-\" * 50)\n",
        "                \n",
        "                # Show metadata\n",
        "                if 'metadata' in chunk:\n",
        "                    print(f\"Metadata: {chunk['metadata']}\")\n",
        "            else:\n",
        "                print(f\"Please enter a number between 0 and {len(chunks)-1}\")\n",
        "        except ValueError:\n",
        "            print(\"Please enter a valid number or 'q'\")\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nExiting exploration...\")\n",
        "            break\n",
        "\n",
        "# Let's explore Wikipedia articles\n",
        "if wiki_data:\n",
        "    print(\"Wikipedia data exploration available!\")\n",
        "    print(\"Run: explore_documents(wiki_data, 'Wikipedia article')\")\n",
        "    \n",
        "# Let's explore ArXiv abstracts\n",
        "if arxiv_data:\n",
        "    print(\"ArXiv data exploration available!\")\n",
        "    print(\"Run: explore_documents(arxiv_data, 'ArXiv abstract')\")\n",
        "    \n",
        "# Let's explore chunks if we have them\n",
        "if 'all_chunks' in locals():\n",
        "    print(\"Chunk data exploration available!\")\n",
        "    print(\"Run: explore_chunks(all_chunks)\")\n",
        "\n",
        "print(\"\\nTry these exploration functions:\")\n",
        "print(\"1. explore_documents(wiki_data, 'Wikipedia article')\")\n",
        "print(\"2. explore_documents(arxiv_data, 'ArXiv abstract')\")\n",
        "if 'all_chunks' in locals():\n",
        "    print(\"3. explore_chunks(all_chunks)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "Great! You've successfully collected and processed real data for your RAG system. Here's what we've accomplished:\n",
        "\n",
        "### What We've Done\n",
        "1. **Explored data sources** - Understood the structure of Wikipedia and ArXiv datasets\n",
        "2. **Collected sample data** - Downloaded and processed real documents\n",
        "3. **Analyzed data characteristics** - Understood word counts, distributions, and quality\n",
        "4. **Compared data sources** - Saw how Wikipedia and ArXiv complement each other\n",
        "5. **Processed and chunked data** - Prepared text for embedding generation\n",
        "6. **Created interactive tools** - Built functions to explore your data\n",
        "\n",
        "### Key Insights\n",
        "- **Wikipedia** provides broad, general knowledge with longer articles\n",
        "- **ArXiv** provides focused, technical content with shorter abstracts\n",
        "- Both sources have good coverage for building a comprehensive RAG system\n",
        "- **Semantic chunking** works well for preserving meaning while creating manageable pieces\n",
        "- Data quality is generally good, with some variability in length\n",
        "\n",
        "### Next Steps\n",
        "Now that we have our processed data, the next steps in building our RAG system are:\n",
        "1. **Generate embeddings** - Convert text chunks to vector representations\n",
        "2. **Build vector store** - Create a searchable database of embeddings\n",
        "3. **Implement retrieval** - Find relevant chunks for queries\n",
        "4. **Connect LLM** - Generate answers based on retrieved context\n",
        "\n",
        "### Files Created\n",
        "- Raw data: `data/raw/wikipedia_sample.json`, `data/raw/arxiv_sample.json`\n",
        "- Processed chunks: `data/processed/wikipedia_chunks.json`, `data/processed/arxiv_chunks.json`, `data/processed/all_chunks.json`\n",
        "\n",
        "The data you've collected and processed will be the foundation for all these next steps. Each chunk will be converted into embeddings that can be searched and retrieved.\n",
        "\n",
        "**Ready to move on to the next notebook?** The next step is embedding generation, where we'll learn how to convert our text chunks into mathematical representations for similarity search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Data Collection and Processing Summary ===\n",
            "Wikipedia articles collected: 0\n",
            "ArXiv abstracts collected: 0\n",
            "Total documents: 0\n",
            "\n",
            "Data saved to:\n",
            "  Raw data: /Users/scienceman/Desktop/LLM/data/raw\n",
            "  Processed data: /Users/scienceman/Desktop/LLM/data/processed\n",
            "\n",
            "Files created:\n",
            "\n",
            "Data collection and processing completed successfully!\n",
            "\n",
            "Next: Open '03_embeddings_and_vector_store.ipynb' to learn about converting text to embeddings.\n"
          ]
        }
      ],
      "source": [
        "# Final summary and verification\n",
        "print(\"=== Data Collection and Processing Summary ===\")\n",
        "print(f\"Wikipedia articles collected: {len(wiki_data) if 'wiki_data' in locals() and wiki_data else 0}\")\n",
        "print(f\"ArXiv abstracts collected: {len(arxiv_data) if 'arxiv_data' in locals() and arxiv_data else 0}\")\n",
        "print(f\"Total documents: {(len(wiki_data) if 'wiki_data' in locals() and wiki_data else 0) + (len(arxiv_data) if 'arxiv_data' in locals() and arxiv_data else 0)}\")\n",
        "\n",
        "if 'all_chunks' in locals() and all_chunks:\n",
        "    print(f\"Total chunks created: {len(all_chunks)}\")\n",
        "    word_counts = [chunk['word_count'] for chunk in all_chunks]\n",
        "    print(f\"Average words per chunk: {np.mean(word_counts):.1f}\")\n",
        "    print(f\"Total words in all chunks: {sum(word_counts):,}\")\n",
        "\n",
        "print(f\"\\nData saved to:\")\n",
        "print(f\"  Raw data: {collector.output_dir}\")\n",
        "print(f\"  Processed data: {preprocessor.output_dir}\")\n",
        "\n",
        "print(f\"\\nFiles created:\")\n",
        "for file in collector.output_dir.glob(\"*.json\"):\n",
        "    print(f\"  - {file.name}\")\n",
        "for file in preprocessor.output_dir.glob(\"*.json\"):\n",
        "    print(f\"  - {file.name}\")\n",
        "\n",
        "print(\"\\nData collection and processing completed successfully!\")\n",
        "print(\"\\nNext: Open '03_embeddings_and_vector_store.ipynb' to learn about converting text to embeddings.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}