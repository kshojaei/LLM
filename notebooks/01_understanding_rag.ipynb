{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this notebook, you will understand:\n",
        "1. What RAG is and why it's important\n",
        "2. The components of a RAG system\n",
        "3. How retrieval and generation work together\n",
        "4. Common challenges and solutions\n",
        "\n",
        "## What is RAG?\n",
        "\n",
        "RAG stands for **Retrieval-Augmented Generation**. It's a technique that combines:\n",
        "- **Retrieval**: Finding relevant information from a knowledge base\n",
        "- **Augmented**: Enhancing the LLM's input with retrieved information\n",
        "- **Generation**: Using an LLM to generate responses based on the augmented input\n",
        "\n",
        "### Why do we need RAG?\n",
        "\n",
        "1. **Knowledge Cutoff**: LLMs have training data cutoffs and don't know recent information\n",
        "2. **Hallucination**: LLMs can make up facts that sound plausible\n",
        "3. **Domain Specificity**: General LLMs may not be experts in specific domains\n",
        "4. **Cost Efficiency**: Retrieving relevant context reduces the need for larger models\n",
        "\n",
        "## RAG Architecture Components\n",
        "\n",
        "```\n",
        "Query → Retriever → Retrieved Docs → Generator → Response\n",
        "                    ↑                        ↑\n",
        "                Knowledge Base            LLM\n",
        "```\n",
        "\n",
        "### 1. Knowledge Base\n",
        "- **Documents**: Raw text data (Wikipedia, PDFs, etc.)\n",
        "- **Chunks**: Split documents into manageable pieces\n",
        "- **Embeddings**: Vector representations of chunks\n",
        "- **Vector Store**: Database for similarity search\n",
        "\n",
        "### 2. Retriever\n",
        "- **Embedding Model**: Converts text to vectors\n",
        "- **Similarity Search**: Finds relevant chunks\n",
        "- **Ranking**: Orders results by relevance\n",
        "\n",
        "### 3. Generator\n",
        "- **LLM**: Large Language Model (Llama, GPT, etc.)\n",
        "- **Prompt Engineering**: How we format input to the LLM\n",
        "- **Context Management**: How we handle retrieved information\n",
        "\n",
        "## Let's Build Our Understanding Step by Step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "Ready to learn about RAG systems!\n"
          ]
        }
      ],
      "source": [
        "# First, let's import the libraries we'll need\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Ready to learn about RAG systems!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Understanding Text Embeddings\n",
        "\n",
        "Let's start by understanding how text becomes numbers that computers can work with. This is the foundation of everything we'll do.\n",
        "\n",
        "### What are Embeddings?\n",
        "\n",
        "Embeddings are numerical representations of text that capture semantic meaning. Words or sentences that are similar in meaning have similar embedding vectors.\n",
        "\n",
        "Think of it like this: if you had to describe a word using just 5 numbers, how would you do it? Embeddings do this with hundreds or thousands of dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Embedding Models: The Foundation of RAG\n",
        "\n",
        "Now that we understand what embeddings are, let's dive deeper into the different types of embedding models available and how to choose the right one for your task.\n",
        "\n",
        "### What are Embedding Models?\n",
        "\n",
        "Embedding models are neural networks trained to convert text into dense vector representations. They learn to map semantically similar text to nearby points in a high-dimensional space.\n",
        "\n",
        "### Types of Embedding Models\n",
        "\n",
        "#### 1. **Sentence Transformers** (What we're using)\n",
        "- **Purpose**: Convert entire sentences/paragraphs to vectors\n",
        "- **Best for**: Semantic search, document similarity, RAG systems\n",
        "- **Examples**: `all-MiniLM-L6-v2`, `all-mpnet-base-v2`, `BAAI/bge-base-en-v1.5`\n",
        "\n",
        "#### 2. **Word Embeddings**\n",
        "- **Purpose**: Convert individual words to vectors\n",
        "- **Best for**: Word-level analysis, language modeling\n",
        "- **Examples**: Word2Vec, GloVe, FastText\n",
        "\n",
        "#### 3. **Contextual Embeddings**\n",
        "- **Purpose**: Generate different vectors for the same word based on context\n",
        "- **Best for**: Understanding word meaning in context\n",
        "- **Examples**: BERT, RoBERTa, DeBERTa\n",
        "\n",
        "### Popular Embedding Models Comparison\n",
        "\n",
        "| Model | Size | Dimensions | Speed | Quality | Use Case |\n",
        "|-------|------|------------|-------|---------|----------|\n",
        "| **all-MiniLM-L6-v2** | 22MB | 384 | Very Fast | Good | Quick prototyping, small datasets |\n",
        "| **all-mpnet-base-v2** | 420MB | 768 | Fast | Very Good | General purpose, balanced |\n",
        "| **BAAI/bge-base-en-v1.5** | 438MB | 768 | Fast | Excellent | Production RAG systems |\n",
        "| **text-embedding-ada-002** | API | 1536 | Medium | Excellent | OpenAI's production model |\n",
        "| **text-embedding-3-large** | API | 3072 | Medium | Best | OpenAI's latest, highest quality |\n",
        "\n",
        "### What Do Big Companies Use?\n",
        "\n",
        "#### **OpenAI**\n",
        "- **Current**: `text-embedding-3-large` (3072 dimensions)\n",
        "- **Previous**: `text-embedding-ada-002` (1536 dimensions)\n",
        "- **Why**: Highest quality, but requires API calls and costs money\n",
        "\n",
        "#### **Facebook/Meta**\n",
        "- **Research**: DINOv2, LLaMA embeddings\n",
        "- **Production**: Often use sentence-transformers or custom models\n",
        "- **Why**: Need control over the model and data privacy\n",
        "\n",
        "#### **Google**\n",
        "- **Current**: Universal Sentence Encoder (USE)\n",
        "- **Research**: PaLM embeddings, T5-based models\n",
        "- **Why**: Integration with Google Cloud services\n",
        "\n",
        "#### **Microsoft**\n",
        "- **Current**: Azure OpenAI embeddings\n",
        "- **Research**: DeBERTa, CodeBERT\n",
        "- **Why**: Enterprise integration and security\n",
        "\n",
        "### Why all-MiniLM-L6-v2?\n",
        "\n",
        "We're using `all-MiniLM-L6-v2` because:\n",
        "\n",
        "1. **Small Size**: Only 22MB - easy to download and run locally\n",
        "2. **Fast**: Quick inference for learning and experimentation\n",
        "3. **Good Quality**: Decent performance for most tasks\n",
        "4. **Free**: No API costs or rate limits\n",
        "5. **Educational**: Perfect for understanding concepts\n",
        "\n",
        "### When to Use Different Models\n",
        "\n",
        "#### **For Learning/Prototyping**\n",
        "- `all-MiniLM-L6-v2` - Fast, small, good enough\n",
        "- `all-MiniLM-L12-v2` - Slightly better quality, still fast\n",
        "\n",
        "#### **For Production (Small Scale)**\n",
        "- `all-mpnet-base-v2` - Better quality, still manageable size\n",
        "- `BAAI/bge-base-en-v1.5` - Excellent quality, good for RAG\n",
        "\n",
        "#### **For Production (Large Scale)**\n",
        "- `text-embedding-3-large` - Best quality, but expensive\n",
        "- Custom fine-tuned models - Optimized for your specific domain\n",
        "\n",
        "### Understanding Cosine Similarity\n",
        "\n",
        "Cosine similarity is the most common way to measure how similar two embedding vectors are.\n",
        "\n",
        "#### **Mathematical Formula**\n",
        "\n",
        "For two vectors **A** and **B**, cosine similarity is:\n",
        "\n",
        "```\n",
        "cos(θ) = (A · B) / (||A|| × ||B||)\n",
        "```\n",
        "\n",
        "Where:\n",
        "- **A · B** = dot product of vectors A and B\n",
        "- **||A||** = magnitude (length) of vector A\n",
        "- **||B||** = magnitude (length) of vector B\n",
        "- **θ** = angle between the vectors\n",
        "\n",
        "#### **In Simple Terms**\n",
        "\n",
        "1. **Dot Product (A · B)**: Multiply corresponding elements and sum them up\n",
        "2. **Magnitude (||A||)**: Square each element, sum them, take square root\n",
        "3. **Cosine**: Divide dot product by the product of magnitudes\n",
        "\n",
        "#### **Why Cosine Similarity?**\n",
        "\n",
        "- **Range**: Always between -1 and 1\n",
        "- **Scale Invariant**: Doesn't matter if vectors are long or short\n",
        "- **Intuitive**: 1 = identical, 0 = orthogonal (no similarity), -1 = opposite\n",
        "- **Efficient**: Fast to compute\n",
        "\n",
        "#### **Example Calculation**\n",
        "\n",
        "If we have two simple 2D vectors:\n",
        "- A = [3, 4]\n",
        "- B = [1, 2]\n",
        "\n",
        "1. **Dot Product**: A · B = (3×1) + (4×2) = 3 + 8 = 11\n",
        "2. **Magnitude A**: ||A|| = √(3² + 4²) = √(9 + 16) = √25 = 5\n",
        "3. **Magnitude B**: ||B|| = √(1² + 2²) = √(1 + 4) = √5 ≈ 2.24\n",
        "4. **Cosine Similarity**: 11 / (5 × 2.24) ≈ 0.98\n",
        "\n",
        "This means the vectors are very similar (98% similar)!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exploring ArXiv dataset structure...\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Dataset scripts are no longer supported, but found scientific_papers.py",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[50], line 4\u001b[0m\n",
            "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now let's look at ArXiv data\u001b[39;00m\n",
            "\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExploring ArXiv dataset structure...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;32m----> 4\u001b[0m arxiv_sample \u001b[38;5;241m=\u001b[39m load_dataset(\n",
            "\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscientific_papers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marxiv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m      7\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain[:5]\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Just 5 samples\u001b[39;00m\n",
            "\u001b[1;32m      8\u001b[0m     \n",
            "\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mArXiv sample contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(arxiv_sample)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m papers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mKeys in each paper: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marxiv_sample\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1392\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n",
            "\u001b[1;32m   1387\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n",
            "\u001b[1;32m   1388\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n",
            "\u001b[1;32m   1389\u001b[0m )\n",
            "\u001b[1;32m   1391\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n",
            "\u001b[0;32m-> 1392\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n",
            "\u001b[1;32m   1393\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n",
            "\u001b[1;32m   1394\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n",
            "\u001b[1;32m   1395\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n",
            "\u001b[1;32m   1396\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n",
            "\u001b[1;32m   1397\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n",
            "\u001b[1;32m   1398\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n",
            "\u001b[1;32m   1399\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n",
            "\u001b[1;32m   1400\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n",
            "\u001b[1;32m   1401\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n",
            "\u001b[1;32m   1402\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n",
            "\u001b[1;32m   1403\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n",
            "\u001b[1;32m   1404\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n",
            "\u001b[1;32m   1405\u001b[0m )\n",
            "\u001b[1;32m   1407\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n",
            "\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1132\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n",
            "\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;32m   1131\u001b[0m     features \u001b[38;5;241m=\u001b[39m _fix_for_backward_compatible_features(features)\n",
            "\u001b[0;32m-> 1132\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m dataset_module_factory(\n",
            "\u001b[1;32m   1133\u001b[0m     path,\n",
            "\u001b[1;32m   1134\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n",
            "\u001b[1;32m   1135\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n",
            "\u001b[1;32m   1136\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n",
            "\u001b[1;32m   1137\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n",
            "\u001b[1;32m   1138\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n",
            "\u001b[1;32m   1139\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n",
            "\u001b[1;32m   1140\u001b[0m )\n",
            "\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n",
            "\u001b[1;32m   1142\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:1031\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n",
            "\u001b[1;32m   1026\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n",
            "\u001b[1;32m   1027\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n",
            "\u001b[1;32m   1028\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m   1029\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;32m   1030\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;32m-> 1031\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/load.py:989\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n",
            "\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[1;32m    982\u001b[0m     api\u001b[38;5;241m.\u001b[39mhf_hub_download(\n",
            "\u001b[1;32m    983\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n",
            "\u001b[1;32m    984\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n",
            "\u001b[0;32m   (...)\u001b[0m\n",
            "\u001b[1;32m    987\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n",
            "\u001b[1;32m    988\u001b[0m     )\n",
            "\u001b[0;32m--> 989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
            "\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n",
            "\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Dataset scripts are no longer supported, but found scientific_papers.py"
          ]
        }
      ],
      "source": [
        "# Use our DataCollector instead of direct load_dataset\n",
        "collector = DataCollector()\n",
        "arxiv_sample_data = collector.collect_arxiv_data(max_documents=5)\n",
        "print(f'Collected {len(arxiv_sample_data)} ArXiv papers')\n",
        "\n",
        "# Convert to the format expected by the rest of the notebook\n",
        "arxiv_sample = []\n",
        "for paper in arxiv_sample_data:\n",
        "    arxiv_sample.append({\n",
        "        'title': paper['title'],\n",
        "        'abstract': paper['abstract']\n",
        "    })\n",
        "\n",
        "print(f'ArXiv sample structure: {len(arxiv_sample)} papers')\n",
        "if arxiv_sample:\n",
        "    print(f'First paper title: {arxiv_sample[0][\"title\"]}')\n",
        "    print(f'First paper abstract length: {len(arxiv_sample[0][\"abstract\"])} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exploring Different Embedding Models\n",
            "==================================================\n",
            "Test sentences:\n",
            "1. Machine learning is a subset of artificial intelligence\n",
            "2. AI includes machine learning as one of its branches\n",
            "3. The weather is sunny today\n",
            "4. It's raining outside\n",
            "\n",
            "==================================================\n",
            "\n",
            "Testing all-MiniLM-L6-v2\n",
            "Description: Small, fast model (22MB)\n",
            "Loaded in 2.21s\n",
            "Encoded in 0.75s\n",
            "Dimensions: 384\n",
            "Similarity between sentences 1&2: 0.752\n",
            "\n",
            "Testing all-MiniLM-L12-v2\n",
            "Description: Slightly larger, better quality (33MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "824e2ab59ea2434397065edc9d3e1611",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a3d75a827df405cbb413dcc23ff7aa4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90586673081549c194d9d2553c170864",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "028b97d8b5584b1caaffbcba8ee02f51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1419350f4ae342eba0e75e1c71e2f9e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07dba8b49df44565988c0942a2ec8c18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a81e75ec5496408baf81320a862cb617",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3708113af21c44278d1a2303487a1113",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "765efae204d2467b8e59b803755bd123",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a07f6461ab04ab39876b98ad5ee4783",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "073ff90b452a405f824dbd4c5b6b3e38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded in 11.29s\n",
            "Encoded in 0.03s\n",
            "Dimensions: 384\n",
            "Similarity between sentences 1&2: 0.758\n",
            "\n",
            "Testing all-mpnet-base-v2\n",
            "Description: High quality, balanced (420MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "534e2f3c2bb3458aa4170e6951ca484c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4a8ea89ce0d4d6880592e1c8290f634",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f652b7e7dd9a419b9d0e6d891ea8a165",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f57b29ccf1ef4a17bc5275bdb8f19e72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "604d22d4c5bc4319b7332539762861a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb771355c5bd44b0ac309c54ac6d8bbb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49508985c1214c5c8f55f5b557c1cf20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3ac715430bb4c42b6fbb073863e8471",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c7ac3cafbf945c3b18b85bc0d244cdf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1f8d0248f2946d1a91941f296c7bcff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "363d1323a7974ec68861aa62e9e6276a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded in 1121.40s\n",
            "Encoded in 2.50s\n",
            "Dimensions: 768\n",
            "Similarity between sentences 1&2: 0.776\n",
            "\n",
            "==================================================\n",
            "Performance Comparison\n",
            "==================================================\n",
            "\n",
            "all-MiniLM-L6-v2:\n",
            "  Dimensions: 384\n",
            "  Load time: 2.21s\n",
            "  Encode time: 0.75s\n",
            "  Similarity (1&2): 0.752\n",
            "  Similarity (3&4): 0.455\n",
            "\n",
            "all-MiniLM-L12-v2:\n",
            "  Dimensions: 384\n",
            "  Load time: 11.29s\n",
            "  Encode time: 0.03s\n",
            "  Similarity (1&2): 0.758\n",
            "  Similarity (3&4): 0.420\n",
            "\n",
            "all-mpnet-base-v2:\n",
            "  Dimensions: 768\n",
            "  Load time: 1121.40s\n",
            "  Encode time: 2.50s\n",
            "  Similarity (1&2): 0.776\n",
            "  Similarity (3&4): 0.291\n",
            "\n",
            "==================================================\n",
            "Manual Cosine Similarity Calculation\n",
            "==================================================\n",
            "Using all-MiniLM-L6-v2 embeddings for manual calculation:\n",
            "Vector 1 shape: (384,)\n",
            "Vector 2 shape: (384,)\n",
            "\n",
            "Manual Calculation:\n",
            "1. Dot product: 0.752\n",
            "2. Magnitude 1: 1.000\n",
            "3. Magnitude 2: 1.000\n",
            "4. Cosine similarity: 0.752\n",
            "\n",
            "Sklearn result: 0.752\n",
            "Difference: 0.000000\n",
            "\n",
            "==================================================\n",
            "Key Takeaways\n",
            "==================================================\n",
            "1. Different models have different trade-offs between speed and quality\n",
            "2. Cosine similarity measures the angle between vectors, not their magnitude\n",
            "3. Values closer to 1 mean more similar, closer to 0 means less similar\n",
            "4. The mathematical formula is: cos(θ) = (A·B) / (||A|| × ||B||)\n",
            "5. For RAG systems, you need to balance speed, quality, and cost\n"
          ]
        }
      ],
      "source": [
        "# Let's explore different embedding models and see how they compare\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "\n",
        "print(\"Exploring Different Embedding Models\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test sentences\n",
        "test_sentences = [\n",
        "    \"Machine learning is a subset of artificial intelligence\",\n",
        "    \"AI includes machine learning as one of its branches\", \n",
        "    \"The weather is sunny today\",\n",
        "    \"It's raining outside\"\n",
        "]\n",
        "\n",
        "print(\"Test sentences:\")\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "    print(f\"{i+1}. {sentence}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Let's try different models and compare their performance\n",
        "models_to_test = [\n",
        "    (\"all-MiniLM-L6-v2\", \"Small, fast model (22MB)\"),\n",
        "    (\"all-MiniLM-L12-v2\", \"Slightly larger, better quality (33MB)\"),\n",
        "    (\"all-mpnet-base-v2\", \"High quality, balanced (420MB)\")\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, description in models_to_test:\n",
        "    print(f\"\\nTesting {model_name}\")\n",
        "    print(f\"Description: {description}\")\n",
        "    \n",
        "    try:\n",
        "        # Load model and measure time\n",
        "        start_time = time.time()\n",
        "        model = SentenceTransformer(model_name)\n",
        "        load_time = time.time() - start_time\n",
        "        \n",
        "        # Create embeddings and measure time\n",
        "        start_time = time.time()\n",
        "        embeddings = model.encode(test_sentences)\n",
        "        encode_time = time.time() - start_time\n",
        "        \n",
        "        # Calculate similarities\n",
        "        similarities = cosine_similarity(embeddings)\n",
        "        \n",
        "        # Store results\n",
        "        results[model_name] = {\n",
        "            'embeddings': embeddings,\n",
        "            'similarities': similarities,\n",
        "            'load_time': load_time,\n",
        "            'encode_time': encode_time,\n",
        "            'dimensions': embeddings.shape[1]\n",
        "        }\n",
        "        \n",
        "        print(f\"Loaded in {load_time:.2f}s\")\n",
        "        print(f\"Encoded in {encode_time:.2f}s\")\n",
        "        print(f\"Dimensions: {embeddings.shape[1]}\")\n",
        "        \n",
        "        # Show similarity between first two sentences (should be high)\n",
        "        sim_score = similarities[0][1]\n",
        "        print(f\"Similarity between sentences 1&2: {sim_score:.3f}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {model_name}: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Performance Comparison\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Compare results\n",
        "for model_name, result in results.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  Dimensions: {result['dimensions']}\")\n",
        "    print(f\"  Load time: {result['load_time']:.2f}s\")\n",
        "    print(f\"  Encode time: {result['encode_time']:.2f}s\")\n",
        "    print(f\"  Similarity (1&2): {result['similarities'][0][1]:.3f}\")\n",
        "    print(f\"  Similarity (3&4): {result['similarities'][2][3]:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Manual Cosine Similarity Calculation\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Let's manually calculate cosine similarity for educational purposes\n",
        "def manual_cosine_similarity(vec1, vec2):\n",
        "    \"\"\"Calculate cosine similarity manually for educational purposes.\"\"\"\n",
        "    # Step 1: Calculate dot product\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    \n",
        "    # Step 2: Calculate magnitudes\n",
        "    magnitude1 = np.sqrt(np.sum(vec1**2))\n",
        "    magnitude2 = np.sqrt(np.sum(vec2**2))\n",
        "    \n",
        "    # Step 3: Calculate cosine similarity\n",
        "    cosine_sim = dot_product / (magnitude1 * magnitude2)\n",
        "    \n",
        "    return dot_product, magnitude1, magnitude2, cosine_sim\n",
        "\n",
        "# Use the first model's embeddings for demonstration\n",
        "if results:\n",
        "    model_name = list(results.keys())[0]\n",
        "    embeddings = results[model_name]['embeddings']\n",
        "    \n",
        "    print(f\"Using {model_name} embeddings for manual calculation:\")\n",
        "    print(f\"Vector 1 shape: {embeddings[0].shape}\")\n",
        "    print(f\"Vector 2 shape: {embeddings[1].shape}\")\n",
        "    \n",
        "    # Calculate manually\n",
        "    dot_prod, mag1, mag2, cos_sim = manual_cosine_similarity(embeddings[0], embeddings[1])\n",
        "    \n",
        "    print(f\"\\nManual Calculation:\")\n",
        "    print(f\"1. Dot product: {dot_prod:.3f}\")\n",
        "    print(f\"2. Magnitude 1: {mag1:.3f}\")\n",
        "    print(f\"3. Magnitude 2: {mag2:.3f}\")\n",
        "    print(f\"4. Cosine similarity: {cos_sim:.3f}\")\n",
        "    \n",
        "    # Compare with sklearn\n",
        "    sklearn_sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "    print(f\"\\nSklearn result: {sklearn_sim:.3f}\")\n",
        "    print(f\"Difference: {abs(cos_sim - sklearn_sim):.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Key Takeaways\")\n",
        "print(\"=\"*50)\n",
        "print(\"1. Different models have different trade-offs between speed and quality\")\n",
        "print(\"2. Cosine similarity measures the angle between vectors, not their magnitude\")\n",
        "print(\"3. Values closer to 1 mean more similar, closer to 0 means less similar\")\n",
        "print(\"4. The mathematical formula is: cos(θ) = (A·B) / (||A|| × ||B||)\")\n",
        "print(\"5. For RAG systems, you need to balance speed, quality, and cost\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's create a simple example to understand embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# We'll use a lightweight embedding model for demonstration\n",
        "print(\"Loading embedding model...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # Small, fast model for learning\n",
        "\n",
        "# Let's create some example sentences\n",
        "sentences = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"A feline rested on the rug\", \n",
        "    \"Dogs are loyal pets\",\n",
        "    \"Cats and dogs are different animals\",\n",
        "    \"The weather is nice today\",\n",
        "    \"It's sunny outside\"\n",
        "]\n",
        "\n",
        "print(\"Creating embeddings for our example sentences...\")\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "print(f\"Created embeddings with shape: {embeddings.shape}\")\n",
        "print(f\"Each sentence is now represented by {embeddings.shape[1]} numbers\")\n",
        "\n",
        "# Let's see what these numbers look like\n",
        "print(f\"\\nFirst sentence embedding (first 10 values): {embeddings[0][:10]}\")\n",
        "print(f\"Second sentence embedding (first 10 values): {embeddings[1][:10]}\")\n",
        "\n",
        "# Calculate similarities\n",
        "similarities = cosine_similarity(embeddings)\n",
        "\n",
        "print(\"\\nSimilarity matrix (higher = more similar):\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"{i}: {sentence}\")\n",
        "\n",
        "print(\"\\nSimilarity scores:\")\n",
        "for i in range(len(sentences)):\n",
        "    for j in range(i+1, len(sentences)):\n",
        "        sim = similarities[i][j]\n",
        "        print(f\"'{sentences[i][:20]}...' vs '{sentences[j][:20]}...': {sim:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's visualize the embeddings to better understand how they work\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"📊 Visualizing Embeddings\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Use the first available model for visualization\n",
        "if results:\n",
        "    model_name = list(results.keys())[0]\n",
        "    embeddings = results[model_name]['embeddings']\n",
        "    similarities = results[model_name]['similarities']\n",
        "    \n",
        "    print(f\"Visualizing {model_name} embeddings...\")\n",
        "    \n",
        "    # Reduce dimensions to 2D for visualization\n",
        "    pca = PCA(n_components=2)\n",
        "    embeddings_2d = pca.fit_transform(embeddings)\n",
        "    \n",
        "    # Create the visualization\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Plot 1: 2D scatter plot of embeddings\n",
        "    colors = ['red', 'blue', 'green', 'orange']\n",
        "    for i, (x, y) in enumerate(embeddings_2d):\n",
        "        ax1.scatter(x, y, c=colors[i], s=200, alpha=0.7)\n",
        "        ax1.annotate(f'{i+1}', (x, y), xytext=(5, 5), textcoords='offset points', fontsize=12, fontweight='bold')\n",
        "    \n",
        "    ax1.set_title(f'2D Projection of Embeddings\\n({model_name})', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('First Principal Component')\n",
        "    ax1.set_ylabel('Second Principal Component')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add sentence labels\n",
        "    for i, sentence in enumerate(test_sentences):\n",
        "        ax1.text(0.02, 0.98 - i*0.15, f'{i+1}. {sentence[:30]}...', \n",
        "                transform=ax1.transAxes, fontsize=10, \n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=colors[i], alpha=0.3))\n",
        "    \n",
        "    # Plot 2: Similarity heatmap\n",
        "    im = ax2.imshow(similarities, cmap='RdYlBu_r', vmin=0, vmax=1)\n",
        "    ax2.set_title('Cosine Similarity Matrix', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Sentence Index')\n",
        "    ax2.set_ylabel('Sentence Index')\n",
        "    \n",
        "    # Add similarity values to the heatmap\n",
        "    for i in range(len(test_sentences)):\n",
        "        for j in range(len(test_sentences)):\n",
        "            text = ax2.text(j, i, f'{similarities[i, j]:.2f}',\n",
        "                           ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
        "    \n",
        "    # Set tick labels\n",
        "    ax2.set_xticks(range(len(test_sentences)))\n",
        "    ax2.set_yticks(range(len(test_sentences)))\n",
        "    ax2.set_xticklabels([f'S{i+1}' for i in range(len(test_sentences))])\n",
        "    ax2.set_yticklabels([f'S{i+1}' for i in range(len(test_sentences))])\n",
        "    \n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(im, ax=ax2)\n",
        "    cbar.set_label('Cosine Similarity', rotation=270, labelpad=20)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print interpretation\n",
        "    print(f\"\\n🔍 Interpretation:\")\n",
        "    print(f\"• Points closer together in the left plot are more similar\")\n",
        "    print(f\"• Red (S1) and Blue (S2) should be close - they're about ML/AI\")\n",
        "    print(f\"• Green (S3) and Orange (S4) should be close - they're about weather\")\n",
        "    print(f\"• The heatmap shows similarity scores between all pairs\")\n",
        "    print(f\"• Higher values (closer to 1) = more similar\")\n",
        "    \n",
        "    # Show the actual distances\n",
        "    print(f\"\\n📏 Actual Distances in 2D space:\")\n",
        "    for i in range(len(test_sentences)):\n",
        "        for j in range(i+1, len(test_sentences)):\n",
        "            dist = np.linalg.norm(embeddings_2d[i] - embeddings_2d[j])\n",
        "            sim = similarities[i][j]\n",
        "            print(f\"S{i+1} ↔ S{j+1}: Distance={dist:.3f}, Similarity={sim:.3f}\")\n",
        "\n",
        "else:\n",
        "    print(\"No model results available for visualization\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🎓 Educational Summary\")\n",
        "print(\"=\"*50)\n",
        "print(\"\"\"\n",
        "What we learned about embedding models:\n",
        "\n",
        "1. **Model Types**:\n",
        "   - Sentence Transformers: Best for RAG (what we use)\n",
        "   - Word Embeddings: For individual words\n",
        "   - Contextual Embeddings: For context-aware understanding\n",
        "\n",
        "2. **Popular Models**:\n",
        "   - all-MiniLM-L6-v2: Small, fast, good for learning\n",
        "   - all-mpnet-base-v2: Balanced quality and speed\n",
        "   - BAAI/bge-base-en-v1.5: Excellent for production RAG\n",
        "   - OpenAI's text-embedding-3-large: Best quality, but expensive\n",
        "\n",
        "3. **Big Company Choices**:\n",
        "   - OpenAI: Uses their own API models\n",
        "   - Meta/Facebook: Often uses sentence-transformers\n",
        "   - Google: Universal Sentence Encoder\n",
        "   - Microsoft: Azure OpenAI or custom models\n",
        "\n",
        "4. **Cosine Similarity Formula**:\n",
        "   cos(θ) = (A · B) / (||A|| × ||B||)\n",
        "   \n",
        "5. **Why Cosine Similarity**:\n",
        "   - Scale invariant (ignores vector length)\n",
        "   - Range from -1 to 1 (intuitive)\n",
        "   - Fast to compute\n",
        "   - Measures angle between vectors\n",
        "\n",
        "6. **Choosing a Model**:\n",
        "   - Learning: all-MiniLM-L6-v2 (small, fast)\n",
        "   - Production: BAAI/bge-base-en-v1.5 (good quality)\n",
        "   - Enterprise: OpenAI API (best quality, but costs money)\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Visualizing Embeddings\n",
        "\n",
        "Let's visualize these embeddings to see how similar sentences cluster together. This will help you understand how the system \"sees\" text relationships.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize embeddings using dimensionality reduction\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reduce dimensions from 384 to 2 for visualization\n",
        "print(\"Reducing dimensions for visualization...\")\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(sentences)-1))\n",
        "embeddings_2d = tsne.fit_transform(embeddings)\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot each sentence as a point\n",
        "for i, sentence in enumerate(sentences):\n",
        "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], s=200, alpha=0.7)\n",
        "    plt.annotate(sentence, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
        "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
        "\n",
        "plt.title('Embedding Visualization: Similar Sentences Cluster Together')\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add some insight\n",
        "plt.figtext(0.5, 0.02, \n",
        "           'Notice how \"cat\" sentences are close together, and \"weather\" sentences are close together!', \n",
        "           ha='center', fontsize=12, style='italic')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Let's also create a heatmap of similarities\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(similarities, annot=True, cmap='YlOrRd', \n",
        "           xticklabels=[f\"S{i}\" for i in range(len(sentences))],\n",
        "           yticklabels=[f\"S{i}\" for i in range(len(sentences))])\n",
        "plt.title('Similarity Heatmap')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Insights:\")\n",
        "print(\"1. Sentences about cats are more similar to each other than to weather sentences\")\n",
        "print(\"2. The embedding model understands semantic similarity, not just word overlap\")\n",
        "print(\"3. This is how RAG systems find relevant documents - by similarity in embedding space!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Building a Simple Retrieval System\n",
        "\n",
        "Now let's build a mini retrieval system to see how this works in practice. We'll create a small \"knowledge base\" and see how we can find relevant information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's create a simple knowledge base\n",
        "knowledge_base = [\n",
        "    \"Cats are small, typically furry, carnivorous mammals. They are often kept as house pets.\",\n",
        "    \"Dogs are domesticated mammals that have been selectively bred over thousands of years.\",\n",
        "    \"The weather forecast predicts rain for tomorrow with temperatures around 15 degrees Celsius.\",\n",
        "    \"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\",\n",
        "    \"Python is a high-level programming language known for its simplicity and readability.\",\n",
        "    \"The solar system consists of eight planets orbiting around the sun.\",\n",
        "    \"Cats communicate through various vocalizations including meowing, purring, and hissing.\",\n",
        "    \"Dogs are known for their loyalty and are often called 'man's best friend'.\",\n",
        "    \"Deep learning uses neural networks with multiple layers to process complex data patterns.\",\n",
        "    \"Jupiter is the largest planet in our solar system and has a Great Red Spot.\"\n",
        "]\n",
        "\n",
        "print(\"Our Knowledge Base:\")\n",
        "for i, fact in enumerate(knowledge_base):\n",
        "    print(f\"{i+1}. {fact}\")\n",
        "\n",
        "# Create embeddings for our knowledge base\n",
        "print(\"\\nCreating embeddings for knowledge base...\")\n",
        "kb_embeddings = model.encode(knowledge_base)\n",
        "\n",
        "print(f\"Knowledge base embeddings shape: {kb_embeddings.shape}\")\n",
        "\n",
        "# Function to find most relevant documents\n",
        "def find_relevant_docs(query, knowledge_base, kb_embeddings, model, top_k=3):\n",
        "    \"\"\"\n",
        "    Find the most relevant documents for a given query.\n",
        "    \"\"\"\n",
        "    # Encode the query\n",
        "    query_embedding = model.encode([query])\n",
        "    \n",
        "    # Calculate similarities\n",
        "    similarities = cosine_similarity(query_embedding, kb_embeddings)[0]\n",
        "    \n",
        "    # Get top-k most similar documents\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "    \n",
        "    results = []\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        results.append({\n",
        "            'rank': i + 1,\n",
        "            'similarity': similarities[idx],\n",
        "            'document': knowledge_base[idx]\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test our retrieval system\n",
        "test_queries = [\n",
        "    \"Tell me about cats\",\n",
        "    \"What is machine learning?\",\n",
        "    \"Information about planets\",\n",
        "    \"Dog behavior and characteristics\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING OUR RETRIEVAL SYSTEM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\nQuery: '{query}'\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    results = find_relevant_docs(query, knowledge_base, kb_embeddings, model)\n",
        "    \n",
        "    for result in results:\n",
        "        print(f\"Rank {result['rank']} (Similarity: {result['similarity']:.3f}): {result['document']}\")\n",
        "    \n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Interactive Query Testing\n",
        "\n",
        "Let's make this interactive! You can test your own queries and see how well our simple retrieval system works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive query testing\n",
        "def interactive_search():\n",
        "    \"\"\"\n",
        "    Interactive function to test queries against our knowledge base.\n",
        "    \"\"\"\n",
        "    print(\"🔍 Interactive Knowledge Base Search\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Enter your questions about cats, dogs, weather, programming, or planets!\")\n",
        "    print(\"Type 'quit' to exit.\")\n",
        "    print()\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            query = input(\"Enter your question: \").strip()\n",
        "            \n",
        "            if query.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "            \n",
        "            if not query:\n",
        "                print(\"Please enter a question.\")\n",
        "                continue\n",
        "            \n",
        "            print(f\"\\nSearching for: '{query}'\")\n",
        "            print(\"-\" * 40)\n",
        "            \n",
        "            results = find_relevant_docs(query, knowledge_base, kb_embeddings, model)\n",
        "            \n",
        "            if results:\n",
        "                for result in results:\n",
        "                    print(f\"📄 Rank {result['rank']} (Similarity: {result['similarity']:.3f})\")\n",
        "                    print(f\"   {result['document']}\")\n",
        "                    print()\n",
        "            else:\n",
        "                print(\"No results found.\")\n",
        "            \n",
        "            print(\"-\" * 40)\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nGoodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "# Uncomment the line below to run interactive search\n",
        "# interactive_search()\n",
        "\n",
        "# For notebook demonstration, let's show a few more examples\n",
        "demo_queries = [\n",
        "    \"How do cats communicate?\",\n",
        "    \"What programming language is simple?\",\n",
        "    \"Which planet is the largest?\",\n",
        "    \"Tell me about artificial intelligence\"\n",
        "]\n",
        "\n",
        "print(\"Demo: Testing more queries\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "for query in demo_queries:\n",
        "    print(f\"\\nQuery: '{query}'\")\n",
        "    results = find_relevant_docs(query, knowledge_base, kb_embeddings, model)\n",
        "    \n",
        "    for result in results[:2]:  # Show top 2 results\n",
        "        print(f\"  • {result['document']} (Score: {result['similarity']:.3f})\")\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(\"Try running 'interactive_search()' to test your own queries!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Understanding RAG Challenges\n",
        "\n",
        "Now that we've built a simple retrieval system, let's explore some of the challenges and limitations we'll face when building a real RAG system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's explore some RAG challenges with examples\n",
        "\n",
        "print(\"🔍 RAG System Challenges and Solutions\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Challenge 1: Ambiguous queries\n",
        "print(\"\\n1. AMBIGUOUS QUERIES\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "ambiguous_queries = [\n",
        "    \"What is Python?\",  # Could be programming language or snake\n",
        "    \"Tell me about Apple\",  # Could be fruit or company\n",
        "    \"What is the capital?\"  # Capital of what?\n",
        "]\n",
        "\n",
        "for query in ambiguous_queries:\n",
        "    print(f\"\\nQuery: '{query}'\")\n",
        "    results = find_relevant_docs(query, knowledge_base, kb_embeddings, model, top_k=2)\n",
        "    for result in results:\n",
        "        print(f\"  • {result['document']} (Score: {result['similarity']:.3f})\")\n",
        "\n",
        "print(\"\\n Challenge: Without context, the system might retrieve irrelevant information\")\n",
        "print(\" Solution: Better query understanding, context from conversation history\")\n",
        "\n",
        "# Challenge 2: Chunking problems\n",
        "print(\"\\n\\n2. CHUNKING CHALLENGES\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Let's create a longer document and see how chunking affects retrieval\n",
        "long_document = \"\"\"\n",
        "Cats are fascinating animals with a rich history of domestication. \n",
        "They were first domesticated in ancient Egypt around 4,000 years ago.\n",
        "Cats are obligate carnivores, meaning they must eat meat to survive.\n",
        "They have excellent night vision and can see in light levels six times lower than humans.\n",
        "Cats communicate through various methods including vocalizations, body language, and scent marking.\n",
        "The domestic cat is a member of the Felidae family, which includes lions, tigers, and other wild cats.\n",
        "Cats are known for their independent nature, but they can form strong bonds with their human companions.\n",
        "They spend a significant portion of their day sleeping, typically 12-16 hours.\n",
        "Cats have retractable claws that help them climb and hunt effectively.\n",
        "The average lifespan of a domestic cat is 13-17 years, though some live much longer.\n",
        "\"\"\"\n",
        "\n",
        "# Split into chunks (simulating chunking)\n",
        "chunks = [\n",
        "    \"Cats are fascinating animals with a rich history of domestication. They were first domesticated in ancient Egypt around 4,000 years ago.\",\n",
        "    \"Cats are obligate carnivores, meaning they must eat meat to survive. They have excellent night vision and can see in light levels six times lower than humans.\",\n",
        "    \"Cats communicate through various methods including vocalizations, body language, and scent marking. The domestic cat is a member of the Felidae family, which includes lions, tigers, and other wild cats.\",\n",
        "    \"Cats are known for their independent nature, but they can form strong bonds with their human companions. They spend a significant portion of their day sleeping, typically 12-16 hours.\",\n",
        "    \"Cats have retractable claws that help them climb and hunt effectively. The average lifespan of a domestic cat is 13-17 years, though some live much longer.\"\n",
        "]\n",
        "\n",
        "print(\"Original document split into chunks:\")\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk[:60]}...\")\n",
        "\n",
        "# Test retrieval with chunked content\n",
        "chunk_embeddings = model.encode(chunks)\n",
        "test_query = \"How long do cats sleep?\"\n",
        "\n",
        "print(f\"\\nQuery: '{test_query}'\")\n",
        "results = find_relevant_docs(test_query, chunks, chunk_embeddings, model, top_k=2)\n",
        "\n",
        "for result in results:\n",
        "    print(f\"  • {result['document']} (Score: {result['similarity']:.3f})\")\n",
        "\n",
        "print(\"\\n Challenge: Information might be split across chunks\")\n",
        "print(\" Solution: Overlapping chunks, better chunking strategies, re-ranking\")\n",
        "\n",
        "# Challenge 3: Similarity threshold\n",
        "print(\"\\n\\n3. SIMILARITY THRESHOLDS\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "test_queries = [\n",
        "    \"What is machine learning?\",\n",
        "    \"Tell me about neural networks\",\n",
        "    \"How does artificial intelligence work?\",\n",
        "    \"What is the weather like?\"  # This should have low similarity to our knowledge base\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    results = find_relevant_docs(query, knowledge_base, kb_embeddings, model, top_k=1)\n",
        "    best_match = results[0]\n",
        "    \n",
        "    relevance = \" Relevant\" if best_match['similarity'] > 0.3 else \" Not Relevant\"\n",
        "    print(f\"'{query}' -> Best match: {best_match['similarity']:.3f} {relevance}\")\n",
        "\n",
        "print(\"\\n Challenge: Setting appropriate similarity thresholds\")\n",
        "print(\" Solution: Experiment with thresholds, use multiple retrieval strategies\")\n",
        "\n",
        "# Challenge 4: Hallucination prevention\n",
        "print(\"\\n\\n4. HALLUCINATION PREVENTION\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Simulate what might happen if we don't have relevant information\n",
        "irrelevant_query = \"What is the population of Mars?\"\n",
        "results = find_relevant_docs(irrelevant_query, knowledge_base, kb_embeddings, model, top_k=3)\n",
        "\n",
        "print(f\"Query: '{irrelevant_query}'\")\n",
        "print(\"Retrieved documents:\")\n",
        "for result in results:\n",
        "    print(f\"  • {result['document']} (Score: {result['similarity']:.3f})\")\n",
        "\n",
        "print(\"\\n Challenge: LLM might generate answers even when no relevant info exists\")\n",
        "print(\" Solution: Check retrieval quality, use confidence thresholds, provide 'no relevant info' responses\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"These challenges show why RAG systems need careful design!\")\n",
        "print(\"In the next notebooks, we'll learn how to address each of these issues.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: What We've Learned\n",
        "\n",
        "Congratulations! You've now built and tested a basic retrieval system. Here's what we've accomplished:\n",
        "\n",
        "### Key Concepts Covered:\n",
        "1. **Embeddings**: Converting text to numerical vectors that capture semantic meaning\n",
        "2. **Similarity Search**: Finding relevant documents using cosine similarity\n",
        "3. **Retrieval Pipeline**: Query → Embedding → Similarity → Ranking\n",
        "4. **Real Challenges**: Ambiguous queries, chunking, thresholds, hallucination\n",
        "\n",
        "### What You Can Do Now:\n",
        "- Understand how text embeddings work\n",
        "- Build a simple retrieval system\n",
        "- Test queries against a knowledge base\n",
        "- Identify common RAG challenges\n",
        "- Visualize how embeddings represent text relationships\n",
        "\n",
        "### Next Steps:\n",
        "In the upcoming notebooks, we'll tackle:\n",
        "1. **Data Collection** - Getting real Wikipedia and ArXiv data\n",
        "2. **Text Preprocessing** - Cleaning and chunking strategies\n",
        "3. **Advanced Retrieval** - Better embedding models and search techniques\n",
        "4. **Generation** - Connecting LLMs to create complete answers\n",
        "5. **Evaluation** - Measuring how well our system works\n",
        "\n",
        "### Try This:\n",
        "Run the `interactive_search()` function above to test your own queries against our knowledge base. Experiment with different types of questions and see how the similarity scores change!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
