{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this notebook, you will understand:\n",
        "1. What RAG is and why it's important\n",
        "2. The components of a RAG system\n",
        "3. How retrieval and generation work together\n",
        "4. Common challenges and solutions\n",
        "\n",
        "## What is RAG?\n",
        "\n",
        "RAG stands for **Retrieval-Augmented Generation**. It's a technique that combines:\n",
        "- **Retrieval**: Finding relevant information from a knowledge base\n",
        "- **Augmented**: Enhancing the LLM's input with retrieved information\n",
        "- **Generation**: Using an LLM to generate responses based on the augmented input\n",
        "\n",
        "### Why do we need RAG?\n",
        "\n",
        "1. **Knowledge Cutoff**: LLMs have training data cutoffs and don't know recent information\n",
        "2. **Hallucination**: LLMs can make up facts that sound plausible\n",
        "3. **Domain Specificity**: General LLMs may not be experts in specific domains\n",
        "4. **Cost Efficiency**: Retrieving relevant context reduces the need for larger models\n",
        "\n",
        "## RAG Architecture Components\n",
        "\n",
        "```\n",
        "Query â†’ Retriever â†’ Retrieved Docs â†’ Generator â†’ Response\n",
        "                    â†‘                        â†‘\n",
        "                Knowledge Base            LLM\n",
        "```\n",
        "\n",
        "### 1. Knowledge Base\n",
        "- **Documents**: Raw text data (Wikipedia, PDFs, etc.)\n",
        "- **Chunks**: Split documents into manageable pieces\n",
        "- **Embeddings**: Vector representations of chunks\n",
        "- **Vector Store**: Database for similarity search\n",
        "\n",
        "### 2. Retriever\n",
        "- **Embedding Model**: Converts text to vectors\n",
        "- **Similarity Search**: Finds relevant chunks\n",
        "- **Ranking**: Orders results by relevance\n",
        "\n",
        "### 3. Generator\n",
        "- **LLM**: Large Language Model (Llama, GPT, etc.)\n",
        "- **Prompt Engineering**: How we format input to the LLM\n",
        "- **Context Management**: How we handle retrieved information\n",
        "\n",
        "## Let's Build Our Understanding Step by Step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "Ready to learn about RAG systems!\n"
          ]
        }
      ],
      "source": [
        "# First, let's import the libraries we'll need\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Ready to learn about RAG systems!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 1: Understanding Text Embeddings\n",
        "\n",
        "Let's start by understanding how text becomes numbers that computers can work with. This is the foundation of everything we'll do.\n",
        "\n",
        "### What are Embeddings?\n",
        "\n",
        "Embeddings are numerical representations of text that capture semantic meaning. Words or sentences that are similar in meaning have similar embedding vectors.\n",
        "\n",
        "Think of it like this: if you had to describe a word using just 5 numbers, how would you do it? Embeddings do this with hundreds or thousands of dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's create a simple example to understand embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# We'll use a lightweight embedding model for demonstration\n",
        "print(\"Loading embedding model...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # Small, fast model for learning\n",
        "\n",
        "# Let's create some example sentences\n",
        "sentences = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"A feline rested on the rug\", \n",
        "    \"Dogs are loyal pets\",\n",
        "    \"Cats and dogs are different animals\",\n",
        "    \"The weather is nice today\",\n",
        "    \"It's sunny outside\"\n",
        "]\n",
        "\n",
        "print(\"Creating embeddings for our example sentences...\")\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "print(f\"Created embeddings with shape: {embeddings.shape}\")\n",
        "print(f\"Each sentence is now represented by {embeddings.shape[1]} numbers\")\n",
        "\n",
        "# Let's see what these numbers look like\n",
        "print(f\"\\nFirst sentence embedding (first 10 values): {embeddings[0][:10]}\")\n",
        "print(f\"Second sentence embedding (first 10 values): {embeddings[1][:10]}\")\n",
        "\n",
        "# Calculate similarities\n",
        "similarities = cosine_similarity(embeddings)\n",
        "\n",
        "print(\"\\nSimilarity matrix (higher = more similar):\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"{i}: {sentence}\")\n",
        "\n",
        "print(\"\\nSimilarity scores:\")\n",
        "for i in range(len(sentences)):\n",
        "    for j in range(i+1, len(sentences)):\n",
        "        sim = similarities[i][j]\n",
        "        print(f\"'{sentences[i][:20]}...' vs '{sentences[j][:20]}...': {sim:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Visualizing Embeddings\n",
        "\n",
        "Let's visualize these embeddings to see how similar sentences cluster together. This will help you understand how the system \"sees\" text relationships.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize embeddings using dimensionality reduction\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reduce dimensions from 384 to 2 for visualization\n",
        "print(\"Reducing dimensions for visualization...\")\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(sentences)-1))\n",
        "embeddings_2d = tsne.fit_transform(embeddings)\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot each sentence as a point\n",
        "for i, sentence in enumerate(sentences):\n",
        "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], s=200, alpha=0.7)\n",
        "    plt.annotate(sentence, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
        "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
        "\n",
        "plt.title('Embedding Visualization: Similar Sentences Cluster Together')\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add some insight\n",
        "plt.figtext(0.5, 0.02, \n",
        "           'Notice how \"cat\" sentences are close together, and \"weather\" sentences are close together!', \n",
        "           ha='center', fontsize=12, style='italic')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Let's also create a heatmap of similarities\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(similarities, annot=True, cmap='YlOrRd', \n",
        "           xticklabels=[f\"S{i}\" for i in range(len(sentences))],\n",
        "           yticklabels=[f\"S{i}\" for i in range(len(sentences))])\n",
        "plt.title('Similarity Heatmap')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Insights:\")\n",
        "print(\"1. Sentences about cats are more similar to each other than to weather sentences\")\n",
        "print(\"2. The embedding model understands semantic similarity, not just word overlap\")\n",
        "print(\"3. This is how RAG systems find relevant documents - by similarity in embedding space!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Building a Simple Retrieval System\n",
        "\n",
        "Now let's build a mini retrieval system to see how this works in practice. We'll create a small \"knowledge base\" and see how we can find relevant information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's create a simple knowledge base\n",
        "knowledge_base = [\n",
        "    \"Cats are small, typically furry, carnivorous mammals. They are often kept as house pets.\",\n",
        "    \"Dogs are domesticated mammals that have been selectively bred over thousands of years.\",\n",
        "    \"The weather forecast predicts rain for tomorrow with temperatures around 15 degrees Celsius.\",\n",
        "    \"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\",\n",
        "    \"Python is a high-level programming language known for its simplicity and readability.\",\n",
        "    \"The solar system consists of eight planets orbiting around the sun.\",\n",
        "    \"Cats communicate through various vocalizations including meowing, purring, and hissing.\",\n",
        "    \"Dogs are known for their loyalty and are often called 'man's best friend'.\",\n",
        "    \"Deep learning uses neural networks with multiple layers to process complex data patterns.\",\n",
        "    \"Jupiter is the largest planet in our solar system and has a Great Red Spot.\"\n",
        "]\n",
        "\n",
        "print(\"Our Knowledge Base:\")\n",
        "for i, fact in enumerate(knowledge_base):\n",
        "    print(f\"{i+1}. {fact}\")\n",
        "\n",
        "# Create embeddings for our knowledge base\n",
        "print(\"\\nCreating embeddings for knowledge base...\")\n",
        "kb_embeddings = model.encode(knowledge_base)\n",
        "\n",
        "print(f\"Knowledge base embeddings shape: {kb_embeddings.shape}\")\n",
        "\n",
        "# Function to find most relevant documents\n",
        "def find_relevant_docs(query, knowledge_base, kb_embeddings, model, top_k=3):\n",
        "    \"\"\"\n",
        "    Find the most relevant documents for a given query.\n",
        "    \"\"\"\n",
        "    # Encode the query\n",
        "    query_embedding = model.encode([query])\n",
        "    \n",
        "    # Calculate similarities\n",
        "    similarities = cosine_similarity(query_embedding, kb_embeddings)[0]\n",
        "    \n",
        "    # Get top-k most similar documents\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "    \n",
        "    results = []\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        results.append({\n",
        "            'rank': i + 1,\n",
        "            'similarity': similarities[idx],\n",
        "            'document': knowledge_base[idx]\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test our retrieval system\n",
        "test_queries = [\n",
        "    \"Tell me about cats\",\n",
        "    \"What is machine learning?\",\n",
        "    \"Information about planets\",\n",
        "    \"Dog behavior and characteristics\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING OUR RETRIEVAL SYSTEM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\nQuery: '{query}'\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    results = find_relevant_docs(query, knowledge_base, kb_embeddings, model)\n",
        "    \n",
        "    for result in results:\n",
        "        print(f\"Rank {result['rank']} (Similarity: {result['similarity']:.3f}): {result['document']}\")\n",
        "    \n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Interactive Query Testing\n",
        "\n",
        "Let's make this interactive! You can test your own queries and see how well our simple retrieval system works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive query testing\n",
        "def interactive_search():\n",
        "    \"\"\"\n",
        "    Interactive function to test queries against our knowledge base.\n",
        "    \"\"\"\n",
        "    print(\"ðŸ” Interactive Knowledge Base Search\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Enter your questions about cats, dogs, weather, programming, or planets!\")\n",
        "    print(\"Type 'quit' to exit.\")\n",
        "    print()\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            query = input(\"Enter your question: \").strip()\n",
        "            \n",
        "            if query.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "            \n",
        "            if not query:\n",
        "                print(\"Please enter a question.\")\n",
        "                continue\n",
        "            \n",
        "            print(f\"\\nSearching for: '{query}'\")\n",
        "            print(\"-\" * 40)\n",
        "            \n",
        "            results = find_relevant_docs(query, knowledge_base, kb_embeddings, model)\n",
        "            \n",
        "            if results:\n",
        "                for result in results:\n",
        "                    print(f\"ðŸ“„ Rank {result['rank']} (Similarity: {result['similarity']:.3f})\")\n",
        "                    print(f\"   {result['document']}\")\n",
        "                    print()\n",
        "            else:\n",
        "                print(\"No results found.\")\n",
        "            \n",
        "            print(\"-\" * 40)\n",
        "            \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nGoodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "# Uncomment the line below to run interactive search\n",
        "# interactive_search()\n",
        "\n",
        "# For notebook demonstration, let's show a few more examples\n",
        "demo_queries = [\n",
        "    \"How do cats communicate?\",\n",
        "    \"What programming language is simple?\",\n",
        "    \"Which planet is the largest?\",\n",
        "    \"Tell me about artificial intelligence\"\n",
        "]\n",
        "\n",
        "print(\"Demo: Testing more queries\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "for query in demo_queries:\n",
        "    print(f\"\\nQuery: '{query}'\")\n",
        "    results = find_relevant_docs(query, knowledge_base, kb_embeddings, model)\n",
        "    \n",
        "    for result in results[:2]:  # Show top 2 results\n",
        "        print(f\"  â€¢ {result['document']} (Score: {result['similarity']:.3f})\")\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(\"Try running 'interactive_search()' to test your own queries!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Understanding RAG Challenges\n",
        "\n",
        "Now that we've built a simple retrieval system, let's explore some of the challenges and limitations we'll face when building a real RAG system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's explore some RAG challenges with examples\n",
        "\n",
        "print(\"ðŸ” RAG System Challenges and Solutions\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Challenge 1: Ambiguous queries\n",
        "print(\"\\n1. AMBIGUOUS QUERIES\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "ambiguous_queries = [\n",
        "    \"What is Python?\",  # Could be programming language or snake\n",
        "    \"Tell me about Apple\",  # Could be fruit or company\n",
        "    \"What is the capital?\"  # Capital of what?\n",
        "]\n",
        "\n",
        "for query in ambiguous_queries:\n",
        "    print(f\"\\nQuery: '{query}'\")\n",
        "    results = find_relevant_docs(query, knowledge_base, kb_embeddings, model, top_k=2)\n",
        "    for result in results:\n",
        "        print(f\"  â€¢ {result['document']} (Score: {result['similarity']:.3f})\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Challenge: Without context, the system might retrieve irrelevant information\")\n",
        "print(\"ðŸ’¡ Solution: Better query understanding, context from conversation history\")\n",
        "\n",
        "# Challenge 2: Chunking problems\n",
        "print(\"\\n\\n2. CHUNKING CHALLENGES\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Let's create a longer document and see how chunking affects retrieval\n",
        "long_document = \"\"\"\n",
        "Cats are fascinating animals with a rich history of domestication. \n",
        "They were first domesticated in ancient Egypt around 4,000 years ago.\n",
        "Cats are obligate carnivores, meaning they must eat meat to survive.\n",
        "They have excellent night vision and can see in light levels six times lower than humans.\n",
        "Cats communicate through various methods including vocalizations, body language, and scent marking.\n",
        "The domestic cat is a member of the Felidae family, which includes lions, tigers, and other wild cats.\n",
        "Cats are known for their independent nature, but they can form strong bonds with their human companions.\n",
        "They spend a significant portion of their day sleeping, typically 12-16 hours.\n",
        "Cats have retractable claws that help them climb and hunt effectively.\n",
        "The average lifespan of a domestic cat is 13-17 years, though some live much longer.\n",
        "\"\"\"\n",
        "\n",
        "# Split into chunks (simulating chunking)\n",
        "chunks = [\n",
        "    \"Cats are fascinating animals with a rich history of domestication. They were first domesticated in ancient Egypt around 4,000 years ago.\",\n",
        "    \"Cats are obligate carnivores, meaning they must eat meat to survive. They have excellent night vision and can see in light levels six times lower than humans.\",\n",
        "    \"Cats communicate through various methods including vocalizations, body language, and scent marking. The domestic cat is a member of the Felidae family, which includes lions, tigers, and other wild cats.\",\n",
        "    \"Cats are known for their independent nature, but they can form strong bonds with their human companions. They spend a significant portion of their day sleeping, typically 12-16 hours.\",\n",
        "    \"Cats have retractable claws that help them climb and hunt effectively. The average lifespan of a domestic cat is 13-17 years, though some live much longer.\"\n",
        "]\n",
        "\n",
        "print(\"Original document split into chunks:\")\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk[:60]}...\")\n",
        "\n",
        "# Test retrieval with chunked content\n",
        "chunk_embeddings = model.encode(chunks)\n",
        "test_query = \"How long do cats sleep?\"\n",
        "\n",
        "print(f\"\\nQuery: '{test_query}'\")\n",
        "results = find_relevant_docs(test_query, chunks, chunk_embeddings, model, top_k=2)\n",
        "\n",
        "for result in results:\n",
        "    print(f\"  â€¢ {result['document']} (Score: {result['similarity']:.3f})\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Challenge: Information might be split across chunks\")\n",
        "print(\"ðŸ’¡ Solution: Overlapping chunks, better chunking strategies, re-ranking\")\n",
        "\n",
        "# Challenge 3: Similarity threshold\n",
        "print(\"\\n\\n3. SIMILARITY THRESHOLDS\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "test_queries = [\n",
        "    \"What is machine learning?\",\n",
        "    \"Tell me about neural networks\",\n",
        "    \"How does artificial intelligence work?\",\n",
        "    \"What is the weather like?\"  # This should have low similarity to our knowledge base\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    results = find_relevant_docs(query, knowledge_base, kb_embeddings, model, top_k=1)\n",
        "    best_match = results[0]\n",
        "    \n",
        "    relevance = \"âœ… Relevant\" if best_match['similarity'] > 0.3 else \"âŒ Not Relevant\"\n",
        "    print(f\"'{query}' -> Best match: {best_match['similarity']:.3f} {relevance}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Challenge: Setting appropriate similarity thresholds\")\n",
        "print(\"ðŸ’¡ Solution: Experiment with thresholds, use multiple retrieval strategies\")\n",
        "\n",
        "# Challenge 4: Hallucination prevention\n",
        "print(\"\\n\\n4. HALLUCINATION PREVENTION\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Simulate what might happen if we don't have relevant information\n",
        "irrelevant_query = \"What is the population of Mars?\"\n",
        "results = find_relevant_docs(irrelevant_query, knowledge_base, kb_embeddings, model, top_k=3)\n",
        "\n",
        "print(f\"Query: '{irrelevant_query}'\")\n",
        "print(\"Retrieved documents:\")\n",
        "for result in results:\n",
        "    print(f\"  â€¢ {result['document']} (Score: {result['similarity']:.3f})\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Challenge: LLM might generate answers even when no relevant info exists\")\n",
        "print(\"ðŸ’¡ Solution: Check retrieval quality, use confidence thresholds, provide 'no relevant info' responses\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"These challenges show why RAG systems need careful design!\")\n",
        "print(\"In the next notebooks, we'll learn how to address each of these issues.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: What We've Learned\n",
        "\n",
        "Congratulations! You've now built and tested a basic retrieval system. Here's what we've accomplished:\n",
        "\n",
        "### Key Concepts Covered:\n",
        "1. **Embeddings**: Converting text to numerical vectors that capture semantic meaning\n",
        "2. **Similarity Search**: Finding relevant documents using cosine similarity\n",
        "3. **Retrieval Pipeline**: Query â†’ Embedding â†’ Similarity â†’ Ranking\n",
        "4. **Real Challenges**: Ambiguous queries, chunking, thresholds, hallucination\n",
        "\n",
        "### What You Can Do Now:\n",
        "- âœ… Understand how text embeddings work\n",
        "- âœ… Build a simple retrieval system\n",
        "- âœ… Test queries against a knowledge base\n",
        "- âœ… Identify common RAG challenges\n",
        "- âœ… Visualize how embeddings represent text relationships\n",
        "\n",
        "### Next Steps:\n",
        "In the upcoming notebooks, we'll tackle:\n",
        "1. **Data Collection** - Getting real Wikipedia and ArXiv data\n",
        "2. **Text Preprocessing** - Cleaning and chunking strategies\n",
        "3. **Advanced Retrieval** - Better embedding models and search techniques\n",
        "4. **Generation** - Connecting LLMs to create complete answers\n",
        "5. **Evaluation** - Measuring how well our system works\n",
        "\n",
        "### Try This:\n",
        "Run the `interactive_search()` function above to test your own queries against our knowledge base. Experiment with different types of questions and see how the similarity scores change!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
