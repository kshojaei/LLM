{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vector Database for Production RAG Systems\n",
        "\n",
        "This notebook demonstrates how to build and manage vector databases for production RAG applications.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand vector database architectures\n",
        "- Learn to manage multiple vector database backends\n",
        "- Implement production-ready vector operations\n",
        "- Handle large-scale document indexing\n",
        "- Monitor and maintain vector databases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import time\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Import our vector database manager\n",
        "from vector_db.vector_manager import VectorDatabaseManager, create_vector_database\n",
        "from data.preprocess_data import TextPreprocessor\n",
        "from config import DATA_DIR\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Vector Database Backends Comparison\n",
        "\n",
        "Let's compare different vector database backends for production use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sample data\n",
        "chunks_path = DATA_DIR / \"processed\" / \"all_chunks.json\"\n",
        "if chunks_path.exists():\n",
        "    with open(chunks_path, 'r') as f:\n",
        "        chunks_data = json.load(f)\n",
        "    print(f\"Loaded {len(chunks_data)} chunks from processed data\")\n",
        "else:\n",
        "    print(\"No processed chunks found. Please run previous notebooks first.\")\n",
        "    chunks_data = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare sample documents for testing\n",
        "sample_docs = []\n",
        "for i, chunk in enumerate(chunks_data[:100]):  # Use first 100 chunks\n",
        "    sample_docs.append({\n",
        "        \"id\": f\"chunk_{i}\",\n",
        "        \"text\": chunk.get(\"text\", \"\"),\n",
        "        \"metadata\": {\n",
        "            \"source\": chunk.get(\"source\", \"unknown\"),\n",
        "            \"chunk_id\": i,\n",
        "            \"length\": len(chunk.get(\"text\", \"\")),\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "    })\n",
        "\n",
        "print(f\"Prepared {len(sample_docs)} sample documents\")\n",
        "print(f\"Sample document: {sample_docs[0]['text'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ChromaDB - Local Vector Database\n",
        "\n",
        "ChromaDB is excellent for local development and small to medium-scale applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize ChromaDB\n",
        "print(\"Initializing ChromaDB...\")\n",
        "chroma_manager = VectorDatabaseManager(\n",
        "    backend=\"chromadb\",\n",
        "    collection_name=\"production_docs\",\n",
        "    embedding_model=\"BAAI/bge-base-en-v1.5\"\n",
        ")\n",
        "\n",
        "# Add documents\n",
        "print(\"Adding documents to ChromaDB...\")\n",
        "start_time = time.time()\n",
        "\n",
        "texts = [doc[\"text\"] for doc in sample_docs]\n",
        "metadatas = [doc[\"metadata\"] for doc in sample_docs]\n",
        "ids = [doc[\"id\"] for doc in sample_docs]\n",
        "\n",
        "result = chroma_manager.add_documents(texts, metadatas, ids)\n",
        "chroma_time = time.time() - start_time\n",
        "\n",
        "print(f\"ChromaDB Results: {result}\")\n",
        "print(f\"Time taken: {chroma_time:.2f} seconds\")\n",
        "print(f\"Documents per second: {len(sample_docs)/chroma_time:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test ChromaDB search\n",
        "print(\"Testing ChromaDB search...\")\n",
        "queries = [\n",
        "    \"What is machine learning?\",\n",
        "    \"How does deep learning work?\",\n",
        "    \"Explain artificial intelligence\",\n",
        "    \"What are neural networks?\"\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    results = chroma_manager.search(query, top_k=3)\n",
        "    for i, result in enumerate(results):\n",
        "        print(f\"  {i+1}. {result['document'][:100]}... (score: {result['score']:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. FAISS - High-Performance Vector Search\n",
        "\n",
        "FAISS is Facebook's library for efficient similarity search and clustering of dense vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize FAISS\n",
        "print(\"Initializing FAISS...\")\n",
        "faiss_manager = VectorDatabaseManager(\n",
        "    backend=\"faiss\",\n",
        "    collection_name=\"production_docs_faiss\",\n",
        "    embedding_model=\"BAAI/bge-base-en-v1.5\"\n",
        ")\n",
        "\n",
        "# Add documents\n",
        "print(\"Adding documents to FAISS...\")\n",
        "start_time = time.time()\n",
        "\n",
        "result = faiss_manager.add_documents(texts, metadatas, ids)\n",
        "faiss_time = time.time() - start_time\n",
        "\n",
        "print(f\"FAISS Results: {result}\")\n",
        "print(f\"Time taken: {faiss_time:.2f} seconds\")\n",
        "print(f\"Documents per second: {len(sample_docs)/faiss_time:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test FAISS search\n",
        "print(\"Testing FAISS search...\")\n",
        "for query in queries:\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    results = faiss_manager.search(query, top_k=3)\n",
        "    for i, result in enumerate(results):\n",
        "        print(f\"  {i+1}. {result['document'][:100]}... (score: {result['score']:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Performance Comparison\n",
        "\n",
        "Let's compare the performance of different backends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance comparison\n",
        "performance_data = {\n",
        "    'Backend': ['ChromaDB', 'FAISS'],\n",
        "    'Indexing Time (s)': [chroma_time, faiss_time],\n",
        "    'Docs per Second': [len(sample_docs)/chroma_time, len(sample_docs)/faiss_time]\n",
        "}\n",
        "\n",
        "df_performance = pd.DataFrame(performance_data)\n",
        "print(\"Performance Comparison:\")\n",
        "print(df_performance.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Indexing time comparison\n",
        "ax1.bar(df_performance['Backend'], df_performance['Indexing Time (s)'], color=['skyblue', 'lightcoral'])\n",
        "ax1.set_title('Indexing Time Comparison')\n",
        "ax1.set_ylabel('Time (seconds)')\n",
        "\n",
        "# Documents per second comparison\n",
        "ax2.bar(df_performance['Backend'], df_performance['Docs per Second'], color=['skyblue', 'lightcoral'])\n",
        "ax2.set_title('Indexing Speed Comparison')\n",
        "ax2.set_ylabel('Documents per Second')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Search Performance Testing\n",
        "\n",
        "Test search performance across different backends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search performance test\n",
        "def test_search_performance(manager, queries, top_k=5, iterations=10):\n",
        "    \"\"\"Test search performance for a given manager.\"\"\"\n",
        "    times = []\n",
        "    \n",
        "    for _ in range(iterations):\n",
        "        for query in queries:\n",
        "            start_time = time.time()\n",
        "            results = manager.search(query, top_k=top_k)\n",
        "            search_time = time.time() - start_time\n",
        "            times.append(search_time)\n",
        "    \n",
        "    return {\n",
        "        'avg_time': np.mean(times),\n",
        "        'std_time': np.std(times),\n",
        "        'min_time': np.min(times),\n",
        "        'max_time': np.max(times)\n",
        "    }\n",
        "\n",
        "# Test both backends\n",
        "print(\"Testing search performance...\")\n",
        "\n",
        "chroma_perf = test_search_performance(chroma_manager, queries)\n",
        "faiss_perf = test_search_performance(faiss_manager, queries)\n",
        "\n",
        "print(\"\\nSearch Performance Results:\")\n",
        "print(f\"ChromaDB - Avg: {chroma_perf['avg_time']:.4f}s, Std: {chroma_perf['std_time']:.4f}s\")\n",
        "print(f\"FAISS - Avg: {faiss_perf['avg_time']:.4f}s, Std: {faiss_perf['std_time']:.4f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize search performance\n",
        "search_data = {\n",
        "    'Backend': ['ChromaDB', 'FAISS'],\n",
        "    'Avg Search Time (ms)': [chroma_perf['avg_time']*1000, faiss_perf['avg_time']*1000],\n",
        "    'Std Dev (ms)': [chroma_perf['std_time']*1000, faiss_perf['std_time']*1000]\n",
        "}\n",
        "\n",
        "df_search = pd.DataFrame(search_data)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = np.arange(len(df_search))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, df_search['Avg Search Time (ms)'], width, \n",
        "        label='Average Time', color='skyblue', alpha=0.7)\n",
        "plt.bar(x + width/2, df_search['Std Dev (ms)'], width,\n",
        "        label='Standard Deviation', color='lightcoral', alpha=0.7)\n",
        "\n",
        "plt.xlabel('Backend')\n",
        "plt.ylabel('Time (milliseconds)')\n",
        "plt.title('Search Performance Comparison')\n",
        "plt.xticks(x, df_search['Backend'])\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Database Management Operations\n",
        "\n",
        "Learn how to manage vector databases in production.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get database statistics\n",
        "print(\"ChromaDB Statistics:\")\n",
        "chroma_stats = chroma_manager.get_stats()\n",
        "for key, value in chroma_stats.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\nFAISS Statistics:\")\n",
        "faiss_stats = faiss_manager.get_stats()\n",
        "for key, value in faiss_stats.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test document retrieval by ID\n",
        "print(\"Testing document retrieval by ID...\")\n",
        "test_id = \"chunk_0\"\n",
        "\n",
        "chroma_doc = chroma_manager.get_document(test_id)\n",
        "faiss_doc = faiss_manager.get_document(test_id)\n",
        "\n",
        "print(f\"\\nChromaDB Document {test_id}:\")\n",
        "if chroma_doc:\n",
        "    print(f\"  Text: {chroma_doc['document'][:100]}...\")\n",
        "    print(f\"  Metadata: {chroma_doc['metadata']}\")\n",
        "else:\n",
        "    print(\"  Document not found\")\n",
        "\n",
        "print(f\"\\nFAISS Document {test_id}:\")\n",
        "if faiss_doc:\n",
        "    print(f\"  Text: {faiss_doc['document'][:100]}...\")\n",
        "    print(f\"  Metadata: {faiss_doc['metadata']}\")\n",
        "else:\n",
        "    print(\"  Document not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test metadata filtering\n",
        "print(\"Testing metadata filtering...\")\n",
        "\n",
        "# Filter by source\n",
        "filtered_results = chroma_manager.search(\n",
        "    \"machine learning\", \n",
        "    top_k=3,\n",
        "    filter_dict={\"source\": \"wikipedia\"}\n",
        ")\n",
        "\n",
        "print(f\"\\nFiltered results (source='wikipedia'): {len(filtered_results)} documents\")\n",
        "for i, result in enumerate(filtered_results):\n",
        "    print(f\"  {i+1}. {result['document'][:80]}... (score: {result['score']:.3f})\")\n",
        "    print(f\"      Metadata: {result['metadata']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Production Deployment Considerations\n",
        "\n",
        "Key considerations for deploying vector databases in production.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Database backup example\n",
        "print(\"Testing database backup...\")\n",
        "backup_path = str(DATA_DIR / \"vector_db\" / \"backup\")\n",
        "\n",
        "backup_success = chroma_manager.backup_database(backup_path)\n",
        "print(f\"ChromaDB backup successful: {backup_success}\")\n",
        "\n",
        "backup_success = faiss_manager.backup_database(backup_path)\n",
        "print(f\"FAISS backup successful: {backup_success}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory usage analysis\n",
        "import psutil\n",
        "import tracemalloc\n",
        "\n",
        "def analyze_memory_usage(manager, operation_name):\n",
        "    \"\"\"Analyze memory usage for a given operation.\"\"\"\n",
        "    tracemalloc.start()\n",
        "    \n",
        "    # Get initial memory\n",
        "    process = psutil.Process()\n",
        "    initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
        "    \n",
        "    # Perform operation\n",
        "    if operation_name == \"search\":\n",
        "        results = manager.search(\"machine learning\", top_k=10)\n",
        "    elif operation_name == \"stats\":\n",
        "        stats = manager.get_stats()\n",
        "    \n",
        "    # Get final memory\n",
        "    final_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
        "    \n",
        "    # Get tracemalloc info\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    \n",
        "    return {\n",
        "        \"initial_memory_mb\": initial_memory,\n",
        "        \"final_memory_mb\": final_memory,\n",
        "        \"memory_delta_mb\": final_memory - initial_memory,\n",
        "        \"tracemalloc_current_mb\": current / 1024 / 1024,\n",
        "        \"tracemalloc_peak_mb\": peak / 1024 / 1024\n",
        "    }\n",
        "\n",
        "# Analyze memory usage\n",
        "print(\"Memory Usage Analysis:\")\n",
        "print(\"\\nChromaDB:\")\n",
        "chroma_memory = analyze_memory_usage(chroma_manager, \"search\")\n",
        "for key, value in chroma_memory.items():\n",
        "    print(f\"  {key}: {value:.2f} MB\")\n",
        "\n",
        "print(\"\\nFAISS:\")\n",
        "faiss_memory = analyze_memory_usage(faiss_manager, \"search\")\n",
        "for key, value in faiss_memory.items():\n",
        "    print(f\"  {key}: {value:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Production Best Practices\n",
        "\n",
        "Key recommendations for production vector database deployment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Production recommendations\n",
        "recommendations = {\n",
        "    \"ChromaDB\": {\n",
        "        \"Best For\": \"Local development, small to medium scale (< 1M docs)\",\n",
        "        \"Pros\": [\n",
        "            \"Easy to set up and use\",\n",
        "            \"Built-in metadata filtering\",\n",
        "            \"Persistent storage\",\n",
        "            \"Good for prototyping\"\n",
        "        ],\n",
        "        \"Cons\": [\n",
        "            \"Limited scalability\",\n",
        "            \"Single-node only\",\n",
        "            \"Memory intensive for large datasets\"\n",
        "        ],\n",
        "        \"Use Cases\": [\n",
        "            \"Development and testing\",\n",
        "            \"Small production applications\",\n",
        "            \"Rapid prototyping\"\n",
        "        ]\n",
        "    },\n",
        "    \"FAISS\": {\n",
        "        \"Best For\": \"High-performance search, large scale (> 1M docs)\",\n",
        "        \"Pros\": [\n",
        "            \"Extremely fast search\",\n",
        "            \"Memory efficient\",\n",
        "            \"Multiple index types\",\n",
        "            \"GPU acceleration support\"\n",
        "        ],\n",
        "        \"Cons\": [\n",
        "            \"No built-in metadata filtering\",\n",
        "            \"More complex setup\",\n",
        "            \"Requires manual persistence management\"\n",
        "        ],\n",
        "        \"Use Cases\": [\n",
        "            \"Large-scale production systems\",\n",
        "            \"High-throughput applications\",\n",
        "            \"Research and experimentation\"\n",
        "        ]\n",
        "    },\n",
        "    \"Pinecone\": {\n",
        "        \"Best For\": \"Cloud-native, fully managed solutions\",\n",
        "        \"Pros\": [\n",
        "            \"Fully managed service\",\n",
        "            \"Automatic scaling\",\n",
        "            \"Built-in metadata filtering\",\n",
        "            \"High availability\"\n",
        "        ],\n",
        "        \"Cons\": [\n",
        "            \"Cost for large datasets\",\n",
        "            \"Vendor lock-in\",\n",
        "            \"Requires internet connection\"\n",
        "        ],\n",
        "        \"Use Cases\": [\n",
        "            \"Production applications\",\n",
        "            \"Multi-tenant systems\",\n",
        "            \"Global deployments\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Production Vector Database Recommendations:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for backend, info in recommendations.items():\n",
        "    print(f\"\\n{backend}:\")\n",
        "    print(f\"  Best For: {info['Best For']}\")\n",
        "    print(f\"  Pros: {', '.join(info['Pros'])}\")\n",
        "    print(f\"  Cons: {', '.join(info['Cons'])}\")\n",
        "    print(f\"  Use Cases: {', '.join(info['Use Cases'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance summary\n",
        "print(\"\\nPerformance Summary:\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "print(f\"\\nChromaDB:\")\n",
        "print(f\"  Indexing Speed: {len(sample_docs)/chroma_time:.2f} docs/sec\")\n",
        "print(f\"  Search Speed: {chroma_perf['avg_time']*1000:.2f} ms/query\")\n",
        "print(f\"  Memory Usage: {chroma_memory['final_memory_mb']:.2f} MB\")\n",
        "print(f\"  Total Documents: {chroma_stats['total_documents']}\")\n",
        "\n",
        "print(f\"\\nFAISS:\")\n",
        "print(f\"  Indexing Speed: {len(sample_docs)/faiss_time:.2f} docs/sec\")\n",
        "print(f\"  Search Speed: {faiss_perf['avg_time']*1000:.2f} ms/query\")\n",
        "print(f\"  Memory Usage: {faiss_memory['final_memory_mb']:.2f} MB\")\n",
        "print(f\"  Total Documents: {faiss_stats['total_documents']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Integration with RAG System\n",
        "\n",
        "Now let's integrate the vector database with our RAG system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a production-ready RAG system with vector database\n",
        "class ProductionRAGSystem:\n",
        "    \"\"\"Production RAG system with vector database integration.\"\"\"\n",
        "    \n",
        "    def __init__(self, vector_db_manager, llm_model=None):\n",
        "        self.vector_db = vector_db_manager\n",
        "        self.llm_model = llm_model\n",
        "        \n",
        "    def query(self, question: str, top_k: int = 5, use_reranking: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"Query the RAG system.\"\"\"\n",
        "        # Retrieve relevant documents\n",
        "        retrieved_docs = self.vector_db.search(question, top_k=top_k)\n",
        "        \n",
        "        # Simple reranking (in production, use a proper reranker)\n",
        "        if use_reranking and len(retrieved_docs) > 1:\n",
        "            # Simple keyword-based reranking\n",
        "            question_keywords = set(question.lower().split())\n",
        "            for doc in retrieved_docs:\n",
        "                doc_keywords = set(doc['document'].lower().split())\n",
        "                keyword_overlap = len(question_keywords.intersection(doc_keywords))\n",
        "                doc['rerank_score'] = doc['score'] + (keyword_overlap * 0.1)\n",
        "            \n",
        "            retrieved_docs.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
        "        \n",
        "        # Generate response (simplified)\n",
        "        context = \" \".join([doc['document'] for doc in retrieved_docs[:3]])\n",
        "        \n",
        "        if self.llm_model:\n",
        "            # Use actual LLM here\n",
        "            response = f\"Based on the context: {context[:200]}...\"\n",
        "        else:\n",
        "            # Simple response generation\n",
        "            response = f\"I found {len(retrieved_docs)} relevant documents. Here's the most relevant information: {context[:200]}...\"\n",
        "        \n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"response\": response,\n",
        "            \"retrieved_documents\": retrieved_docs,\n",
        "            \"num_documents\": len(retrieved_docs)\n",
        "        }\n",
        "    \n",
        "    def add_documents(self, documents: List[Dict]) -> Dict[str, Any]:\n",
        "        \"\"\"Add documents to the RAG system.\"\"\"\n",
        "        texts = [doc.get(\"text\", \"\") for doc in documents]\n",
        "        metadatas = [doc.get(\"metadata\", {}) for doc in documents]\n",
        "        ids = [doc.get(\"id\", f\"doc_{i}\") for i, doc in enumerate(documents)]\n",
        "        \n",
        "        return self.vector_db.add_documents(texts, metadatas, ids)\n",
        "    \n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get system statistics.\"\"\"\n",
        "        return self.vector_db.get_stats()\n",
        "\n",
        "# Initialize production RAG system\n",
        "print(\"Initializing Production RAG System...\")\n",
        "production_rag = ProductionRAGSystem(chroma_manager)\n",
        "\n",
        "print(\"Production RAG System initialized successfully!\")\n",
        "print(f\"Database stats: {production_rag.get_stats()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the production RAG system\n",
        "test_questions = [\n",
        "    \"What is machine learning?\",\n",
        "    \"How does deep learning work?\",\n",
        "    \"Explain artificial intelligence\",\n",
        "    \"What are the applications of AI?\"\n",
        "]\n",
        "\n",
        "print(\"Testing Production RAG System:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "    result = production_rag.query(question, top_k=3)\n",
        "    print(f\"Response: {result['response']}\")\n",
        "    print(f\"Retrieved {result['num_documents']} documents\")\n",
        "    \n",
        "    # Show top document\n",
        "    if result['retrieved_documents']:\n",
        "        top_doc = result['retrieved_documents'][0]\n",
        "        print(f\"Top document: {top_doc['document'][:100]}... (score: {top_doc['score']:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **Vector Database Backends**: ChromaDB, FAISS, and Pinecone\n",
        "2. **Performance Comparison**: Indexing and search performance\n",
        "3. **Production Features**: Backup, monitoring, scalability\n",
        "4. **RAG Integration**: Complete production RAG system\n",
        "5. **Best Practices**: Recommendations for different use cases\n",
        "\n",
        "### Key Takeaways:\n",
        "- **ChromaDB**: Best for development and small-scale production\n",
        "- **FAISS**: Best for high-performance, large-scale applications\n",
        "- **Pinecone**: Best for cloud-native, fully managed solutions\n",
        "- **Production**: Always consider scalability, monitoring, and backup strategies\n",
        "\n",
        "### Next Steps for Learners:\n",
        "1. **Run Notebook 10** to understand vector database concepts\n",
        "2. **Compare backends** to see performance differences\n",
        "3. **Test the web app** to see production integration\n",
        "4. **Experiment with metadata filtering** for advanced use cases\n",
        "5. **Scale up** with larger datasets to see performance impact\n",
        "\n",
        "This vector database system provides a solid foundation for building production RAG applications that can scale from prototype to enterprise-level deployments!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
