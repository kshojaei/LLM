{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Preprocessing and Chunking Strategies\n",
        "\n",
        "In this notebook, we'll explore different text preprocessing and chunking strategies that are crucial for building effective RAG systems. The way we split and prepare our documents directly impacts retrieval quality.\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this notebook, you will:\n",
        "1. Understand different chunking strategies and their trade-offs\n",
        "2. Learn how to preprocess text for optimal retrieval\n",
        "3. Compare chunking methods on real documents\n",
        "4. Implement quality filtering and metadata preservation\n",
        "5. Understand the impact of chunking on RAG performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Let's import the libraries we need and load our data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Add project root to path\n",
        "import sys\n",
        "sys.path.append(str(Path.cwd().parent))\n",
        "\n",
        "# Import our modules\n",
        "from src.data.preprocess_data import TextPreprocessor, ChunkingConfig\n",
        "from src.config import DATA_DIR\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    print(\"Downloaded NLTK punkt tokenizer\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Sample Data\n",
        "\n",
        "Let's load some sample documents to work with different chunking strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 22 chunks from previous notebook\n"
          ]
        }
      ],
      "source": [
        "# Load processed chunks from previous notebook\n",
        "chunks_file = DATA_DIR / \"processed\" / \"all_chunks.json\"\n",
        "\n",
        "if chunks_file.exists():\n",
        "    with open(chunks_file, 'r', encoding='utf-8') as f:\n",
        "        all_chunks = json.load(f)\n",
        "    print(f\"Loaded {len(all_chunks)} chunks from previous notebook\")\n",
        "else:\n",
        "    print(\"No processed chunks found. Creating sample data...\")\n",
        "    \n",
        "    # Create sample documents for demonstration\n",
        "    sample_documents = [\n",
        "        {\n",
        "            'id': 'doc1',\n",
        "            'title': 'Machine Learning Fundamentals',\n",
        "            'text': '''Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data. It involves training models on historical data to make predictions or decisions without being explicitly programmed for every scenario. There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning. Supervised learning uses labeled training data to learn a mapping from inputs to outputs. Unsupervised learning finds hidden patterns in data without labeled examples. Reinforcement learning learns through interaction with an environment, receiving rewards or penalties for actions.''',\n",
        "            'source': 'wikipedia'\n",
        "        },\n",
        "        {\n",
        "            'id': 'doc2',\n",
        "            'title': 'Deep Learning and Neural Networks',\n",
        "            'text': '''Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers to model and understand complex patterns in data. These networks are inspired by the structure and function of the human brain. Deep learning has revolutionized many fields including computer vision, natural language processing, and speech recognition. Convolutional Neural Networks (CNNs) are particularly effective for image processing tasks. Recurrent Neural Networks (RNNs) and their variants like LSTM and GRU are well-suited for sequential data. The success of deep learning is largely due to the availability of large datasets, increased computational power, and improved algorithms.''',\n",
        "            'source': 'wikipedia'\n",
        "        },\n",
        "        {\n",
        "            'id': 'doc3',\n",
        "            'title': 'Natural Language Processing',\n",
        "            'text': '''Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. It involves developing algorithms and models that can understand, interpret, and generate human language in a valuable way. NLP tasks include text classification, sentiment analysis, named entity recognition, machine translation, and question answering. Recent advances in transformer models like BERT, GPT, and T5 have significantly improved NLP performance. These models use attention mechanisms to process sequences of text and can capture long-range dependencies in language.''',\n",
        "            'source': 'wikipedia'\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    all_chunks = sample_documents\n",
        "    print(f\"Created {len(all_chunks)} sample documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Chunking Strategies\n",
        "\n",
        "Let's explore different chunking strategies and their characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data.preprocess_data:Text preprocessor initialized. Output directory: /Users/scienceman/Desktop/LLM/data/processed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text Preprocessor initialized!\n",
            "Default chunk size: 512\n",
            "Default chunk overlap: 50\n",
            "\n",
            "Sample document: Machine learning\n",
            "Text length: 460 characters\n",
            "Word count: 68 words\n",
            "\n",
            "Text preview:\n",
            "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions Within a subdiscipline in machine learning, advances in...\n"
          ]
        }
      ],
      "source": [
        "# Initialize text preprocessor\n",
        "preprocessor = TextPreprocessor()\n",
        "\n",
        "print(\"Text Preprocessor initialized!\")\n",
        "print(f\"Default chunk size: {preprocessor.config.chunk_size}\")\n",
        "print(f\"Default chunk overlap: {preprocessor.config.chunk_overlap}\")\n",
        "\n",
        "# Let's examine one document in detail\n",
        "if all_chunks:\n",
        "    sample_doc = all_chunks[0]\n",
        "    print(f\"\\nSample document: {sample_doc['source_title']}\")\n",
        "    print(f\"Text length: {len(sample_doc['text'])} characters\")\n",
        "    print(f\"Word count: {len(sample_doc['text'].split())} words\")\n",
        "    print(f\"\\nText preview:\")\n",
        "    print(sample_doc['text'][:300] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fixed-Size Chunking\n",
        "\n",
        "Fixed-size chunking splits text into chunks of a predetermined size. This is simple but may break sentences or paragraphs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data.preprocess_data:Text preprocessor initialized. Output directory: /Users/scienceman/Desktop/LLM/data/processed\n",
            "INFO:src.data.preprocess_data:Text preprocessor initialized. Output directory: /Users/scienceman/Desktop/LLM/data/processed\n",
            "INFO:src.data.preprocess_data:Text preprocessor initialized. Output directory: /Users/scienceman/Desktop/LLM/data/processed\n",
            "INFO:src.data.preprocess_data:Text preprocessor initialized. Output directory: /Users/scienceman/Desktop/LLM/data/processed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixed-Size Chunking Comparison:\n",
            "==================================================\n",
            "\n",
            "Chunk size: 100\n",
            "Number of chunks: 0\n",
            "\n",
            "Chunk size: 200\n",
            "Number of chunks: 3\n",
            "Chunk lengths: min=170, max=195, avg=186.7\n",
            "First chunk: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "Chunks ending with sentence punctuation: 0/3\n",
            "\n",
            "Chunk size: 300\n",
            "Number of chunks: 2\n",
            "Chunk lengths: min=213, max=297, avg=255.0\n",
            "First chunk: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "Chunks ending with sentence punctuation: 0/2\n",
            "\n",
            "Chunk size: 500\n",
            "Number of chunks: 1\n",
            "Chunk lengths: min=460, max=460, avg=460.0\n",
            "First chunk: Machine learning (ML) is a field of study in artificial intelligence concerned with the development ...\n",
            "Chunks ending with sentence punctuation: 0/1\n"
          ]
        }
      ],
      "source": [
        "# Test fixed-size chunking with different sizes\n",
        "if all_chunks:\n",
        "    sample_doc = all_chunks[0]\n",
        "    \n",
        "    chunk_sizes = [100, 200, 300, 500]\n",
        "    \n",
        "    print(\"Fixed-Size Chunking Comparison:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for chunk_size in chunk_sizes:\n",
        "        # Create custom config\n",
        "        config = ChunkingConfig(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=50,\n",
        "            chunk_by_sentences=False\n",
        "        )\n",
        "        \n",
        "        # Create preprocessor with custom config\n",
        "        custom_preprocessor = TextPreprocessor(config)\n",
        "        \n",
        "        # Chunk the document\n",
        "        chunks = custom_preprocessor.chunk_document(sample_doc, strategy='fixed')\n",
        "        \n",
        "        print(f\"\\nChunk size: {chunk_size}\")\n",
        "        print(f\"Number of chunks: {len(chunks)}\")\n",
        "        \n",
        "        if chunks:\n",
        "            chunk_lengths = [len(chunk['text']) for chunk in chunks]\n",
        "            print(f\"Chunk lengths: min={min(chunk_lengths)}, max={max(chunk_lengths)}, avg={np.mean(chunk_lengths):.1f}\")\n",
        "            \n",
        "            # Show first chunk\n",
        "            print(f\"First chunk: {chunks[0]['text'][:100]}...\")\n",
        "            \n",
        "            # Check for sentence breaks\n",
        "            sentence_breaks = sum(1 for chunk in chunks if chunk['text'].endswith('.') or chunk['text'].endswith('!') or chunk['text'].endswith('?'))\n",
        "            print(f\"Chunks ending with sentence punctuation: {sentence_breaks}/{len(chunks)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Semantic Chunking\n",
        "\n",
        "Semantic chunking tries to keep related content together by splitting at natural boundaries like sentences or paragraphs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.data.preprocess_data:Text preprocessor initialized. Output directory: /Users/scienceman/Desktop/LLM/data/processed\n",
            "INFO:src.data.preprocess_data:Text preprocessor initialized. Output directory: /Users/scienceman/Desktop/LLM/data/processed\n",
            "INFO:src.data.preprocess_data:Text preprocessor initialized. Output directory: /Users/scienceman/Desktop/LLM/data/processed\n",
            "INFO:src.data.preprocess_data:Text preprocessor initialized. Output directory: /Users/scienceman/Desktop/LLM/data/processed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic Chunking:\n",
            "==============================\n",
            "\n",
            "Default semantic:\n",
            "  Chunk size: 512, Overlap: 50, By sentences: True\n",
            "  Number of chunks: 1\n",
            "  Chunk lengths: min=460, max=460, avg=460.0\n",
            "  First chunk: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn...\n",
            "\n",
            "Smaller chunks:\n",
            "  Chunk size: 256, Overlap: 25, By sentences: True\n",
            "  Number of chunks: 1\n",
            "  Chunk lengths: min=460, max=460, avg=460.0\n",
            "  First chunk: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn...\n",
            "\n",
            "Larger chunks:\n",
            "  Chunk size: 1024, Overlap: 100, By sentences: True\n",
            "  Number of chunks: 1\n",
            "  Chunk lengths: min=460, max=460, avg=460.0\n",
            "  First chunk: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn...\n",
            "\n",
            "No sentence boundary respect:\n",
            "  Chunk size: 512, Overlap: 50, By sentences: False\n",
            "  Number of chunks: 1\n",
            "  Chunk lengths: min=460, max=460, avg=460.0\n",
            "  First chunk: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn...\n"
          ]
        }
      ],
      "source": [
        "# Test semantic chunking\n",
        "if all_chunks:\n",
        "    sample_doc = all_chunks[0]\n",
        "    \n",
        "    print(\"Semantic Chunking:\")\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    # Test with different configurations\n",
        "    configs = [\n",
        "        (512, 50, True, \"Default semantic\"),\n",
        "        (256, 25, True, \"Smaller chunks\"),\n",
        "        (1024, 100, True, \"Larger chunks\"),\n",
        "        (512, 50, False, \"No sentence boundary respect\")\n",
        "    ]\n",
        "    \n",
        "    for chunk_size, overlap, by_sentences, description in configs:\n",
        "        config = ChunkingConfig(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=overlap,\n",
        "            chunk_by_sentences=by_sentences\n",
        "        )\n",
        "        \n",
        "        custom_preprocessor = TextPreprocessor(config)\n",
        "        chunks = custom_preprocessor.chunk_document(sample_doc, strategy='semantic')\n",
        "        \n",
        "        print(f\"\\n{description}:\")\n",
        "        print(f\"  Chunk size: {chunk_size}, Overlap: {overlap}, By sentences: {by_sentences}\")\n",
        "        print(f\"  Number of chunks: {len(chunks)}\")\n",
        "        \n",
        "        if chunks:\n",
        "            chunk_lengths = [len(chunk['text']) for chunk in chunks]\n",
        "            print(f\"  Chunk lengths: min={min(chunk_lengths)}, max={max(chunk_lengths)}, avg={np.mean(chunk_lengths):.1f}\")\n",
        "            \n",
        "            # Show first chunk\n",
        "            print(f\"  First chunk: {chunks[0]['text'][:150]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
